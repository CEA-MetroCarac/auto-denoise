{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Auto-Denoise","text":"<p>Auto-denoise (autoden) provides implementations for a small selection of unsupervised and self-supervised CNN denoising methods. These methods currently include:</p> <ul> <li>Noise2Noise (N2N) - A self-supervised denoising method using pairs of images of the same object [1].</li> <li>Noise2Void (N2V) - A self-supervised denoising method capable of working with a single image [2]. We have also implemented a later development of the method that can work with structured noise [3].</li> <li>Deep Image Prior (DIP) - An unsupervised denoising/upsampling/deconvolution method that can also work with a single image [4].</li> </ul> <p>We also provide example implementations of supervised denoising methods, and the tomography specific Noise2Inverse (N2I) method [5].</p> <p>References:</p> <ul> <li>[1] J. Lehtinen et al., \u201cNoise2Noise: Learning Image Restoration without Clean Data,\u201d in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, Eds., in Proceedings of Machine Learning Research, vol. 80. PMLR, 2018, pp. 2965\u20132974. https://proceedings.mlr.press/v80/lehtinen18a.html</li> <li>[2] A. Krull, T.-O. Buchholz, and F. Jug, \u201cNoise2Void - Learning Denoising From Single Noisy Images,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Jun. 2019, pp. 2124\u20132132. doi: 10.1109/CVPR.2019.00223.</li> <li>[3] C. Broaddus, A. Krull, M. Weigert, U. Schmidt, and G. Myers, \u201cRemoving Structured Noise with Self-Supervised Blind-Spot Networks,\u201d in 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE, Apr. 2020, pp. 159\u2013163. doi: 10.1109/ISBI45749.2020.9098336.</li> <li>[4] V. Lempitsky, A. Vedaldi, and D. Ulyanov, \u201cDeep Image Prior,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, Jun. 2018, pp. 9446\u20139454. doi: 10.1109/CVPR.2018.00984.</li> <li>[5] A. A. Hendriksen, D. M. Pelt, and K. J. Batenburg, \"Noise2Inverse: Self-Supervised Deep Convolutional Denoising for Tomography,\" IEEE Transactions on Computational Imaging, vol. 6, pp. 1320\u20131335, 2020, doi: 10.1109/TCI.2020.3019647.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>It takes just a few steps to setup Auto-Denoise on your machine.</p>"},{"location":"#installing-with-conda","title":"Installing with conda","text":"<p>We recommend using Miniforge. Once installed <code>miniforge</code>, simply install <code>autoden</code> with: <pre><code>conda install auto-denoise -c n-vigano\n</code></pre></p>"},{"location":"#installing-from-pypi","title":"Installing from PyPI","text":"<p>Simply install with: <pre><code>python -m pip install auto-denoise\n</code></pre></p> <p>If you are on jupyter, and don't have the rights to install packages system-wide, then you can install with: <pre><code>! python -m pip install --user auto-denoise\n</code></pre></p>"},{"location":"#installing-from-source","title":"Installing from source","text":"<p>To install Auto-Denoise, simply clone this github.com project with either: <pre><code>git clone https://github.com/CEA-MetroCarac/auto-denoise.git auto-denoise\n</code></pre> or: <pre><code>git clone git@github.com:CEA-MetroCarac/auto-denoise.git auto-denoise\n</code></pre></p> <p>Then go to the cloned directory and run <code>pip</code> installer: <pre><code>cd auto-denoise\npip install -e .\n</code></pre></p>"},{"location":"#how-to-contribute","title":"How to contribute","text":"<p>Contributions are always welcome. Please submit pull requests against the <code>main</code> branch.</p> <p>If you have any issues, questions, or remarks, then please open an issue on github.com.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#v100-2025-05-06","title":"v1.0.0 - 2025-05-06","text":"<p>This is the first major release of Auto-denoise, providing initial implementations for a small selection of unsupervised and self-supervised CNN denoising methods. These methods currently include:</p> <pre><code>Noise2Noise (N2N) - A self-supervised denoising method using pairs of images of the same object.\nNoise2Void (N2V) - A self-supervised denoising method capable of working with a single image. We have also implemented a later development of the method that can work with structured noise.\nDeep Image Prior (DIP) - An unsupervised denoising/upsampling/deconvolution method that can also work with a single image.\nSupervised denoising methods, and the tomography-specific Noise2Inverse (N2I) method.\n</code></pre> <p>We also provide a small set of pre-configured models for these algorithms: U-net, MS-D net, DnCNN, and a custom ResNet implementation.</p> <p>Compare with first commit</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nicola.vigano@cea.fr. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier!</p> <p>Fork and clone the repository, then:</p> <pre><code>cd auto-denoise\nmake setup\n</code></pre>"},{"location":"contributing/#development","title":"Development","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git switch -c feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>Before committing:</p> <ol> <li>format the code with <code>black -l 127 &lt;your_file_path&gt;.py</code></li> <li>run the tests with <code>pytest tests/*</code> (fix any issue)</li> <li>if you updated the documentation or the project dependencies:<ol> <li>run <code>mkdocs serve</code></li> <li>go to http://localhost:8000 and check that everything looks good</li> </ol> </li> <li>follow our commit message convention</li> </ol> <p>If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review.</p> <p>Don't bother updating the changelog, we will take care of this.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit message convention","text":"<p>Commit messages must follow our convention based on the Angular style or the Karma convention:</p> <pre><code>&lt;type&gt;[(scope)]: Subject\n\n[Body]\n</code></pre> <p>Subject and body must be valid Markdown. Subject must have proper casing (uppercase for first letter if it makes sense), but no dot at the end, and no punctuation in general.</p> <p>Scope and body are optional. Type can be:</p> <ul> <li><code>build</code>: About packaging, building wheels, etc.</li> <li><code>chore</code>: About packaging or repo/files management.</li> <li><code>ci</code>: About Continuous Integration.</li> <li><code>deps</code>: Dependencies update.</li> <li><code>docs</code>: About documentation.</li> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>perf</code>: About performance.</li> <li><code>refactor</code>: Changes that are not features or bug fixes.</li> <li><code>style</code>: A change in code style/format.</li> <li><code>tests</code>: About tests.</li> </ul> <p>If you write a body, please add trailers at the end (for example issues and PR references, or co-authors), without relying on GitHub's flavored Markdown:</p> <pre><code>Body.\n\nIssue #10: https://github.com/namespace/project/issues/10\nRelated to PR namespace/other-project#15: https://github.com/namespace/other-project/pull/15\n</code></pre> <p>These \"trailers\" must appear at the end of the body, without any blank lines between them. The trailer title can contain any character except colons <code>:</code>. We expect a full URI for each trailer, not just GitHub autolinks (for example, full GitHub URLs for commits and issues, not the hash or the #issue-number).</p> <p>We do not enforce a line length on commit messages summary and body, but please avoid very long summaries, and very long lines in the body, unless they are part of code blocks that must not be wrapped.</p>"},{"location":"contributing/#pull-requests-guidelines","title":"Pull requests guidelines","text":"<p>Link to any related issue in the Pull Request message.</p> <p>During the review, we recommend using fixups:</p> <pre><code># SHA is the SHA of the commit you want to fix\ngit commit --fixup=SHA\n</code></pre> <p>Once all the changes are approved, you can squash your commits:</p> <pre><code>git rebase -i --autosquash main\n</code></pre> <p>And force-push:</p> <pre><code>git push -f\n</code></pre> <p>If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.</p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2023 Nicola VIGANO\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> autoden<ul> <li> algorithms</li> <li> cli</li> <li> debug</li> <li> losses</li> <li> models<ul> <li> config</li> <li> dncnn</li> <li> io</li> <li> msd</li> <li> param_utils</li> <li> resnet</li> <li> unet</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/autoden/","title":"autoden","text":""},{"location":"reference/autoden/#autoden","title":"autoden","text":"<p>Auto-Denoise package.</p> <p>Unsupervised and self-supervised CNN denoising methods.</p> <p>Modules:</p> <ul> <li> <code>algorithms</code>           \u2013            <p>Implementation of various unsupervised and self-supervised denoising methods.</p> </li> <li> <code>cli</code>           \u2013            <p>Module that contains the command line application.</p> </li> <li> <code>debug</code>           \u2013            <p>Debugging utilities.</p> </li> <li> <code>losses</code>           \u2013            <p>Data losses definitions.</p> </li> <li> <code>models</code>           \u2013            <p>Models sub-package.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>DIP</code>           \u2013            <p>Deep image prior.</p> </li> <li> <code>DataScaleBias</code>           \u2013            <p>Data scale and bias.</p> </li> <li> <code>Denoiser</code>           \u2013            <p>Denoising images.</p> </li> <li> <code>LossRegularizer</code>           \u2013            <p>Base class for the regularizer losses.</p> </li> <li> <code>LossTV</code>           \u2013            <p>Total Variation loss function.</p> </li> <li> <code>N2N</code>           \u2013            <p>Self-supervised denoising from pairs of images.</p> </li> <li> <code>N2V</code>           \u2013            <p>Self-supervised denoising from single images.</p> </li> <li> <code>NetworkParams</code>           \u2013            <p>Abstract base class for storing network parameters.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>compute_scaling_selfsupervised</code>             \u2013              <p>Compute input data scaling and bias for self-supervised learning.</p> </li> <li> <code>compute_scaling_supervised</code>             \u2013              <p>Compute input and target data scaling and bias for supervised learning.</p> </li> <li> <code>create_network</code>             \u2013              <p>Create and return a neural network model based on the provided network configuration.</p> </li> <li> <code>create_optimizer</code>             \u2013              <p>Instantiates the desired optimizer for the given model.</p> </li> <li> <code>fix_invalid_gradient_values</code>             \u2013              <p>Fixes invalid gradient values in the model's parameters.</p> </li> <li> <code>get_num_parameters</code>             \u2013              <p>Returns the number of trainable parameters in the model.</p> </li> <li> <code>load_model_state</code>             \u2013              <p>Load a model from disk.</p> </li> <li> <code>save_model_state</code>             \u2013              <p>Save a model's state to disk.</p> </li> </ul>"},{"location":"reference/autoden/#autoden.DIP","title":"DIP","text":"<pre><code>DIP(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Deep image prior.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> <li> <code>train_unsupervised</code>             \u2013              <p>Train the model in an unsupervised manner.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.DIP.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.DIP.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/#autoden.DIP.train_unsupervised","title":"train_unsupervised","text":"<pre><code>train_unsupervised(\n    tgt: NDArray,\n    epochs: int,\n    inp: NDArray | None = None,\n    num_tst_ratio: float = 0.2,\n    algo: str = \"adam\",\n) -&gt; NDArray\n</code></pre> <p>Train the model in an unsupervised manner.</p> <p>Parameters:</p> <ul> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target image to be denoised.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of training epochs.</p> </li> <li> <code>inp</code>               (<code>NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The input image. If None, a random image will be generated. Default is None.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the test set size to the total dataset size. Default is 0.2.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to use. Default is \"adam\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised input image.</p> </li> </ul> Notes <p>This method trains the model using the deep image prior approach in an unsupervised manner. It uses a random initialization for the input image if not provided and applies a scaling and bias transformation to the input and target images. It then splits the data into training and test sets based on the provided ratio and trains the model using the specified optimization algorithm.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_unsupervised(\n    self, tgt: NDArray, epochs: int, inp: NDArray | None = None, num_tst_ratio: float = 0.2, algo: str = \"adam\"\n) -&gt; NDArray:\n    \"\"\"\n    Train the model in an unsupervised manner.\n\n    Parameters\n    ----------\n    tgt : NDArray\n        The target image to be denoised.\n    epochs : int\n        The number of training epochs.\n    inp : NDArray | None, optional\n        The input image. If None, a random image will be generated.\n        Default is None.\n    num_tst_ratio : float, optional\n        The ratio of the test set size to the total dataset size.\n        Default is 0.2.\n    algo : str, optional\n        The optimization algorithm to use. Default is \"adam\".\n\n    Returns\n    -------\n    NDArray\n        The denoised input image.\n\n    Notes\n    -----\n    This method trains the model using the deep image prior approach in an unsupervised manner.\n    It uses a random initialization for the input image if not provided and applies a scaling and bias\n    transformation to the input and target images. It then splits the data into training and test sets\n    based on the provided ratio and trains the model using the specified optimization algorithm.\n    \"\"\"\n    if inp is None:\n        inp = np.random.normal(size=tgt.shape[-2:], scale=0.25).astype(tgt.dtype)\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    tmp_inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tmp_tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    mask_trn = np.ones_like(tgt, dtype=bool)\n    rnd_inds = np.random.random_integers(low=0, high=mask_trn.size - 1, size=int(mask_trn.size * num_tst_ratio))\n    mask_trn[np.unravel_index(rnd_inds, shape=mask_trn.shape)] = False\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_small(tmp_inp, tmp_tgt, mask_trn, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Unsupervised {self.__class__.__name__} {algo.upper()}\")\n\n    return inp\n</code></pre>"},{"location":"reference/autoden/#autoden.DataScaleBias","title":"DataScaleBias  <code>dataclass</code>","text":"<pre><code>DataScaleBias(\n    scale_inp: float | NDArray = 1.0,\n    scale_out: float | NDArray = 1.0,\n    scale_tgt: float | NDArray = 1.0,\n    bias_inp: float | NDArray = 0.0,\n    bias_out: float | NDArray = 0.0,\n    bias_tgt: float | NDArray = 0.0,\n)\n</code></pre> <p>Data scale and bias.</p>"},{"location":"reference/autoden/#autoden.Denoiser","title":"Denoiser","text":"<pre><code>Denoiser(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>Denoising images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.Denoiser.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.Denoiser.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/#autoden.LossRegularizer","title":"LossRegularizer","text":"<p>               Bases: <code>MSELoss</code></p> <p>Base class for the regularizer losses.</p>"},{"location":"reference/autoden/#autoden.LossTV","title":"LossTV","text":"<pre><code>LossTV(\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    ndims: int = 2,\n)\n</code></pre> <p>               Bases: <code>LossRegularizer</code></p> <p>Total Variation loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute total variation statistics on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    ndims: int = 2,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.ndims = ndims\n</code></pre>"},{"location":"reference/autoden/#autoden.LossTV.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    _check_input_tensor(img, self.ndims)\n    axes = list(range(-(self.ndims + 1), 0))\n\n    diffs = [_differentiate(img, dim=dim, position=\"post\") for dim in range(-self.ndims, 0)]\n    diffs = pt.stack(diffs, dim=0)\n\n    if self.isotropic:\n        # tv_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffs], dim=0).sum(dim=0))\n        tv_val = pt.sqrt(pt.pow(diffs, 2).sum(dim=0))\n    else:\n        # tv_val = pt.stack([d.abs() for d in diffs], dim=0).sum(dim=0)\n        tv_val = diffs.abs().sum(dim=0)\n\n    return self.lambda_val * tv_val.sum(axes).mean()\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N","title":"N2N","text":"<pre><code>N2N(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from pairs of images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_selfsupervised</code>             \u2013              <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N.train_selfsupervised","title":"train_selfsupervised","text":"<pre><code>train_selfsupervised(\n    inp: NDArray,\n    epochs: int,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"1:X\",\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; None\n</code></pre> <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of epochs to train the model.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the input data to be used for testing. The remaining data will be used for training. Default is 0.2.</p> </li> <li> <code>strategy</code>               (<code>str</code>, default:                   <code>'1:X'</code> )           \u2013            <p>The strategy to be used for creating input-target pairs. The available strategies are: - \"1:X\": Use the mean of the remaining samples as the target for each sample. - \"X:1\": Use the mean of the remaining samples as the input for each sample. Default is \"1:X\".</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to be used for training. The available algorithms are: - \"adam\": Adam optimizer. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> </ul> Notes <p>This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to generate target data based on the specified strategy. The training process involves creating pairs of input and target data and then training the model to minimize the difference between the predicted and target data.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_selfsupervised(\n    self,\n    inp: NDArray,\n    epochs: int,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"1:X\",\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; None:\n    \"\"\"\n    Train the denoiser using the Noise2Noise self-supervised approach.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    epochs : int\n        The number of epochs to train the model.\n    num_tst_ratio : float, optional\n        The ratio of the input data to be used for testing. The remaining data will be used for training.\n        Default is 0.2.\n    strategy : str, optional\n        The strategy to be used for creating input-target pairs. The available strategies are:\n        - \"1:X\": Use the mean of the remaining samples as the target for each sample.\n        - \"X:1\": Use the mean of the remaining samples as the input for each sample.\n        Default is \"1:X\".\n    algo : str, optional\n        The optimization algorithm to be used for training. The available algorithms are:\n        - \"adam\": Adam optimizer.\n        Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n\n    Notes\n    -----\n    This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to\n    generate target data based on the specified strategy. The training process involves creating pairs of input\n    and target data and then training the model to minimize the difference between the predicted and target data.\n    \"\"\"\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    mask_trn = np.ones_like(inp, dtype=bool)\n    rnd_inds = np.random.random_integers(low=0, high=mask_trn.size - 1, size=int(mask_trn.size * num_tst_ratio))\n    mask_trn[np.unravel_index(rnd_inds, shape=mask_trn.shape)] = False\n\n    inp_x = np.stack([np.delete(inp, obj=ii, axis=0).mean(axis=0) for ii in range(len(inp))], axis=0)\n    if strategy.upper() == \"1:X\":\n        tmp_inp = inp\n        tmp_tgt = inp_x\n    elif strategy.upper() == \"X:1\":\n        tmp_inp = inp_x\n        tmp_tgt = inp\n    else:\n        raise ValueError(f\"Strategy {strategy} not implemented. Please choose one of: ['1:X', 'X:1']\")\n\n    tmp_inp = tmp_inp.astype(np.float32)\n    tmp_tgt = tmp_tgt.astype(np.float32)\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_small(\n        tmp_inp, tmp_tgt, mask_trn, epochs=epochs, algo=algo, regularizer=reg, lower_limit=lower_limit\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V","title":"N2V","text":"<pre><code>N2V(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from single images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_selfsupervised</code>             \u2013              <p>Self-supervised training.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V.train_selfsupervised","title":"train_selfsupervised","text":"<pre><code>train_selfsupervised(\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Self-supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images, which will also be targets</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>mask_shape</code>               (<code>int | Sequence[int] | NDArray</code>, default:                   <code>1</code> )           \u2013            <p>Shape of the blind spot mask, by default 1.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_selfsupervised(\n    self,\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n):\n    \"\"\"Self-supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images, which will also be targets\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    mask_shape : int | Sequence[int] | NDArray\n        Shape of the blind spot mask, by default 1.\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_trn = inp[trn_inds]\n    inp_tst = inp[tst_inds]\n\n    reg = self._get_regularization()\n    losses = self._train_n2v_pixelmask_small(\n        inp_trn,\n        inp_tst,\n        epochs=epochs,\n        mask_shape=mask_shape,\n        ratio_blind_spot=ratio_blind_spot,\n        algo=algo,\n        regularizer=reg,\n    )\n\n    self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/#autoden.NetworkParams","title":"NetworkParams","text":"<pre><code>NetworkParams(\n    n_features: int,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for storing network parameters.</p> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get the associated model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(self, n_features: int, n_channels_in: int = 1, n_channels_out: int = 1) -&gt; None:\n    self.n_channels_in = n_channels_in\n    self.n_channels_out = n_channels_out\n    self.n_features = n_features\n</code></pre>"},{"location":"reference/autoden/#autoden.NetworkParams.get_model","title":"get_model  <code>abstractmethod</code>","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get the associated model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>@abstractmethod\ndef get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get the associated model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoden/#autoden.compute_scaling_selfsupervised","title":"compute_scaling_selfsupervised","text":"<pre><code>compute_scaling_selfsupervised(\n    inp: NDArray,\n) -&gt; DataScaleBias\n</code></pre> <p>Compute input data scaling and bias for self-supervised learning.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>Input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>An instance of DataScaleBias containing the computed scaling and bias values.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def compute_scaling_selfsupervised(inp: NDArray) -&gt; DataScaleBias:\n    \"\"\"\n    Compute input data scaling and bias for self-supervised learning.\n\n    Parameters\n    ----------\n    inp : NDArray\n        Input data.\n\n    Returns\n    -------\n    DataScaleBias\n        An instance of DataScaleBias containing the computed scaling and bias values.\n    \"\"\"\n    range_vals_inp = _get_normalization(inp, percentile=0.001)\n\n    sb = DataScaleBias()\n    sb.scale_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    sb.scale_out = sb.scale_tgt = sb.scale_inp\n\n    sb.bias_inp = range_vals_inp[2] * sb.scale_inp\n    sb.bias_out = sb.bias_tgt = sb.bias_inp\n\n    return sb\n</code></pre>"},{"location":"reference/autoden/#autoden.compute_scaling_supervised","title":"compute_scaling_supervised","text":"<pre><code>compute_scaling_supervised(\n    inp: NDArray, tgt: NDArray\n) -&gt; DataScaleBias\n</code></pre> <p>Compute input and target data scaling and bias for supervised learning.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>Input data.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>Target data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>An instance of DataScaleBias containing the computed scaling and bias values.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def compute_scaling_supervised(inp: NDArray, tgt: NDArray) -&gt; DataScaleBias:\n    \"\"\"\n    Compute input and target data scaling and bias for supervised learning.\n\n    Parameters\n    ----------\n    inp : NDArray\n        Input data.\n    tgt : NDArray\n        Target data.\n\n    Returns\n    -------\n    DataScaleBias\n        An instance of DataScaleBias containing the computed scaling and bias values.\n    \"\"\"\n    range_vals_inp = _get_normalization(inp, percentile=0.001)\n    range_vals_tgt = _get_normalization(tgt, percentile=0.001)\n\n    sb = DataScaleBias()\n    sb.scale_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    sb.scale_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n    sb.scale_out = sb.scale_tgt\n\n    sb.bias_inp = range_vals_inp[2] * sb.scale_inp\n    sb.bias_tgt = range_vals_tgt[2] * sb.scale_tgt\n    sb.bias_out = sb.bias_tgt\n\n    return sb\n</code></pre>"},{"location":"reference/autoden/#autoden.create_network","title":"create_network","text":"<pre><code>create_network(\n    model: str | NetworkParams | Mapping | Module,\n    init_params: Mapping | None = None,\n    state_dict: Mapping | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Create and return a neural network model based on the provided network configuration.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Mapping | Module</code>)           \u2013            <p>The network configuration. It can be a string specifying the network type, an instance of <code>NetworkParams</code>, or an already instantiated <code>Module</code>. If a string is provided, it must be one of the supported network types: \"msd\", \"unet\", or \"dncnn\".</p> </li> <li> <code>state_dict</code>               (<code>Mapping | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing the state dictionary of the model. If provided, the model's parameters will be loaded from this dictionary. Default is None.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device to which the model should be moved. Default is \"cuda\" if CUDA is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The created neural network model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the provided network name is invalid or the network type is not supported.</p> </li> </ul> Notes <p>The function supports the following network types: - \"msd\": Multi-Scale Dense Network. - \"unet\": U-Net. - \"dncnn\": Denoising Convolutional Neural Network.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net = create_network(\"unet\")\n&gt;&gt;&gt; print(net)\nModel UNet - num. parameters: 1234567\n</code></pre> Source code in <code>src/autoden/models/config.py</code> <pre><code>def create_network(\n    model: str | NetworkParams | Mapping | Module,\n    init_params: Mapping | None = None,\n    state_dict: Mapping | None = None,\n    device: str = \"cuda\" if is_cuda_available() else \"cpu\",\n) -&gt; Module:\n    \"\"\"\n    Create and return a neural network model based on the provided network configuration.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | Mapping | Module\n        The network configuration. It can be a string specifying the network type,\n        an instance of `NetworkParams`, or an already instantiated `Module`.\n        If a string is provided, it must be one of the supported network types:\n        \"msd\", \"unet\", or \"dncnn\".\n    state_dict : Mapping | None, optional\n        A dictionary containing the state dictionary of the model. If provided,\n        the model's parameters will be loaded from this dictionary. Default is None.\n    device : str, optional\n        The device to which the model should be moved. Default is \"cuda\" if CUDA is available,\n        otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The created neural network model.\n\n    Raises\n    ------\n    ValueError\n        If the provided network name is invalid or the network type is not supported.\n\n    Notes\n    -----\n    The function supports the following network types:\n    - \"msd\": Multi-Scale Dense Network.\n    - \"unet\": U-Net.\n    - \"dncnn\": Denoising Convolutional Neural Network.\n\n    Examples\n    --------\n    &gt;&gt;&gt; net = create_network(\"unet\")\n    &gt;&gt;&gt; print(net)\n    Model UNet - num. parameters: 1234567\n    \"\"\"\n    if isinstance(model, Mapping):\n        if not all(key in model for key in (\"model_class\", \"init_params\", \"state_dict\")):\n            raise ValueError(\n                \"Malformed model state dictionary. Expected mandatory fields: 'model_class', 'init_params', and 'state_dict'\"\n            )\n        state_dict = model[\"state_dict\"]\n        init_params = model[\"init_params\"]\n        model = model[\"model_class\"]\n\n    if init_params is None:\n        init_params = dict()\n    else:\n        init_params = dict(**init_params)\n\n    for par in (\"device\", \"verbose\"):\n        if par in init_params:\n            del init_params[par]\n\n    if isinstance(model, str):\n        if model.lower() in (\"msd\", MSDnet.__name__.lower()):\n            model = NetworkParamsMSD(**init_params)\n        elif model.lower() == UNet.__name__.lower():\n            model = NetworkParamsUNet(**init_params)\n        elif model.lower() == DnCNN.__name__.lower():\n            model = NetworkParamsDnCNN(**init_params)\n        elif model.lower() == Resnet.__name__.lower():\n            model = NetworkParamsResnet(**init_params)\n        else:\n            raise ValueError(f\"Invalid model name: {model}\")\n\n    if isinstance(model, NetworkParams):\n        net = model.get_model(device)\n    elif isinstance(model, Module):\n        net = model.to(device=device)\n    else:\n        raise ValueError(f\"Invalid model type: {type(model)}\")\n\n    if state_dict is not None:\n        net.load_state_dict(state_dict)\n        net.to(device)  # Needed to ensure that the model lives in the correct device\n\n    print(f\"Model {net.__class__.__name__} - num. parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad)}\")\n    return net\n</code></pre>"},{"location":"reference/autoden/#autoden.create_optimizer","title":"create_optimizer","text":"<pre><code>create_optimizer(\n    network: Module,\n    algo: str = \"adam\",\n    learning_rate: float = 0.001,\n    weight_decay: float = 0.01,\n    optim_state: Mapping | None = None,\n) -&gt; Optimizer\n</code></pre> <p>Instantiates the desired optimizer for the given model.</p> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>Module</code>)           \u2013            <p>The network to train.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The requested optimizer, by default \"adam\".</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The desired learning rate, by default 1e-3.</p> </li> <li> <code>weight_decay</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The desired weight decay, by default 1e-2.</p> </li> <li> <code>optim_state</code>               (<code>Mapping | None</code>, default:                   <code>None</code> )           \u2013            <p>The state dictionary for the optimizer, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>The chosen optimizer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unsupported algorithm is requested.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def create_optimizer(\n    network: Module,\n    algo: str = \"adam\",\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-2,\n    optim_state: Mapping | None = None,\n) -&gt; Optimizer:\n    \"\"\"Instantiates the desired optimizer for the given model.\n\n    Parameters\n    ----------\n    network : torch.nn.Module\n        The network to train.\n    algo : str, optional\n        The requested optimizer, by default \"adam\".\n    learning_rate : float, optional\n        The desired learning rate, by default 1e-3.\n    weight_decay : float, optional\n        The desired weight decay, by default 1e-2.\n    optim_state : Mapping | None, optional\n        The state dictionary for the optimizer, by default None.\n\n    Returns\n    -------\n    torch.optim.Optimizer\n        The chosen optimizer.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported algorithm is requested.\n    \"\"\"\n    if algo.lower() == \"adam\":\n        optimizer = pt.optim.AdamW(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"sgd\":\n        optimizer = pt.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"rmsprop\":\n        optimizer = pt.optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"lbfgs\":\n        optimizer = pt.optim.LBFGS(network.parameters(), lr=learning_rate, max_iter=10000, history_size=50)\n    else:\n        raise ValueError(f\"Unknown algorithm: {algo}\")\n\n    if optim_state is not None:\n        optimizer.load_state_dict(dict(**optim_state))\n\n    return optimizer\n</code></pre>"},{"location":"reference/autoden/#autoden.fix_invalid_gradient_values","title":"fix_invalid_gradient_values","text":"<pre><code>fix_invalid_gradient_values(model: Module) -&gt; None\n</code></pre> <p>Fixes invalid gradient values in the model's parameters.</p> <p>This function iterates over all parameters of the given model and sets the gradient values to zero where they are not finite (i.e., NaN or infinity).</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The neural network model whose gradient values need to be fixed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>This function modifies the gradients in place and does not return anything.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def fix_invalid_gradient_values(model: nn.Module) -&gt; None:\n    \"\"\"\n    Fixes invalid gradient values in the model's parameters.\n\n    This function iterates over all parameters of the given model and sets the\n    gradient values to zero where they are not finite (i.e., NaN or infinity).\n\n    Parameters\n    ----------\n    model : nn.Module\n        The neural network model whose gradient values need to be fixed.\n\n    Returns\n    -------\n    None\n        This function modifies the gradients in place and does not return anything.\n    \"\"\"\n    for pars in model.parameters():\n        if pars.grad is not None:\n            pars.grad[pt.logical_not(pt.isfinite(pars.grad))] = 0.0\n</code></pre>"},{"location":"reference/autoden/#autoden.get_num_parameters","title":"get_num_parameters","text":"<pre><code>get_num_parameters(\n    model: Module, verbose: bool = False\n) -&gt; int\n</code></pre> <p>Returns the number of trainable parameters in the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to count the parameters for.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, prints the number of parameters, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of trainable parameters.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_num_parameters(model: nn.Module, verbose: bool = False) -&gt; int:\n    \"\"\"Returns the number of trainable parameters in the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to count the parameters for.\n    verbose : bool, optional\n        If True, prints the number of parameters, by default False.\n\n    Returns\n    -------\n    int\n        The number of trainable parameters.\n    \"\"\"\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if verbose:\n        print(f\"Model {model.__class__.__name__} - num. parameters: {num_params}\")\n    return num_params\n</code></pre>"},{"location":"reference/autoden/#autoden.load_model_state","title":"load_model_state","text":"<pre><code>load_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int | None = None,\n) -&gt; Mapping\n</code></pre> <p>Load a model from disk.</p> <p>Parameters:</p> <ul> <li> <code>save_epochs_dir</code>               (<code>str | Path</code>)           \u2013            <p>The director where the models are saved</p> </li> <li> <code>epoch_num</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The epoch number or if None/-1 the best state will be loaded, by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Mapping</code>           \u2013            <p>The loaded model state and possibly an optimizer state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the directory does not exist or the requested model is not available.</p> </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def load_model_state(save_epochs_dir: str | Path, epoch_num: int | None = None) -&gt; Mapping:\n    \"\"\"Load a model from disk.\n\n    Parameters\n    ----------\n    save_epochs_dir : str | Path\n        The director where the models are saved\n    epoch_num : int | None, optional\n        The epoch number or if None/-1 the best state will be loaded, by default None\n\n    Returns\n    -------\n    Mapping\n        The loaded model state and possibly an optimizer state.\n\n    Raises\n    ------\n    ValueError\n        When the directory does not exist or the requested model is not available.\n    \"\"\"\n    epochs_base_path = Path(save_epochs_dir) / \"weights\"\n    if not epochs_base_path.exists():\n        raise ValueError(f\"Directory of the model state {epochs_base_path} does not exist!\")\n\n    if epoch_num is None or epoch_num == -1:\n        state_path = epochs_base_path / \"weights.pt\"\n    else:\n        state_path = epochs_base_path / f\"weights_epoch_{epoch_num}.pt\"\n    if not state_path.exists():\n        raise ValueError(f\"Model state {state_path} does not exist!\")\n\n    print(f\"Loading state path: {state_path}\")\n    return pt.load(state_path)\n</code></pre>"},{"location":"reference/autoden/#autoden.save_model_state","title":"save_model_state","text":"<pre><code>save_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int,\n    model: Module,\n    optim_state: Mapping | None = None,\n    is_best: bool = False,\n) -&gt; None\n</code></pre> <p>Save a model's state to disk.</p> <p>This function saves the state of a model and optionally its optimizer to disk. The model state is saved in a directory specified by <code>save_epochs_dir</code>. If <code>is_best</code> is True, the model state is saved as \"weights.pt\". Otherwise, it is saved with a filename that includes the epoch number.</p> <p>Parameters:</p> <ul> <li> <code>save_epochs_dir</code>               (<code>str | Path</code>)           \u2013            <p>The directory where to save the model state.</p> </li> <li> <code>epoch_num</code>               (<code>int</code>)           \u2013            <p>The epoch number.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model whose state is to be saved.</p> </li> <li> <code>optim_state</code>               (<code>Mapping</code>, default:                   <code>None</code> )           \u2013            <p>The optimizer state to save, by default None.</p> </li> <li> <code>is_best</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether it is the best fitted model, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def save_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int,\n    model: Module,\n    optim_state: Mapping | None = None,\n    is_best: bool = False,\n) -&gt; None:\n    \"\"\"Save a model's state to disk.\n\n    This function saves the state of a model and optionally its optimizer to disk.\n    The model state is saved in a directory specified by `save_epochs_dir`. If\n    `is_best` is True, the model state is saved as \"weights.pt\". Otherwise, it is\n    saved with a filename that includes the epoch number.\n\n    Parameters\n    ----------\n    save_epochs_dir : str | Path\n        The directory where to save the model state.\n    epoch_num : int\n        The epoch number.\n    model : Module\n        The model whose state is to be saved.\n    optim_state : Mapping, optional\n        The optimizer state to save, by default None.\n    is_best : bool, optional\n        Whether it is the best fitted model, by default False.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    epochs_base_path = Path(save_epochs_dir) / \"weights\"\n    epochs_base_path.mkdir(parents=True, exist_ok=True)\n\n    dst_file = epochs_base_path / (\"weights.pt\" if is_best else f\"weights_epoch_{epoch_num}.pt\")\n    save_model(dst_file=dst_file, model=model, optim_state=optim_state, epoch_num=epoch_num)\n</code></pre>"},{"location":"reference/autoden/algorithms/","title":"autoden.algorithms","text":""},{"location":"reference/autoden/algorithms/#autoden.algorithms","title":"algorithms","text":"<p>Implementation of various unsupervised and self-supervised denoising methods.</p> <p>Classes:</p> <ul> <li> <code>DIP</code>           \u2013            <p>Deep image prior.</p> </li> <li> <code>DataScaleBias</code>           \u2013            <p>Data scale and bias.</p> </li> <li> <code>Denoiser</code>           \u2013            <p>Denoising images.</p> </li> <li> <code>N2N</code>           \u2013            <p>Self-supervised denoising from pairs of images.</p> </li> <li> <code>N2V</code>           \u2013            <p>Self-supervised denoising from single images.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>compute_scaling_selfsupervised</code>             \u2013              <p>Compute input data scaling and bias for self-supervised learning.</p> </li> <li> <code>compute_scaling_supervised</code>             \u2013              <p>Compute input and target data scaling and bias for supervised learning.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.DIP","title":"DIP","text":"<pre><code>DIP(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Deep image prior.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> <li> <code>train_unsupervised</code>             \u2013              <p>Train the model in an unsupervised manner.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.DIP.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.DIP.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.DIP.train_unsupervised","title":"train_unsupervised","text":"<pre><code>train_unsupervised(\n    tgt: NDArray,\n    epochs: int,\n    inp: NDArray | None = None,\n    num_tst_ratio: float = 0.2,\n    algo: str = \"adam\",\n) -&gt; NDArray\n</code></pre> <p>Train the model in an unsupervised manner.</p> <p>Parameters:</p> <ul> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target image to be denoised.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of training epochs.</p> </li> <li> <code>inp</code>               (<code>NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The input image. If None, a random image will be generated. Default is None.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the test set size to the total dataset size. Default is 0.2.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to use. Default is \"adam\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised input image.</p> </li> </ul> Notes <p>This method trains the model using the deep image prior approach in an unsupervised manner. It uses a random initialization for the input image if not provided and applies a scaling and bias transformation to the input and target images. It then splits the data into training and test sets based on the provided ratio and trains the model using the specified optimization algorithm.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_unsupervised(\n    self, tgt: NDArray, epochs: int, inp: NDArray | None = None, num_tst_ratio: float = 0.2, algo: str = \"adam\"\n) -&gt; NDArray:\n    \"\"\"\n    Train the model in an unsupervised manner.\n\n    Parameters\n    ----------\n    tgt : NDArray\n        The target image to be denoised.\n    epochs : int\n        The number of training epochs.\n    inp : NDArray | None, optional\n        The input image. If None, a random image will be generated.\n        Default is None.\n    num_tst_ratio : float, optional\n        The ratio of the test set size to the total dataset size.\n        Default is 0.2.\n    algo : str, optional\n        The optimization algorithm to use. Default is \"adam\".\n\n    Returns\n    -------\n    NDArray\n        The denoised input image.\n\n    Notes\n    -----\n    This method trains the model using the deep image prior approach in an unsupervised manner.\n    It uses a random initialization for the input image if not provided and applies a scaling and bias\n    transformation to the input and target images. It then splits the data into training and test sets\n    based on the provided ratio and trains the model using the specified optimization algorithm.\n    \"\"\"\n    if inp is None:\n        inp = np.random.normal(size=tgt.shape[-2:], scale=0.25).astype(tgt.dtype)\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    tmp_inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tmp_tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    mask_trn = np.ones_like(tgt, dtype=bool)\n    rnd_inds = np.random.random_integers(low=0, high=mask_trn.size - 1, size=int(mask_trn.size * num_tst_ratio))\n    mask_trn[np.unravel_index(rnd_inds, shape=mask_trn.shape)] = False\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_small(tmp_inp, tmp_tgt, mask_trn, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Unsupervised {self.__class__.__name__} {algo.upper()}\")\n\n    return inp\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.DataScaleBias","title":"DataScaleBias  <code>dataclass</code>","text":"<pre><code>DataScaleBias(\n    scale_inp: float | NDArray = 1.0,\n    scale_out: float | NDArray = 1.0,\n    scale_tgt: float | NDArray = 1.0,\n    bias_inp: float | NDArray = 0.0,\n    bias_out: float | NDArray = 0.0,\n    bias_tgt: float | NDArray = 0.0,\n)\n</code></pre> <p>Data scale and bias.</p>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.Denoiser","title":"Denoiser","text":"<pre><code>Denoiser(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>Denoising images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.Denoiser.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.Denoiser.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2N","title":"N2N","text":"<pre><code>N2N(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from pairs of images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_selfsupervised</code>             \u2013              <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2N.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2N.train_selfsupervised","title":"train_selfsupervised","text":"<pre><code>train_selfsupervised(\n    inp: NDArray,\n    epochs: int,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"1:X\",\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; None\n</code></pre> <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of epochs to train the model.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the input data to be used for testing. The remaining data will be used for training. Default is 0.2.</p> </li> <li> <code>strategy</code>               (<code>str</code>, default:                   <code>'1:X'</code> )           \u2013            <p>The strategy to be used for creating input-target pairs. The available strategies are: - \"1:X\": Use the mean of the remaining samples as the target for each sample. - \"X:1\": Use the mean of the remaining samples as the input for each sample. Default is \"1:X\".</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to be used for training. The available algorithms are: - \"adam\": Adam optimizer. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> </ul> Notes <p>This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to generate target data based on the specified strategy. The training process involves creating pairs of input and target data and then training the model to minimize the difference between the predicted and target data.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_selfsupervised(\n    self,\n    inp: NDArray,\n    epochs: int,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"1:X\",\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; None:\n    \"\"\"\n    Train the denoiser using the Noise2Noise self-supervised approach.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    epochs : int\n        The number of epochs to train the model.\n    num_tst_ratio : float, optional\n        The ratio of the input data to be used for testing. The remaining data will be used for training.\n        Default is 0.2.\n    strategy : str, optional\n        The strategy to be used for creating input-target pairs. The available strategies are:\n        - \"1:X\": Use the mean of the remaining samples as the target for each sample.\n        - \"X:1\": Use the mean of the remaining samples as the input for each sample.\n        Default is \"1:X\".\n    algo : str, optional\n        The optimization algorithm to be used for training. The available algorithms are:\n        - \"adam\": Adam optimizer.\n        Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n\n    Notes\n    -----\n    This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to\n    generate target data based on the specified strategy. The training process involves creating pairs of input\n    and target data and then training the model to minimize the difference between the predicted and target data.\n    \"\"\"\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    mask_trn = np.ones_like(inp, dtype=bool)\n    rnd_inds = np.random.random_integers(low=0, high=mask_trn.size - 1, size=int(mask_trn.size * num_tst_ratio))\n    mask_trn[np.unravel_index(rnd_inds, shape=mask_trn.shape)] = False\n\n    inp_x = np.stack([np.delete(inp, obj=ii, axis=0).mean(axis=0) for ii in range(len(inp))], axis=0)\n    if strategy.upper() == \"1:X\":\n        tmp_inp = inp\n        tmp_tgt = inp_x\n    elif strategy.upper() == \"X:1\":\n        tmp_inp = inp_x\n        tmp_tgt = inp\n    else:\n        raise ValueError(f\"Strategy {strategy} not implemented. Please choose one of: ['1:X', 'X:1']\")\n\n    tmp_inp = tmp_inp.astype(np.float32)\n    tmp_tgt = tmp_tgt.astype(np.float32)\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_small(\n        tmp_inp, tmp_tgt, mask_trn, epochs=epochs, algo=algo, regularizer=reg, lower_limit=lower_limit\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2N.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2V","title":"N2V","text":"<pre><code>N2V(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-05,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from single images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>1e-05</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train_selfsupervised</code>             \u2013              <p>Self-supervised training.</p> </li> <li> <code>train_supervised</code>             \u2013              <p>Supervised training.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = 1e-5,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2V.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = _single_channel_imgs_to_tensor(inp, device=self.device)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.to(\"cpu\").numpy().reshape(inp.shape)\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2V.train_selfsupervised","title":"train_selfsupervised","text":"<pre><code>train_selfsupervised(\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Self-supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images, which will also be targets</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>mask_shape</code>               (<code>int | Sequence[int] | NDArray</code>, default:                   <code>1</code> )           \u2013            <p>Shape of the blind spot mask, by default 1.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_selfsupervised(\n    self,\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n):\n    \"\"\"Self-supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images, which will also be targets\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    mask_shape : int | Sequence[int] | NDArray\n        Shape of the blind spot mask, by default 1.\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_trn = inp[trn_inds]\n    inp_tst = inp[tst_inds]\n\n    reg = self._get_regularization()\n    losses = self._train_n2v_pixelmask_small(\n        inp_trn,\n        inp_tst,\n        epochs=epochs,\n        mask_shape=mask_shape,\n        ratio_blind_spot=ratio_blind_spot,\n        algo=algo,\n        regularizer=reg,\n    )\n\n    self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.N2V.train_supervised","title":"train_supervised","text":"<pre><code>train_supervised(\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n)\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Learning algorithm to use, by default \"adam\"</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    algo: str = \"adam\",\n):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    # Create datasets\n    dset_trn = (inp[trn_inds], tgt[trn_inds])\n    dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n    reg = self._get_regularization()\n    losses = self._train_selfsimilar(dset_trn, dset_tst, epochs=epochs, algo=algo, regularizer=reg)\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.compute_scaling_selfsupervised","title":"compute_scaling_selfsupervised","text":"<pre><code>compute_scaling_selfsupervised(\n    inp: NDArray,\n) -&gt; DataScaleBias\n</code></pre> <p>Compute input data scaling and bias for self-supervised learning.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>Input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>An instance of DataScaleBias containing the computed scaling and bias values.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def compute_scaling_selfsupervised(inp: NDArray) -&gt; DataScaleBias:\n    \"\"\"\n    Compute input data scaling and bias for self-supervised learning.\n\n    Parameters\n    ----------\n    inp : NDArray\n        Input data.\n\n    Returns\n    -------\n    DataScaleBias\n        An instance of DataScaleBias containing the computed scaling and bias values.\n    \"\"\"\n    range_vals_inp = _get_normalization(inp, percentile=0.001)\n\n    sb = DataScaleBias()\n    sb.scale_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    sb.scale_out = sb.scale_tgt = sb.scale_inp\n\n    sb.bias_inp = range_vals_inp[2] * sb.scale_inp\n    sb.bias_out = sb.bias_tgt = sb.bias_inp\n\n    return sb\n</code></pre>"},{"location":"reference/autoden/algorithms/#autoden.algorithms.compute_scaling_supervised","title":"compute_scaling_supervised","text":"<pre><code>compute_scaling_supervised(\n    inp: NDArray, tgt: NDArray\n) -&gt; DataScaleBias\n</code></pre> <p>Compute input and target data scaling and bias for supervised learning.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>Input data.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>Target data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>An instance of DataScaleBias containing the computed scaling and bias values.</p> </li> </ul> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def compute_scaling_supervised(inp: NDArray, tgt: NDArray) -&gt; DataScaleBias:\n    \"\"\"\n    Compute input and target data scaling and bias for supervised learning.\n\n    Parameters\n    ----------\n    inp : NDArray\n        Input data.\n    tgt : NDArray\n        Target data.\n\n    Returns\n    -------\n    DataScaleBias\n        An instance of DataScaleBias containing the computed scaling and bias values.\n    \"\"\"\n    range_vals_inp = _get_normalization(inp, percentile=0.001)\n    range_vals_tgt = _get_normalization(tgt, percentile=0.001)\n\n    sb = DataScaleBias()\n    sb.scale_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    sb.scale_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n    sb.scale_out = sb.scale_tgt\n\n    sb.bias_inp = range_vals_inp[2] * sb.scale_inp\n    sb.bias_tgt = range_vals_tgt[2] * sb.scale_tgt\n    sb.bias_out = sb.bias_tgt\n\n    return sb\n</code></pre>"},{"location":"reference/autoden/cli/","title":"autoden.cli","text":""},{"location":"reference/autoden/cli/#autoden.cli","title":"cli","text":"<p>Module that contains the command line application.</p> <p>Functions:</p> <ul> <li> <code>get_parser</code>             \u2013              <p>Return the CLI argument parser.</p> </li> <li> <code>main</code>             \u2013              <p>Run the main program.</p> </li> </ul>"},{"location":"reference/autoden/cli/#autoden.cli.get_parser","title":"get_parser","text":"<pre><code>get_parser() -&gt; ArgumentParser\n</code></pre> <p>Return the CLI argument parser.</p> <p>Returns:</p> <ul> <li> <code>ArgumentParser</code>           \u2013            <p>An argparse parser.</p> </li> </ul> Source code in <code>src/autoden/cli.py</code> <pre><code>def get_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"\n    Return the CLI argument parser.\n\n    Returns\n    -------\n    argparse.ArgumentParser\n        An argparse parser.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"autoden\",\n        description=\"Denoise the given images, using deep-learning-based unsupervised or self-supervised algorithms.\",\n    )\n    parser.add_argument(\"algorithm\", choices=[\"N2N\", \"N2V\", \"DIP\"], help=\"Denoising algorithm to use\")\n    parser.add_argument(\n        \"--epochs\",\n        \"-e\",\n        type=int,\n        help=f\"Number of epochs to use, by default {DEFAULT_EPOCHS}.\",\n        metavar=\"E\",\n        default=DEFAULT_EPOCHS,\n    )\n    parser.add_argument(\n        \"--unet-levels\",\n        \"-l\",\n        type=int,\n        help=f\"Number of UNet levels to use, by default: {NetworkParamsUNet.DEFAULT_LEVELS}.\",\n        default=NetworkParamsUNet.DEFAULT_LEVELS,\n        metavar=\"L\",\n    )\n    parser.add_argument(\n        \"--unet-features\",\n        \"-f\",\n        type=int,\n        help=f\"Number of UNet features to use, by default: {NetworkParamsUNet.DEFAULT_FEATURES}.\",\n        default=NetworkParamsUNet.DEFAULT_FEATURES,\n        metavar=\"F\",\n    )\n    parser.add_argument(\n        \"--regularization\",\n        \"-r\",\n        type=float,\n        help=f\"Total Variation regularization value, by default: {DEFAULT_TV_VAL}.\",\n        default=DEFAULT_TV_VAL,\n        metavar=\"R\",\n    )\n    parser.add_argument(\"src_file\", nargs=\"+\", help=\"Path of each input image.\", type=argparse.FileType(\"rb\"))\n    parser.add_argument(\"dst_file\", help=\"Path of the output image.\", type=argparse.FileType(\"wb\"))\n    parser.add_argument(\"-V\", \"--version\", action=\"version\", version=f\"%(prog)s {debug.get_version()}\")\n    parser.add_argument(\"--debug-info\", action=_DebugInfo, help=\"Print debug information.\")\n    return parser\n</code></pre>"},{"location":"reference/autoden/cli/#autoden.cli.main","title":"main","text":"<pre><code>main(args: list[str] | None = None) -&gt; int\n</code></pre> <p>Run the main program.</p> <p>This function is executed when you type <code>autoden</code> or <code>python -m autoden</code>.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Arguments passed from the command line, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>An exit code.</p> </li> </ul> Source code in <code>src/autoden/cli.py</code> <pre><code>def main(args: list[str] | None = None) -&gt; int:\n    \"\"\"\n    Run the main program.\n\n    This function is executed when you type `autoden` or `python -m autoden`.\n\n    Parameters\n    ----------\n    args : list[str] | None\n        Arguments passed from the command line, by default None.\n\n    Returns\n    -------\n    int\n        An exit code.\n    \"\"\"\n    parser = get_parser()\n    opts = parser.parse_args(args=args)\n    # print(opts)  # noqa: WPS421 (side-effect in main is fine)\n\n    inp_imgs = [iio.imread(f) for f in opts.src_file]\n    if any(x.ndim &gt; 2 for x in inp_imgs):\n        print(\"Color images not supported, yet.\")\n        return 1\n    inp_imgs_stack = np.stack(inp_imgs, axis=0)\n\n    net_pars = NetworkParamsUNet(n_levels=opts.unet_levels, n_features=opts.unet_features)\n\n    if opts.algorithm.upper() == \"DIP\":\n        algo = DIP(model=net_pars, reg_val=opts.regularization)\n        inp_img = algo.train_unsupervised(inp_imgs_stack, epochs=opts.epochs)\n        out_img = algo.infer(inp_img)\n    elif opts.algorithm.upper() == \"N2N\":\n        if len(inp_imgs) &lt; 2:\n            print(f\"Not enough input images, only {len(inp_imgs)} were passed.\")\n            return 1\n\n        algo = N2N(model=net_pars, reg_val=opts.regularization)\n        algo.train_selfsupervised(inp_imgs_stack, epochs=opts.epochs)\n        out_img = algo.infer(inp_imgs_stack)\n    else:\n        print(f\"Not implemented support for algorithm {opts.algorithm} in command-line, yet.\")\n        return 1\n\n    iio.imwrite(opts.dst_file, out_img)\n    return 0\n</code></pre>"},{"location":"reference/autoden/debug/","title":"autoden.debug","text":""},{"location":"reference/autoden/debug/#autoden.debug","title":"debug","text":"<p>Debugging utilities.</p> <p>Classes:</p> <ul> <li> <code>Environment</code>           \u2013            <p>Dataclass to store environment information.</p> </li> <li> <code>Package</code>           \u2013            <p>Dataclass describing a Python package.</p> </li> <li> <code>Variable</code>           \u2013            <p>Dataclass describing an environment variable.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_debug_info</code>             \u2013              <p>Get debug/environment information.</p> </li> <li> <code>get_version</code>             \u2013              <p>Get version of the given distribution.</p> </li> <li> <code>print_debug_info</code>             \u2013              <p>Print debug/environment information.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Environment","title":"Environment  <code>dataclass</code>","text":"<pre><code>Environment(\n    interpreter_name: str,\n    interpreter_version: str,\n    interpreter_path: str,\n    platform: str,\n    packages: list[Package],\n    variables: list[Variable],\n)\n</code></pre> <p>Dataclass to store environment information.</p> <p>Attributes:</p> <ul> <li> <code>interpreter_name</code>               (<code>str</code>)           \u2013            <p>Python interpreter name.</p> </li> <li> <code>interpreter_path</code>               (<code>str</code>)           \u2013            <p>Path to Python executable.</p> </li> <li> <code>interpreter_version</code>               (<code>str</code>)           \u2013            <p>Python interpreter version.</p> </li> <li> <code>packages</code>               (<code>list[Package]</code>)           \u2013            <p>Installed packages.</p> </li> <li> <code>platform</code>               (<code>str</code>)           \u2013            <p>Operating System.</p> </li> <li> <code>variables</code>               (<code>list[Variable]</code>)           \u2013            <p>Environment variables.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.interpreter_name","title":"interpreter_name  <code>instance-attribute</code>","text":"<pre><code>interpreter_name: str\n</code></pre> <p>Python interpreter name.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.interpreter_path","title":"interpreter_path  <code>instance-attribute</code>","text":"<pre><code>interpreter_path: str\n</code></pre> <p>Path to Python executable.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.interpreter_version","title":"interpreter_version  <code>instance-attribute</code>","text":"<pre><code>interpreter_version: str\n</code></pre> <p>Python interpreter version.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages: list[Package]\n</code></pre> <p>Installed packages.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.platform","title":"platform  <code>instance-attribute</code>","text":"<pre><code>platform: str\n</code></pre> <p>Operating System.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: list[Variable]\n</code></pre> <p>Environment variables.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Package","title":"Package  <code>dataclass</code>","text":"<pre><code>Package(name: str, version: str)\n</code></pre> <p>Dataclass describing a Python package.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Package name.</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Package version.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Package.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Package name.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Package.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str\n</code></pre> <p>Package version.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Variable","title":"Variable  <code>dataclass</code>","text":"<pre><code>Variable(name: str, value: str)\n</code></pre> <p>Dataclass describing an environment variable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Variable name.</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>Variable value.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Variable.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Variable name.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Variable.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: str\n</code></pre> <p>Variable value.</p>"},{"location":"reference/autoden/debug/#autoden.debug.get_debug_info","title":"get_debug_info","text":"<pre><code>get_debug_info() -&gt; Environment\n</code></pre> <p>Get debug/environment information.</p> <p>Returns:     Environment information.</p> Source code in <code>src/autoden/debug.py</code> <pre><code>def get_debug_info() -&gt; Environment:\n    \"\"\"Get debug/environment information.\n\n    Returns:\n        Environment information.\n    \"\"\"\n    py_name, py_version = _interpreter_name_version()\n    packages = [\"auto-denoise\"]\n    variables = [\"PYTHONPATH\", *[var for var in os.environ if var.startswith(\"AUTO_DENOISE\")]]\n    return Environment(\n        interpreter_name=py_name,\n        interpreter_version=py_version,\n        interpreter_path=sys.executable,\n        platform=platform.platform(),\n        variables=[Variable(var, val) for var in variables if (val := os.getenv(var))],\n        packages=[Package(pkg, get_version(pkg)) for pkg in packages],\n    )\n</code></pre>"},{"location":"reference/autoden/debug/#autoden.debug.get_version","title":"get_version","text":"<pre><code>get_version(dist: str = 'auto-denoise') -&gt; str\n</code></pre> <p>Get version of the given distribution.</p> <p>Parameters:     dist: A distribution name.</p> <p>Returns:     A version number.</p> Source code in <code>src/autoden/debug.py</code> <pre><code>def get_version(dist: str = \"auto-denoise\") -&gt; str:\n    \"\"\"Get version of the given distribution.\n\n    Parameters:\n        dist: A distribution name.\n\n    Returns:\n        A version number.\n    \"\"\"\n    try:\n        return metadata.version(dist)\n    except metadata.PackageNotFoundError:\n        return \"0.0.0\"\n</code></pre>"},{"location":"reference/autoden/debug/#autoden.debug.print_debug_info","title":"print_debug_info","text":"<pre><code>print_debug_info() -&gt; None\n</code></pre> <p>Print debug/environment information.</p> Source code in <code>src/autoden/debug.py</code> <pre><code>def print_debug_info() -&gt; None:\n    \"\"\"Print debug/environment information.\"\"\"\n    info = get_debug_info()\n    print(f\"- __System__: {info.platform}\")\n    print(f\"- __Python__: {info.interpreter_name} {info.interpreter_version} ({info.interpreter_path})\")\n    print(\"- __Environment variables__:\")\n    for var in info.variables:\n        print(f\"  - `{var.name}`: `{var.value}`\")\n    print(\"- __Installed packages__:\")\n    for pkg in info.packages:\n        print(f\"  - `{pkg.name}` v{pkg.version}\")\n</code></pre>"},{"location":"reference/autoden/losses/","title":"autoden.losses","text":""},{"location":"reference/autoden/losses/#autoden.losses","title":"losses","text":"<p>Data losses definitions.</p> <p>Classes:</p> <ul> <li> <code>LossRegularizer</code>           \u2013            <p>Base class for the regularizer losses.</p> </li> <li> <code>LossSWTN</code>           \u2013            <p>Multi-level n-dimensional stationary wavelet transform loss function.</p> </li> <li> <code>LossTGV</code>           \u2013            <p>Total Generalized Variation loss function.</p> </li> <li> <code>LossTV</code>           \u2013            <p>Total Variation loss function.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_nd_wl_filters</code>             \u2013              <p>Generate all possible N-D separable wavelet filters.</p> </li> <li> <code>swt_nd</code>             \u2013              <p>Perform N-dimensional Stationary Wavelet Transform (SWT).</p> </li> </ul>"},{"location":"reference/autoden/losses/#autoden.losses.LossRegularizer","title":"LossRegularizer","text":"<p>               Bases: <code>MSELoss</code></p> <p>Base class for the regularizer losses.</p>"},{"location":"reference/autoden/losses/#autoden.losses.LossSWTN","title":"LossSWTN","text":"<pre><code>LossSWTN(\n    wl_dec_lo: Tensor,\n    wl_dec_hi: Tensor,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    levels: int = 2,\n    ndims: int = 2,\n    min_approx: bool = False,\n)\n</code></pre> <p>               Bases: <code>LossRegularizer</code></p> <p>Multi-level n-dimensional stationary wavelet transform loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute wavelet decomposition on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    wl_dec_lo: pt.Tensor,\n    wl_dec_hi: pt.Tensor,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    levels: int = 2,\n    ndims: int = 2,\n    min_approx: bool = False,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.wl_dec_lo = wl_dec_lo\n    self.wl_dec_hi = wl_dec_hi\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.levels = levels\n    self.ndims = ndims\n    self.min_approx = min_approx\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossSWTN.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute wavelet decomposition on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute wavelet decomposition on current batch.\"\"\"\n    _check_input_tensor(img, self.ndims)\n    axes = list(range(-(self.ndims + 1), 0))\n\n    coeffs = swt_nd(img, wl_dec_lo=self.wl_dec_lo, wl_dec_hi=self.wl_dec_hi, level=self.levels, normalize=\"scale\")\n\n    wl_val = []\n    first_ind = int(not self.min_approx)\n    for lvl_c in coeffs[first_ind:]:\n        coeff = pt.stack(lvl_c, dim=0)\n\n        if self.isotropic:\n            wl_val.append(pt.sqrt(pt.pow(coeff, 2).sum(dim=0)).sum(axes))\n        else:\n            wl_val.append(coeff.abs().sum(dim=0).sum(axes))\n\n    return self.lambda_val * pt.stack(wl_val, dim=0).sum(dim=0).mean() / ((self.levels + self.min_approx) ** 0.5)\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTGV","title":"LossTGV","text":"<pre><code>LossTGV(\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    ndims: int = 2,\n)\n</code></pre> <p>               Bases: <code>LossTV</code></p> <p>Total Generalized Variation loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute total variation statistics on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    ndims: int = 2,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.ndims = ndims\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTGV.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    _check_input_tensor(img, self.ndims)\n    axes = list(range(-(self.ndims + 1), 0))\n\n    diffs = [_differentiate(img, dim=dim, position=\"post\") for dim in range(-self.ndims, 0)]\n    diffdiffs = [_differentiate(d, dim=dim, position=\"pre\") for dim in range(-self.ndims, 0) for d in diffs]\n\n    if self.isotropic:\n        tv_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffs], dim=0).sum(dim=0))\n        jac_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffdiffs], dim=0).sum(dim=0))\n    else:\n        tv_val = pt.stack([d.abs() for d in diffs], dim=0).sum(dim=0)\n        jac_val = pt.stack([d.abs() for d in diffdiffs], dim=0).sum(dim=0)\n\n    return self.lambda_val * (tv_val.sum(axes).mean() + jac_val.sum(axes).mean() / 4)\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTV","title":"LossTV","text":"<pre><code>LossTV(\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    ndims: int = 2,\n)\n</code></pre> <p>               Bases: <code>LossRegularizer</code></p> <p>Total Variation loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute total variation statistics on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    ndims: int = 2,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.ndims = ndims\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTV.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    _check_input_tensor(img, self.ndims)\n    axes = list(range(-(self.ndims + 1), 0))\n\n    diffs = [_differentiate(img, dim=dim, position=\"post\") for dim in range(-self.ndims, 0)]\n    diffs = pt.stack(diffs, dim=0)\n\n    if self.isotropic:\n        # tv_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffs], dim=0).sum(dim=0))\n        tv_val = pt.sqrt(pt.pow(diffs, 2).sum(dim=0))\n    else:\n        # tv_val = pt.stack([d.abs() for d in diffs], dim=0).sum(dim=0)\n        tv_val = diffs.abs().sum(dim=0)\n\n    return self.lambda_val * tv_val.sum(axes).mean()\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.get_nd_wl_filters","title":"get_nd_wl_filters","text":"<pre><code>get_nd_wl_filters(\n    wl_lo: Tensor, wl_hi: Tensor, ndim: int\n) -&gt; list[Tensor]\n</code></pre> <p>Generate all possible N-D separable wavelet filters.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def get_nd_wl_filters(wl_lo: pt.Tensor, wl_hi: pt.Tensor, ndim: int) -&gt; list[pt.Tensor]:\n    \"\"\"\n    Generate all possible N-D separable wavelet filters.\n    \"\"\"\n    filters: list[pt.Tensor] = [wl_lo] + [wl_hi] * ndim\n    for _ in range(ndim - 1):\n        filters[0] = pt.outer(filters[0], wl_lo)\n    for ii in range(ndim):\n        new_shape = [1] * ndim\n        new_shape[ii] = -1\n        filters[ii + 1] = filters[ii + 1].reshape(new_shape)\n    return filters\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.swt_nd","title":"swt_nd","text":"<pre><code>swt_nd(\n    x: Tensor,\n    wl_dec_lo: Tensor,\n    wl_dec_hi: Tensor,\n    level: int = 1,\n    normalize: str | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Perform N-dimensional Stationary Wavelet Transform (SWT).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape (B, 1, *dims) where dims can be 1D, 2D, or 3D.</p> </li> <li> <code>wl_dec_lo</code>               (<code>Tensor</code>)           \u2013            <p>Low-pass wavelet decomposition filter.</p> </li> <li> <code>wl_dec_hi</code>               (<code>Tensor</code>)           \u2013            <p>High-pass wavelet decomposition filter.</p> </li> <li> <code>level</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of decomposition levels (default is 1).</p> </li> <li> <code>normalize</code>               (<code>str or None</code>, default:                   <code>None</code> )           \u2013            <p>Normalization method ('none', 'energy', or 'scale'). If None, no normalization is applied (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list of list of pt.Tensor</code>           \u2013            <p>List like [[approx], [detail_vols], ..., [detail_vols]].</p> </li> </ul> Notes <p>The function performs the SWT on the input tensor <code>x</code> using the specified wavelet filters and decomposition level. The output is a list of lists, where each inner list contains the decomposition volumes. The first inner list contains the approximation coefficients, and the subsequent inner lists contain the detail coefficients for each level.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def swt_nd(\n    x: pt.Tensor, wl_dec_lo: pt.Tensor, wl_dec_hi: pt.Tensor, level: int = 1, normalize: str | None = None\n) -&gt; list[list[pt.Tensor]]:\n    \"\"\"\n    Perform N-dimensional Stationary Wavelet Transform (SWT).\n\n    Parameters\n    ----------\n    x : pt.Tensor\n        Input tensor of shape (B, 1, *dims) where dims can be 1D, 2D, or 3D.\n    wl_dec_lo : pt.Tensor\n        Low-pass wavelet decomposition filter.\n    wl_dec_hi : pt.Tensor\n        High-pass wavelet decomposition filter.\n    level : int, optional\n        Number of decomposition levels (default is 1).\n    normalize : str or None, optional\n        Normalization method ('none', 'energy', or 'scale'). If None, no normalization is applied (default is None).\n\n    Returns\n    -------\n    list of list of pt.Tensor\n        List like [[approx], [detail_vols], ..., [detail_vols]].\n\n    Notes\n    -----\n    The function performs the SWT on the input tensor `x` using the specified wavelet filters and decomposition level.\n    The output is a list of lists, where each inner list contains the decomposition volumes. The first inner list contains\n    the approximation coefficients, and the subsequent inner lists contain the detail coefficients for each level.\n    \"\"\"\n    dims = x.shape[2:]\n    ndim = len(dims)\n    output = []\n    current = x\n\n    base_filters = get_nd_wl_filters(\n        wl_dec_lo.to(dtype=pt.float32, device=x.device), wl_dec_hi.to(dtype=pt.float32, device=x.device), ndim\n    )\n    for l in range(1, level + 1):\n        dilation = 2 ** (l - 1)\n\n        res_l = []\n        for filt in base_filters:\n            filt = _normalize_wl_filter(filt, l, normalize)\n            filt = filt.unsqueeze(0).unsqueeze(0)  # shape (1, 1, ...)\n\n            # Calculate padding for each dimension\n            filt_span_shape = (pt.tensor(filt.shape[2:]).flip(dims=[0]) - 1) * dilation\n            pad = [pt.tensor([k // 2, k - k // 2]) for k in filt_span_shape]\n            pad = pt.concatenate(pad)\n            padded = F.pad(current, pad.tolist(), mode='replicate')\n\n            if ndim == 1:\n                out = F.conv1d(padded, filt, dilation=dilation)\n            elif ndim == 2:\n                out = F.conv2d(padded, filt, dilation=dilation)\n            elif ndim == 3:\n                out = F.conv3d(padded, filt, dilation=dilation)\n            else:\n                raise ValueError(\"Only 1D, 2D, 3D supported\")\n\n            res_l.append(out)\n\n        # Split into approximation and details\n        current = res_l[0]  # recurse on approximation\n        output.append(res_l[1:])\n\n    output.append([current])\n\n    return list(reversed(output))\n</code></pre>"},{"location":"reference/autoden/models/","title":"autoden.models","text":""},{"location":"reference/autoden/models/#autoden.models","title":"models","text":"<p>Models sub-package.</p> <p>Implementation of models like DnCNN, MS-D net, UNet and a custom ResNet.</p> <p>Adapted from: https://github.com/ahendriksen/noise2inverse</p> <p>Modules:</p> <ul> <li> <code>config</code>           \u2013            <p>High level definition of CNN architectures.</p> </li> <li> <code>dncnn</code>           \u2013            </li> <li> <code>io</code>           \u2013            <p>IO module.</p> </li> <li> <code>msd</code>           \u2013            <p>Module implementing MS-D net.</p> </li> <li> <code>param_utils</code>           \u2013            <p>This module provides utility functions for handling PyTorch models, including</p> </li> <li> <code>resnet</code>           \u2013            </li> <li> <code>unet</code>           \u2013            <p>Implementation of a flexible U-net.</p> </li> </ul>"},{"location":"reference/autoden/models/config/","title":"autoden.models.config","text":""},{"location":"reference/autoden/models/config/#autoden.models.config","title":"config","text":"<p>High level definition of CNN architectures.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>NetworkParams</code>           \u2013            <p>Abstract base class for storing network parameters.</p> </li> <li> <code>NetworkParamsDnCNN</code>           \u2013            <p>Store DnCNN parameters.</p> </li> <li> <code>NetworkParamsMSD</code>           \u2013            <p>Store MS-D net parameters.</p> </li> <li> <code>NetworkParamsResnet</code>           \u2013            <p>Store Resnet parameters.</p> </li> <li> <code>NetworkParamsUNet</code>           \u2013            <p>Store UNet parameters.</p> </li> <li> <code>SerializableModel</code>           \u2013            <p>Protocol for serializable models.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_network</code>             \u2013              <p>Create and return a neural network model based on the provided network configuration.</p> </li> <li> <code>create_optimizer</code>             \u2013              <p>Instantiates the desired optimizer for the given model.</p> </li> </ul>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParams","title":"NetworkParams","text":"<pre><code>NetworkParams(\n    n_features: int,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for storing network parameters.</p> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get the associated model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(self, n_features: int, n_channels_in: int = 1, n_channels_out: int = 1) -&gt; None:\n    self.n_channels_in = n_channels_in\n    self.n_channels_out = n_channels_out\n    self.n_features = n_features\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParams.get_model","title":"get_model  <code>abstractmethod</code>","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get the associated model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>@abstractmethod\ndef get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get the associated model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsDnCNN","title":"NetworkParamsDnCNN","text":"<pre><code>NetworkParamsDnCNN(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 20,\n    n_features: int = 64,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store DnCNN parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels. Default is 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels. Default is 1.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Number of layers. Default is 20.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Number of features. Default is 64.</p> </li> <li> <code>kernel_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Size of the convolutional kernel. Default is 3.</p> </li> <li> <code>pad_mode</code>               (<code>str</code>, default:                   <code>'replicate'</code> )           \u2013            <p>Padding mode for the convolutional layers. Default is \"replicate\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a DnCNN model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 20,\n    n_features: int = 64,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the DnCNN network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_layers : int, optional\n        Number of layers. Default is 20.\n    n_features : int, optional\n        Number of features. Default is 64.\n    kernel_size : int, optional\n        Size of the convolutional kernel. Default is 3.\n    pad_mode : str, optional\n        Padding mode for the convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsDnCNN.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a DnCNN model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The DnCNN model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a DnCNN model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The DnCNN model.\n    \"\"\"\n    return DnCNN(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        kernel_size=self.kernel_size,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsMSD","title":"NetworkParamsMSD","text":"<pre><code>NetworkParamsMSD(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    dilations: Sequence[int] | NDArray[integer] = arange(\n        1, 4\n    ),\n    use_dilations: bool = True,\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store MS-D net parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels, by default 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels, by default 1.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>12</code> )           \u2013            <p>Number of layers in the network, by default 12.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of features, by default 1.</p> </li> <li> <code>dilations</code>               (<code>Sequence[int] | NDArray[integer]</code>, default:                   <code>arange(1, 4)</code> )           \u2013            <p>Dilation values for the network, by default np.arange(1, 4).</p> </li> <li> <code>use_dilations</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use dilations in the network, by default True.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a MS-D net model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    dilations: Sequence[int] | NDArray[np.integer] = np.arange(1, 4),\n    use_dilations: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the MS-D network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels, by default 1.\n    n_channels_out : int, optional\n        Number of output channels, by default 1.\n    n_layers : int, optional\n        Number of layers in the network, by default 12.\n    n_features : int, optional\n        Number of features, by default 1.\n    dilations : Sequence[int] | NDArray[np.integer], optional\n        Dilation values for the network, by default np.arange(1, 4).\n    use_dilations : bool, optional\n        Whether to use dilations in the network, by default True.\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_layers = n_layers\n    self.dilations = dilations\n    self.use_dilations = use_dilations\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsMSD.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a MS-D net model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a MS-D net model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n    return MSDnet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        dilations=list(self.dilations),\n        device=device,\n        use_dilations=self.use_dilations,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsResnet","title":"NetworkParamsResnet","text":"<pre><code>NetworkParamsResnet(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 10,\n    n_features: int = 24,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store Resnet parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels. Default is 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels. Default is 1.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of layers. Default is 10.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>24</code> )           \u2013            <p>Number of features. Default is 24.</p> </li> <li> <code>kernel_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Size of the convolutional kernel. Default is 3.</p> </li> <li> <code>pad_mode</code>               (<code>str</code>, default:                   <code>'replicate'</code> )           \u2013            <p>Padding mode for the convolutional layers. Default is \"replicate\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a Resnet model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 10,\n    n_features: int = 24,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the Resnet network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_layers : int, optional\n        Number of layers. Default is 10.\n    n_features : int, optional\n        Number of features. Default is 24.\n    kernel_size : int, optional\n        Size of the convolutional kernel. Default is 3.\n    pad_mode : str, optional\n        Padding mode for the convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsResnet.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a Resnet model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The Resnet model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a Resnet model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The Resnet model.\n    \"\"\"\n    return Resnet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        kernel_size=self.kernel_size,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsUNet","title":"NetworkParamsUNet","text":"<pre><code>NetworkParamsUNet(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_levels: int = DEFAULT_LEVELS,\n    n_features: int = DEFAULT_FEATURES,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store UNet parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels. Default is 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels. Default is 1.</p> </li> <li> <code>n_levels</code>               (<code>int</code>, default:                   <code>DEFAULT_LEVELS</code> )           \u2013            <p>Number of levels in the UNet. Default is 3.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>DEFAULT_FEATURES</code> )           \u2013            <p>Number of features in the UNet. Default is 32.</p> </li> <li> <code>n_channels_skip</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of skip connections channels. Default is None.</p> </li> <li> <code>bilinear</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bilinear interpolation. Default is True.</p> </li> <li> <code>pad_mode</code>               (<code>str</code>, default:                   <code>'replicate'</code> )           \u2013            <p>Padding mode for convolutional layers. Default is \"replicate\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a U-net model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_levels: int = DEFAULT_LEVELS,\n    n_features: int = DEFAULT_FEATURES,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the UNet network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_levels : int, optional\n        Number of levels in the UNet. Default is 3.\n    n_features : int, optional\n        Number of features in the UNet. Default is 32.\n    n_channels_skip : int, optional\n        Number of skip connections channels. Default is None.\n    bilinear : bool, optional\n        Whether to use bilinear interpolation. Default is True.\n    pad_mode : str, optional\n        Padding mode for convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_levels = n_levels\n    self.n_channels_skip = n_channels_skip\n    self.bilinear = bilinear\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsUNet.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a U-net model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The U-net model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a U-net model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The U-net model.\n    \"\"\"\n    return UNet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_features=self.n_features,\n        n_levels=self.n_levels,\n        n_channels_skip=self.n_channels_skip,\n        bilinear=self.bilinear,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.SerializableModel","title":"SerializableModel","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for serializable models.</p> <p>Provides a dictionary containing the initialization parameters of the model.</p>"},{"location":"reference/autoden/models/config/#autoden.models.config.create_network","title":"create_network","text":"<pre><code>create_network(\n    model: str | NetworkParams | Mapping | Module,\n    init_params: Mapping | None = None,\n    state_dict: Mapping | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Create and return a neural network model based on the provided network configuration.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Mapping | Module</code>)           \u2013            <p>The network configuration. It can be a string specifying the network type, an instance of <code>NetworkParams</code>, or an already instantiated <code>Module</code>. If a string is provided, it must be one of the supported network types: \"msd\", \"unet\", or \"dncnn\".</p> </li> <li> <code>state_dict</code>               (<code>Mapping | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing the state dictionary of the model. If provided, the model's parameters will be loaded from this dictionary. Default is None.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device to which the model should be moved. Default is \"cuda\" if CUDA is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The created neural network model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the provided network name is invalid or the network type is not supported.</p> </li> </ul> Notes <p>The function supports the following network types: - \"msd\": Multi-Scale Dense Network. - \"unet\": U-Net. - \"dncnn\": Denoising Convolutional Neural Network.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net = create_network(\"unet\")\n&gt;&gt;&gt; print(net)\nModel UNet - num. parameters: 1234567\n</code></pre> Source code in <code>src/autoden/models/config.py</code> <pre><code>def create_network(\n    model: str | NetworkParams | Mapping | Module,\n    init_params: Mapping | None = None,\n    state_dict: Mapping | None = None,\n    device: str = \"cuda\" if is_cuda_available() else \"cpu\",\n) -&gt; Module:\n    \"\"\"\n    Create and return a neural network model based on the provided network configuration.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | Mapping | Module\n        The network configuration. It can be a string specifying the network type,\n        an instance of `NetworkParams`, or an already instantiated `Module`.\n        If a string is provided, it must be one of the supported network types:\n        \"msd\", \"unet\", or \"dncnn\".\n    state_dict : Mapping | None, optional\n        A dictionary containing the state dictionary of the model. If provided,\n        the model's parameters will be loaded from this dictionary. Default is None.\n    device : str, optional\n        The device to which the model should be moved. Default is \"cuda\" if CUDA is available,\n        otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The created neural network model.\n\n    Raises\n    ------\n    ValueError\n        If the provided network name is invalid or the network type is not supported.\n\n    Notes\n    -----\n    The function supports the following network types:\n    - \"msd\": Multi-Scale Dense Network.\n    - \"unet\": U-Net.\n    - \"dncnn\": Denoising Convolutional Neural Network.\n\n    Examples\n    --------\n    &gt;&gt;&gt; net = create_network(\"unet\")\n    &gt;&gt;&gt; print(net)\n    Model UNet - num. parameters: 1234567\n    \"\"\"\n    if isinstance(model, Mapping):\n        if not all(key in model for key in (\"model_class\", \"init_params\", \"state_dict\")):\n            raise ValueError(\n                \"Malformed model state dictionary. Expected mandatory fields: 'model_class', 'init_params', and 'state_dict'\"\n            )\n        state_dict = model[\"state_dict\"]\n        init_params = model[\"init_params\"]\n        model = model[\"model_class\"]\n\n    if init_params is None:\n        init_params = dict()\n    else:\n        init_params = dict(**init_params)\n\n    for par in (\"device\", \"verbose\"):\n        if par in init_params:\n            del init_params[par]\n\n    if isinstance(model, str):\n        if model.lower() in (\"msd\", MSDnet.__name__.lower()):\n            model = NetworkParamsMSD(**init_params)\n        elif model.lower() == UNet.__name__.lower():\n            model = NetworkParamsUNet(**init_params)\n        elif model.lower() == DnCNN.__name__.lower():\n            model = NetworkParamsDnCNN(**init_params)\n        elif model.lower() == Resnet.__name__.lower():\n            model = NetworkParamsResnet(**init_params)\n        else:\n            raise ValueError(f\"Invalid model name: {model}\")\n\n    if isinstance(model, NetworkParams):\n        net = model.get_model(device)\n    elif isinstance(model, Module):\n        net = model.to(device=device)\n    else:\n        raise ValueError(f\"Invalid model type: {type(model)}\")\n\n    if state_dict is not None:\n        net.load_state_dict(state_dict)\n        net.to(device)  # Needed to ensure that the model lives in the correct device\n\n    print(f\"Model {net.__class__.__name__} - num. parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad)}\")\n    return net\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.create_optimizer","title":"create_optimizer","text":"<pre><code>create_optimizer(\n    network: Module,\n    algo: str = \"adam\",\n    learning_rate: float = 0.001,\n    weight_decay: float = 0.01,\n    optim_state: Mapping | None = None,\n) -&gt; Optimizer\n</code></pre> <p>Instantiates the desired optimizer for the given model.</p> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>Module</code>)           \u2013            <p>The network to train.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The requested optimizer, by default \"adam\".</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The desired learning rate, by default 1e-3.</p> </li> <li> <code>weight_decay</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The desired weight decay, by default 1e-2.</p> </li> <li> <code>optim_state</code>               (<code>Mapping | None</code>, default:                   <code>None</code> )           \u2013            <p>The state dictionary for the optimizer, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>The chosen optimizer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unsupported algorithm is requested.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def create_optimizer(\n    network: Module,\n    algo: str = \"adam\",\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-2,\n    optim_state: Mapping | None = None,\n) -&gt; Optimizer:\n    \"\"\"Instantiates the desired optimizer for the given model.\n\n    Parameters\n    ----------\n    network : torch.nn.Module\n        The network to train.\n    algo : str, optional\n        The requested optimizer, by default \"adam\".\n    learning_rate : float, optional\n        The desired learning rate, by default 1e-3.\n    weight_decay : float, optional\n        The desired weight decay, by default 1e-2.\n    optim_state : Mapping | None, optional\n        The state dictionary for the optimizer, by default None.\n\n    Returns\n    -------\n    torch.optim.Optimizer\n        The chosen optimizer.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported algorithm is requested.\n    \"\"\"\n    if algo.lower() == \"adam\":\n        optimizer = pt.optim.AdamW(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"sgd\":\n        optimizer = pt.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"rmsprop\":\n        optimizer = pt.optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"lbfgs\":\n        optimizer = pt.optim.LBFGS(network.parameters(), lr=learning_rate, max_iter=10000, history_size=50)\n    else:\n        raise ValueError(f\"Unknown algorithm: {algo}\")\n\n    if optim_state is not None:\n        optimizer.load_state_dict(dict(**optim_state))\n\n    return optimizer\n</code></pre>"},{"location":"reference/autoden/models/dncnn/","title":"autoden.models.dncnn","text":""},{"location":"reference/autoden/models/dncnn/#autoden.models.dncnn","title":"dncnn","text":"<p>Classes:</p> <ul> <li> <code>ConvBlock</code>           \u2013            <p>Convolution block: conv =&gt; BN =&gt; act.</p> </li> <li> <code>DnCNN</code>           \u2013            <p>Implementation of the DnCNN architecture from [1].</p> </li> </ul>"},{"location":"reference/autoden/models/dncnn/#autoden.models.dncnn.ConvBlock","title":"ConvBlock","text":"<pre><code>ConvBlock(\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    pad_mode: str = \"replicate\",\n    last_block: bool = False,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Convolution block: conv =&gt; BN =&gt; act.</p> Source code in <code>src/autoden/models/dncnn.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, kernel_size: int, pad_mode: str = \"replicate\", last_block: bool = False):\n    pad_size = (kernel_size - 1) // 2\n    if last_block:\n        post_conv = []\n    else:\n        post_conv = [nn.BatchNorm2d(out_ch), nn.LeakyReLU(0.2, inplace=True)]\n    super().__init__(\n        nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=False),\n        *post_conv,\n    )\n</code></pre>"},{"location":"reference/autoden/models/dncnn/#autoden.models.dncnn.DnCNN","title":"DnCNN","text":"<pre><code>DnCNN(\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 20,\n    n_features: int = 32,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if is_available() else \"cpu\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Implementation of the DnCNN architecture from [1].</p> <p>[1] Zhang, et al., \"Beyond a Gaussian denoiser: Residual learning of deep CNN     for image denoising,\" IEEE Trans. on Image Processing, 2017.</p> Source code in <code>src/autoden/models/dncnn.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 20,\n    n_features: int = 32,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n):\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n    # From zhang-2017-beyon-gauss-denois:\n    #\n    #  Thus, for Gaussian denoising with a certain noise level, we\n    #  set the receptive field size of DnCNN to 35 \u00d7 35 with the\n    #  corresponding depth of 17. For other general image denoising\n    #  tasks, we adopt a larger receptive field and set the depth\n    #  to be 20.\n    #\n    # Hence, we set the standard depth to 20.\n    layers = [\n        ConvBlock(\n            n_channels_in if i_l == 0 else n_features,\n            n_channels_out if i_l == (n_layers - 1) else n_features,\n            kernel_size=kernel_size,\n            pad_mode=pad_mode,\n            last_block=(i_l == (n_layers - 1)),\n        )\n        for i_l in range(n_layers)\n    ]\n\n    super().__init__(*layers)\n    self.init_params = init_params\n    self.device = device\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/io/","title":"autoden.models.io","text":""},{"location":"reference/autoden/models/io/#autoden.models.io","title":"io","text":"<p>IO module.</p> <p>Functions:</p> <ul> <li> <code>load_model_state</code>             \u2013              <p>Load a model from disk.</p> </li> <li> <code>save_model</code>             \u2013              <p>Save a model and optionally its optimizer state to a file.</p> </li> <li> <code>save_model_state</code>             \u2013              <p>Save a model's state to disk.</p> </li> </ul>"},{"location":"reference/autoden/models/io/#autoden.models.io.load_model_state","title":"load_model_state","text":"<pre><code>load_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int | None = None,\n) -&gt; Mapping\n</code></pre> <p>Load a model from disk.</p> <p>Parameters:</p> <ul> <li> <code>save_epochs_dir</code>               (<code>str | Path</code>)           \u2013            <p>The director where the models are saved</p> </li> <li> <code>epoch_num</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The epoch number or if None/-1 the best state will be loaded, by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Mapping</code>           \u2013            <p>The loaded model state and possibly an optimizer state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the directory does not exist or the requested model is not available.</p> </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def load_model_state(save_epochs_dir: str | Path, epoch_num: int | None = None) -&gt; Mapping:\n    \"\"\"Load a model from disk.\n\n    Parameters\n    ----------\n    save_epochs_dir : str | Path\n        The director where the models are saved\n    epoch_num : int | None, optional\n        The epoch number or if None/-1 the best state will be loaded, by default None\n\n    Returns\n    -------\n    Mapping\n        The loaded model state and possibly an optimizer state.\n\n    Raises\n    ------\n    ValueError\n        When the directory does not exist or the requested model is not available.\n    \"\"\"\n    epochs_base_path = Path(save_epochs_dir) / \"weights\"\n    if not epochs_base_path.exists():\n        raise ValueError(f\"Directory of the model state {epochs_base_path} does not exist!\")\n\n    if epoch_num is None or epoch_num == -1:\n        state_path = epochs_base_path / \"weights.pt\"\n    else:\n        state_path = epochs_base_path / f\"weights_epoch_{epoch_num}.pt\"\n    if not state_path.exists():\n        raise ValueError(f\"Model state {state_path} does not exist!\")\n\n    print(f\"Loading state path: {state_path}\")\n    return pt.load(state_path)\n</code></pre>"},{"location":"reference/autoden/models/io/#autoden.models.io.save_model","title":"save_model","text":"<pre><code>save_model(\n    dst_file: str | Path,\n    model: Module,\n    optim_state: Mapping | None = None,\n    epoch_num: int = 0,\n) -&gt; None\n</code></pre> <p>Save a model and optionally its optimizer state to a file.</p> <p>Parameters:</p> <ul> <li> <code>dst_file</code>               (<code>str or Path</code>)           \u2013            <p>The destination file path where the model will be saved.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to be saved. It must implement the <code>SerializableModel</code> protocol.</p> </li> <li> <code>optim_state</code>               (<code>Mapping</code>, default:                   <code>None</code> )           \u2013            <p>The state of the optimizer to be saved. Default is None.</p> </li> <li> <code>epoch_num</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current epoch number. Default is 0.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model does not implement the <code>SerializableModel</code> protocol.</p> </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def save_model(dst_file: str | Path, model: Module, optim_state: Mapping | None = None, epoch_num: int = 0) -&gt; None:\n    \"\"\"\n    Save a model and optionally its optimizer state to a file.\n\n    Parameters\n    ----------\n    dst_file : str or Path\n        The destination file path where the model will be saved.\n    model : Module\n        The model to be saved. It must implement the `SerializableModel` protocol.\n    optim_state : Mapping, optional\n        The state of the optimizer to be saved. Default is None.\n    epoch_num : int, optional\n        The current epoch number. Default is 0.\n\n    Raises\n    ------\n    ValueError\n        If the model does not implement the `SerializableModel` protocol.\n    \"\"\"\n    if not isinstance(model, SerializableModel):\n        raise ValueError(\"The model needs to implement the protocol SerializableModel, in order to be writable to disk\")\n\n    pt.save(\n        {\n            \"model_class\": model.__class__.__name__,\n            \"init_params\": model.init_params,\n            \"epoch\": epoch_num,\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optim_state,\n        },\n        dst_file,\n    )\n</code></pre>"},{"location":"reference/autoden/models/io/#autoden.models.io.save_model_state","title":"save_model_state","text":"<pre><code>save_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int,\n    model: Module,\n    optim_state: Mapping | None = None,\n    is_best: bool = False,\n) -&gt; None\n</code></pre> <p>Save a model's state to disk.</p> <p>This function saves the state of a model and optionally its optimizer to disk. The model state is saved in a directory specified by <code>save_epochs_dir</code>. If <code>is_best</code> is True, the model state is saved as \"weights.pt\". Otherwise, it is saved with a filename that includes the epoch number.</p> <p>Parameters:</p> <ul> <li> <code>save_epochs_dir</code>               (<code>str | Path</code>)           \u2013            <p>The directory where to save the model state.</p> </li> <li> <code>epoch_num</code>               (<code>int</code>)           \u2013            <p>The epoch number.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model whose state is to be saved.</p> </li> <li> <code>optim_state</code>               (<code>Mapping</code>, default:                   <code>None</code> )           \u2013            <p>The optimizer state to save, by default None.</p> </li> <li> <code>is_best</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether it is the best fitted model, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def save_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int,\n    model: Module,\n    optim_state: Mapping | None = None,\n    is_best: bool = False,\n) -&gt; None:\n    \"\"\"Save a model's state to disk.\n\n    This function saves the state of a model and optionally its optimizer to disk.\n    The model state is saved in a directory specified by `save_epochs_dir`. If\n    `is_best` is True, the model state is saved as \"weights.pt\". Otherwise, it is\n    saved with a filename that includes the epoch number.\n\n    Parameters\n    ----------\n    save_epochs_dir : str | Path\n        The directory where to save the model state.\n    epoch_num : int\n        The epoch number.\n    model : Module\n        The model whose state is to be saved.\n    optim_state : Mapping, optional\n        The optimizer state to save, by default None.\n    is_best : bool, optional\n        Whether it is the best fitted model, by default False.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    epochs_base_path = Path(save_epochs_dir) / \"weights\"\n    epochs_base_path.mkdir(parents=True, exist_ok=True)\n\n    dst_file = epochs_base_path / (\"weights.pt\" if is_best else f\"weights_epoch_{epoch_num}.pt\")\n    save_model(dst_file=dst_file, model=model, optim_state=optim_state, epoch_num=epoch_num)\n</code></pre>"},{"location":"reference/autoden/models/msd/","title":"autoden.models.msd","text":""},{"location":"reference/autoden/models/msd/#autoden.models.msd","title":"msd","text":"<p>Module implementing MS-D net.</p> <p>Classes:</p> <ul> <li> <code>DilatedConvBlock</code>           \u2013            <p>Dilated convolution block (dilated_conv =&gt; BN =&gt; ReLU).</p> </li> <li> <code>MSDDilBlock</code>           \u2013            <p>MS-D Block containing the sequence of dilated convolutional layers.</p> </li> <li> <code>MSDSampBlock</code>           \u2013            <p>MS-D Block containing the sequence of dilated convolutional layers.</p> </li> <li> <code>MSDnet</code>           \u2013            <p>Simple MS-D net implementation.</p> </li> <li> <code>SamplingConvBlock</code>           \u2013            <p>Down-sampling convolution module (down-samp =&gt; conv =&gt; BN =&gt; ReLU =&gt; up-samp).</p> </li> </ul>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.DilatedConvBlock","title":"DilatedConvBlock","text":"<pre><code>DilatedConvBlock(\n    in_ch: int,\n    out_ch: int,\n    dilation: int = 1,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Dilated convolution block (dilated_conv =&gt; BN =&gt; ReLU).</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, dilation: int = 1, pad_mode: str = \"replicate\") -&gt; None:\n    super().__init__(\n        nn.Conv2d(in_ch, out_ch, 3, padding=dilation, dilation=dilation, padding_mode=pad_mode),\n        nn.BatchNorm2d(out_ch),\n        nn.LeakyReLU(0.2, inplace=True),\n    )\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.MSDDilBlock","title":"MSDDilBlock","text":"<pre><code>MSDDilBlock(\n    n_channels_in: int,\n    n_features: int,\n    n_layers: int,\n    dilations: Sequence[int],\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MS-D Block containing the sequence of dilated convolutional layers.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(self, n_channels_in: int, n_features: int, n_layers: int, dilations: Sequence[int]) -&gt; None:\n    super().__init__()\n    self.n_features = n_features\n    self.n_layers = n_layers\n    self.dilations = dilations\n    convs = [\n        DilatedConvBlock(n_channels_in + n_features * ii, n_features, dilation=self._layer_dilation(ii))\n        for ii in range(n_layers)\n    ]\n    self.convs = nn.ModuleList(convs)\n    self.n_ch_in = n_channels_in\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.MSDSampBlock","title":"MSDSampBlock","text":"<pre><code>MSDSampBlock(\n    n_channels_in: int,\n    n_features: int,\n    n_layers: int,\n    dilations: Sequence[int],\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MS-D Block containing the sequence of dilated convolutional layers.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(self, n_channels_in: int, n_features: int, n_layers: int, dilations: Sequence[int]) -&gt; None:\n    super().__init__()\n    self.n_features = n_features\n    self.n_layers = n_layers\n    self.dilations = dilations\n    convs = [\n        SamplingConvBlock(n_channels_in + n_features * ii, n_features, samp_factor=self._layer_sampling(ii))\n        for ii in range(n_layers)\n    ]\n    self.convs = nn.ModuleList(convs)\n    self.n_ch_in = n_channels_in\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.MSDnet","title":"MSDnet","text":"<pre><code>MSDnet(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    dilations: Sequence[int] = [1, 2, 3, 4],\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    use_dilations: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple MS-D net implementation.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    dilations: Sequence[int] = [1, 2, 3, 4],\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    use_dilations: bool = True,\n) -&gt; None:\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n\n    super().__init__()\n    self.init_params = init_params\n    self.device = device\n\n    if use_dilations:\n        self.msd_block = MSDDilBlock(n_channels_in, n_features, n_layers, dilations)\n    else:\n        self.msd_block = MSDSampBlock(n_channels_in, n_features, n_layers, dilations)\n    self.outc = nn.Conv2d(n_channels_in + n_features * n_layers, n_channels_out, kernel_size=1)\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.SamplingConvBlock","title":"SamplingConvBlock","text":"<pre><code>SamplingConvBlock(\n    in_ch: int, out_ch: int, samp_factor: int = 1\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Down-sampling convolution module (down-samp =&gt; conv =&gt; BN =&gt; ReLU =&gt; up-samp).</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, samp_factor: int = 1) -&gt; None:\n    if samp_factor &gt; 1:\n        pre = [nn.AvgPool2d(samp_factor)]\n        post = [nn.Upsample(scale_factor=samp_factor, mode=\"bilinear\", align_corners=True)]\n    else:\n        pre = post = []\n    super().__init__(\n        *pre, nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.LeakyReLU(0.2, inplace=True), *post\n    )\n</code></pre>"},{"location":"reference/autoden/models/param_utils/","title":"autoden.models.param_utils","text":""},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils","title":"param_utils","text":"<p>This module provides utility functions for handling PyTorch models, including optimization, parameter management, and gradient retrieval.</p> <p>Functions:     create_optimizer: Instantiates the desired optimizer for the given model.     get_num_parameters: Returns the number of trainable parameters in the model.     set_parameters: Sets the parameters of the model from a given array of values.     get_parameters: Gets the parameters of the model.     get_gradients: Gets the gradients of the model parameters.</p> <p>Functions:</p> <ul> <li> <code>fix_invalid_gradient_values</code>             \u2013              <p>Fixes invalid gradient values in the model's parameters.</p> </li> <li> <code>get_gradients</code>             \u2013              <p>Gets the gradients of the model parameters.</p> </li> <li> <code>get_num_parameters</code>             \u2013              <p>Returns the number of trainable parameters in the model.</p> </li> <li> <code>get_parameters</code>             \u2013              <p>Gets the parameters of the model.</p> </li> <li> <code>set_parameters</code>             \u2013              <p>Sets the parameters of the model from a given array of values.</p> </li> </ul>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.fix_invalid_gradient_values","title":"fix_invalid_gradient_values","text":"<pre><code>fix_invalid_gradient_values(model: Module) -&gt; None\n</code></pre> <p>Fixes invalid gradient values in the model's parameters.</p> <p>This function iterates over all parameters of the given model and sets the gradient values to zero where they are not finite (i.e., NaN or infinity).</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The neural network model whose gradient values need to be fixed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>This function modifies the gradients in place and does not return anything.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def fix_invalid_gradient_values(model: nn.Module) -&gt; None:\n    \"\"\"\n    Fixes invalid gradient values in the model's parameters.\n\n    This function iterates over all parameters of the given model and sets the\n    gradient values to zero where they are not finite (i.e., NaN or infinity).\n\n    Parameters\n    ----------\n    model : nn.Module\n        The neural network model whose gradient values need to be fixed.\n\n    Returns\n    -------\n    None\n        This function modifies the gradients in place and does not return anything.\n    \"\"\"\n    for pars in model.parameters():\n        if pars.grad is not None:\n            pars.grad[pt.logical_not(pt.isfinite(pars.grad))] = 0.0\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.get_gradients","title":"get_gradients","text":"<pre><code>get_gradients(\n    model: Module, flatten: bool = True\n) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]\n</code></pre> <p>Gets the gradients of the model parameters.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to get the gradients from.</p> </li> <li> <code>flatten</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, flattens the gradients, by default True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]</code>           \u2013            <p>A tuple containing the gradient values and their shapes.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_gradients(model: nn.Module, flatten: bool = True) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]:\n    \"\"\"Gets the gradients of the model parameters.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to get the gradients from.\n    flatten : bool, optional\n        If True, flattens the gradients, by default True.\n\n    Returns\n    -------\n    tuple[numpy.typing.NDArray, Sequence[tuple[str, Sequence[int]]]]\n        A tuple containing the gradient values and their shapes.\n    \"\"\"\n    grads = []\n    info = []\n    for name, params in model.named_parameters():\n        if params.grad is not None:\n            g1 = params.grad.view(-1)\n            grad = g1.detach().cpu().numpy().copy()\n            if flatten:\n                grad = grad.flatten()\n            grads.append(grad)\n            info.append((name, [*params.shape]))\n    return np.concatenate(grads), info\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.get_num_parameters","title":"get_num_parameters","text":"<pre><code>get_num_parameters(\n    model: Module, verbose: bool = False\n) -&gt; int\n</code></pre> <p>Returns the number of trainable parameters in the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to count the parameters for.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, prints the number of parameters, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of trainable parameters.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_num_parameters(model: nn.Module, verbose: bool = False) -&gt; int:\n    \"\"\"Returns the number of trainable parameters in the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to count the parameters for.\n    verbose : bool, optional\n        If True, prints the number of parameters, by default False.\n\n    Returns\n    -------\n    int\n        The number of trainable parameters.\n    \"\"\"\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if verbose:\n        print(f\"Model {model.__class__.__name__} - num. parameters: {num_params}\")\n    return num_params\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters(\n    model: Module,\n    parameter_type: str | None = None,\n    filter_params: bool = True,\n) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]\n</code></pre> <p>Gets the parameters of the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to get the parameters from.</p> </li> <li> <code>parameter_type</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The type of parameters to filter, by default None.</p> </li> <li> <code>filter_params</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, filters the parameters based on the parameter_type, by default True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]</code>           \u2013            <p>A tuple containing the parameter values and their shapes.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_parameters(\n    model: nn.Module, parameter_type: str | None = None, filter_params: bool = True\n) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]:\n    \"\"\"Gets the parameters of the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to get the parameters from.\n    parameter_type : str | None, optional\n        The type of parameters to filter, by default None.\n    filter_params : bool, optional\n        If True, filters the parameters based on the parameter_type, by default True.\n\n    Returns\n    -------\n    tuple[numpy.typing.NDArray, Sequence[tuple[str, Sequence[int]]]]\n        A tuple containing the parameter values and their shapes.\n    \"\"\"\n    vals = []\n    info = []\n    for name, params in model.named_parameters():\n        p1 = params.view(-1)\n        if parameter_type is None or name.split(\".\")[-1] == parameter_type.lower():\n            vals.append(p1.detach().cpu().numpy().copy().flatten())\n            info.append((name, [*params.shape]))\n        elif not filter_params:\n            vals.append(np.zeros_like(p1.detach().cpu().numpy()).flatten())\n            info.append((name, [*params.shape]))\n    return np.concatenate(vals), info\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.set_parameters","title":"set_parameters","text":"<pre><code>set_parameters(\n    model: Module,\n    values: NDArray,\n    info: Sequence[tuple[str, Sequence[int]]],\n) -&gt; None\n</code></pre> <p>Sets the parameters of the model from a given array of values.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to set the parameters for.</p> </li> <li> <code>values</code>               (<code>NDArray</code>)           \u2013            <p>The array of parameter values.</p> </li> <li> <code>info</code>               (<code>Sequence[tuple[str, Sequence[int]]]</code>)           \u2013            <p>Information about the parameter names and shapes.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the length of the values array does not match the total number of parameters.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def set_parameters(model: nn.Module, values: NDArray, info: Sequence[tuple[str, Sequence[int]]]) -&gt; None:\n    \"\"\"Sets the parameters of the model from a given array of values.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to set the parameters for.\n    values : numpy.typing.NDArray\n        The array of parameter values.\n    info : Sequence[tuple[str, Sequence[int]]]\n        Information about the parameter names and shapes.\n\n    Raises\n    ------\n    ValueError\n        If the length of the values array does not match the total number of parameters.\n    \"\"\"\n    if len(values) != sum([np.prod(v) for _, v in info]):\n        raise ValueError(\"Inconsistent length of values array and parameters shapes\")\n    state_dict = model.state_dict()\n    params_start = 0\n    for name, p_shape in info:\n        params_end = params_start + np.prod(p_shape)\n        state_dict[name][:] = pt.tensor(values[params_start:params_end].reshape(p_shape))\n        params_start = params_end\n</code></pre>"},{"location":"reference/autoden/models/resnet/","title":"autoden.models.resnet","text":""},{"location":"reference/autoden/models/resnet/#autoden.models.resnet","title":"resnet","text":"<p>Classes:</p> <ul> <li> <code>ResBlock</code>           \u2013            <p>Residual block: conv =&gt; BN =&gt; act. =&gt; conv =&gt; BN =&gt; residual link =&gt; (optional) act.</p> </li> <li> <code>Resnet</code>           \u2013            <p>Implementation of the Resnet architecture.</p> </li> </ul>"},{"location":"reference/autoden/models/resnet/#autoden.models.resnet.ResBlock","title":"ResBlock","text":"<pre><code>ResBlock(\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    pad_mode: str = \"replicate\",\n    last_block: bool = False,\n    bias: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Residual block: conv =&gt; BN =&gt; act. =&gt; conv =&gt; BN =&gt; residual link =&gt; (optional) act.</p> Source code in <code>src/autoden/models/resnet.py</code> <pre><code>def __init__(\n    self,\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    pad_mode: str = \"replicate\",\n    last_block: bool = False,\n    bias: bool = True,\n):\n    super().__init__()\n    pad_size = (kernel_size - 1) // 2\n    self.main_seq = nn.ModuleList(\n        [\n            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=bias),\n            nn.BatchNorm2d(out_ch),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=bias),\n            nn.BatchNorm2d(out_ch),\n        ]\n    )\n    self.scale_inp = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=bias) if in_ch != out_ch else None\n    self.post_res = nn.LeakyReLU(0.2, inplace=True) if not last_block else None\n</code></pre>"},{"location":"reference/autoden/models/resnet/#autoden.models.resnet.Resnet","title":"Resnet","text":"<pre><code>Resnet(\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 10,\n    n_features: int = 32,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if is_available() else \"cpu\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Implementation of the Resnet architecture.</p> Source code in <code>src/autoden/models/resnet.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 10,\n    n_features: int = 32,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n):\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n\n    layers = [\n        ResBlock(\n            n_channels_in if i_l == 0 else n_features,\n            n_channels_out if i_l == (n_layers - 1) else n_features,\n            kernel_size=kernel_size,\n            pad_mode=pad_mode,\n            last_block=(i_l == (n_layers - 1)),\n        )\n        for i_l in range(n_layers)\n    ]\n\n    super().__init__(*layers)\n    self.init_params = init_params\n    self.device = device\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/unet/","title":"autoden.models.unet","text":""},{"location":"reference/autoden/models/unet/#autoden.models.unet","title":"unet","text":"<p>Implementation of a flexible U-net.</p> <p>Originally inspired by: https://github.com/milesial/Pytorch-UNet</p> <p>Classes:</p> <ul> <li> <code>ConvBlock</code>           \u2013            <p>Convolution block: conv =&gt; BN =&gt; act.</p> </li> <li> <code>DoubleConv</code>           \u2013            <p>Double convolution (conv =&gt; BN =&gt; ReLU) * 2.</p> </li> <li> <code>DownBlock</code>           \u2013            <p>Down-scaling block.</p> </li> <li> <code>UNet</code>           \u2013            <p>U-net model.</p> </li> <li> <code>UpBlock</code>           \u2013            <p>Up-scaling block.</p> </li> </ul>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.ConvBlock","title":"ConvBlock","text":"<pre><code>ConvBlock(\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    stride: int = 1,\n    dilation: int = 1,\n    pad_mode: str = \"replicate\",\n    residual: bool = False,\n    bias: bool = True,\n    last_block: bool = False,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Convolution block: conv =&gt; BN =&gt; act.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(\n    self,\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    stride: int = 1,\n    dilation: int = 1,\n    pad_mode: str = \"replicate\",\n    residual: bool = False,\n    bias: bool = True,\n    last_block: bool = False,\n):\n    pad_size = (kernel_size - 1) // 2 + (dilation - 1)\n    if last_block:\n        post_conv = []\n    else:\n        post_conv = [nn.BatchNorm2d(out_ch), nn.LeakyReLU(0.2, inplace=True)]\n    super().__init__(\n        nn.Conv2d(\n            in_ch,\n            out_ch,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            padding=pad_size,\n            padding_mode=pad_mode,\n            bias=bias,\n        ),\n        *post_conv,\n    )\n    if residual and in_ch != out_ch:\n        print(f\"Warning: Residual connections not available when {in_ch=} is different from {out_ch=}\")\n        residual = False\n    self.residual = residual\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.DoubleConv","title":"DoubleConv","text":"<pre><code>DoubleConv(\n    in_ch: int, out_ch: int, pad_mode: str = \"replicate\"\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Double convolution (conv =&gt; BN =&gt; ReLU) * 2.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, pad_mode: str = \"replicate\"):\n    super().__init__(\n        ConvBlock(in_ch, out_ch, kernel_size=3, pad_mode=pad_mode),\n        ConvBlock(out_ch, out_ch, kernel_size=1),\n    )\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.DownBlock","title":"DownBlock","text":"<pre><code>DownBlock(\n    in_ch: int,\n    out_ch: int,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Down-scaling block.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, bilinear: bool = True, pad_mode: str = \"replicate\"):\n    if bilinear:\n        down_block = [nn.AvgPool2d(2)]\n    else:\n        down_block = [ConvBlock(in_ch, in_ch, kernel_size=2, stride=2)]\n    super().__init__(\n        *down_block,\n        DoubleConv(in_ch, out_ch, pad_mode=pad_mode),\n    )\n    self.pad_mode = pad_mode.lower()\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.UNet","title":"UNet","text":"<pre><code>UNet(\n    n_channels_in: int,\n    n_channels_out: int,\n    n_features: int = 32,\n    n_levels: int = 3,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>U-net model.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int,\n    n_channels_out: int,\n    n_features: int = 32,\n    n_levels: int = 3,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    verbose: bool = False,\n):\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n\n    super().__init__()\n    self.init_params = init_params\n    self.device = device\n\n    if pad_mode.lower() not in PAD_MODES:\n        raise ValueError(f\"Padding mode {pad_mode} should be one of {PAD_MODES}\")\n    self.pad_mode = pad_mode.lower()\n\n    encoder, decoder = _compute_architecture(\n        n_levels=n_levels, n_features=n_features, n_skip=n_channels_skip, verbose=verbose\n    )\n\n    self.in_layer = DoubleConv(n_channels_in, n_features, pad_mode=pad_mode)\n    self.encoder_layers = nn.ModuleList([DownBlock(*lvl, bilinear=bilinear, pad_mode=pad_mode) for lvl in encoder])\n    self.decoder_layers = nn.ModuleList([UpBlock(*lvl, bilinear=bilinear, pad_mode=pad_mode) for lvl in decoder])\n    self.out_layer = ConvBlock(n_features, n_channels_out, kernel_size=1, last_block=True, pad_mode=pad_mode)\n\n    if verbose:\n        print(\n            f\"Model {self.__class__.__name__} - \"\n            f\"num. parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\"\n        )\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.UpBlock","title":"UpBlock","text":"<pre><code>UpBlock(\n    in_ch: int,\n    skip_ch: int | None,\n    out_ch: int,\n    bilinear: bool = False,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Up-scaling block.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(self, in_ch: int, skip_ch: int | None, out_ch: int, bilinear: bool = False, pad_mode: str = \"replicate\"):\n    super().__init__()\n    self.skip_ch = skip_ch\n\n    # Bilinear up-sampling tends to give better results, and use fewer weights\n    if bilinear:\n        self.up_block = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n    else:\n        self.up_block = nn.ConvTranspose2d(in_ch, in_ch, kernel_size=2, stride=2)\n\n    if skip_ch is not None:\n        n_skip = skip_ch\n        if skip_ch &gt; 0:\n            self.skip_block = ConvBlock(in_ch, skip_ch, kernel_size=1)\n    else:\n        n_skip = in_ch\n\n    self.conv_block = DoubleConv(in_ch + n_skip, out_ch, pad_mode=pad_mode)\n</code></pre>"},{"location":"tutorials/denoising/","title":"Denoising Images","text":"<p>Auto-Denoise (autoden) provides implementations for a variety of unsupervised and self-supervised Convolutional Neural Network (CNN) denoising methods. This tutorial will guide you through setting up the data, training different denoisers, performing inference, and visualizing the results.</p>"},{"location":"tutorials/denoising/#setting-up-the-data","title":"Setting Up the Data","text":"<p>First, we need to set up the data to be used for training and testing the denoisers. We will use the <code>skimage</code> library to generate a noisy image and create multiple noisy versions of it.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport skimage.color as skc\nimport skimage.data as skd\nimport skimage.transform as skt\nfrom numpy.typing import NDArray\nfrom tqdm.auto import tqdm\nimport autoden as ad\n\nUSE_CAMERA_MAN = True\nNUM_IMGS_TRN = 4\nNUM_IMGS_TST = 2\nNUM_IMGS_TOT = NUM_IMGS_TRN + NUM_IMGS_TST\n\nEPOCHS = 1024\nREG_TV_VAL = 1e-7\n\nif USE_CAMERA_MAN:\n    img_orig = skd.camera()\n    img_orig = skt.downscale_local_mean(img_orig, 4)\nelse:\n    img_orig = skd.cat()\n    img_orig = skc.rgb2gray(img_orig)\n    img_orig *= 255 / img_orig.max()\n\nimgs_noisy: NDArray = np.stack(\n    [(img_orig + 20 * np.random.randn(*img_orig.shape)) for _ in tqdm(range(NUM_IMGS_TOT), desc=\"Create noisy images\")],\n    axis=0,\n)\ntst_inds = np.arange(NUM_IMGS_TRN, NUM_IMGS_TOT)\n\nprint(f\"Img orig -&gt; [{img_orig.min()}, {img_orig.max()}], Img noisy -&gt; [{imgs_noisy[0].min()}, {imgs_noisy[0].max()}]\")\nprint(f\"Img shape: {img_orig.shape}\")\n</code></pre> <p>Training vs testing images</p> <p>Certain algorithms require to allocate a certain number of input images to the test set. This set is used to verify the model's training convergence, and to select the most appropriate epoch. The variable <code>tst_inds</code> serves this purpose, by containing the indexes of the images to use for testing.</p> <p>The algorithms that do not use this variable, will randomly select a certain fixed number of pixels as leave-out set.</p>"},{"location":"tutorials/denoising/#training-the-denoisers","title":"Training the Denoisers","text":"<p>We will train four different denoisers: Supervised Denoiser, Noise2Noise (N2N), Noise2Void (N2V), and Deep Image Prior (DIP). We first define the type of model that we will want to use. In this case it will be a U-net [1], with 16 features: <pre><code>net_params = ad.NetworkParamsUNet(n_features=16)\n</code></pre></p> <p>The variable <code>net_params</code> only defines the type of architecture that we want. When passed to the denoising algorithms, they will use it to create and initialize a U-net model. Other pre-configured models are available: MS-D net [2], DnCNN [3], and a custom ResNet implementation [4].</p>"},{"location":"tutorials/denoising/#supervised-denoiser","title":"Supervised Denoiser","text":"<p>The supervised denoiser is trained using pairs of noisy and clean images. It learns to map noisy images to their clean counterparts.</p> <pre><code>denoiser_sup = ad.Denoiser(model=net_params, reg_val=REG_TV_VAL)\ndenoiser_sup.train_supervised(imgs_noisy, img_orig, epochs=EPOCHS, tst_inds=tst_inds)\n</code></pre>"},{"location":"tutorials/denoising/#noise2noise-n2n","title":"Noise2Noise (N2N)","text":"<p>Noise2Noise is a self-supervised denoising method that uses pairs of noisy images of the same object [5]. It learns to map one noisy image to another noisy image of the same object.</p> <pre><code>denoiser_n2n = ad.N2N(model=net_params, reg_val=REG_TV_VAL)\ndenoiser_n2n.train_selfsupervised(imgs_noisy, epochs=EPOCHS)\n</code></pre>"},{"location":"tutorials/denoising/#noise2void-n2v","title":"Noise2Void (N2V)","text":"<p>Noise2Void is a self-supervised denoising method that can work with a single noisy image [6]. This implementation can also work with structured noise [7]. It applies randomly generated masks to the images and learns to predict the masked pixels.</p> <pre><code>denoiser_n2v = ad.N2V(model=net_params, reg_val=REG_TV_VAL)\ndenoiser_n2v.train_selfsupervised(imgs_noisy, epochs=EPOCHS, tst_inds=tst_inds)\n</code></pre>"},{"location":"tutorials/denoising/#deep-image-prior-dip","title":"Deep Image Prior (DIP)","text":"<p>Deep Image Prior is an unsupervised denoising method that can also work with a single image [8]. It uses the prior knowledge embedded in the network architecture to denoise the image.</p> <pre><code>denoiser_dip = ad.DIP(model=net_params, reg_val=REG_TV_VAL)\ninp_dip = denoiser_dip.train_unsupervised(imgs_noisy, epochs=EPOCHS)\n</code></pre>"},{"location":"tutorials/denoising/#performing-inference","title":"Performing Inference","text":"<p>Inference is the process of using the trained models to denoise new images. The <code>infer</code> method takes the noisy images as input and outputs the denoised images.</p>"},{"location":"tutorials/denoising/#supervised-denoiser-inference","title":"Supervised Denoiser Inference","text":"<pre><code>den_sup = denoiser_sup.infer(imgs_noisy).mean(0)\n</code></pre>"},{"location":"tutorials/denoising/#noise2noise-n2n-inference","title":"Noise2Noise (N2N) Inference","text":"<pre><code>den_n2n = denoiser_n2n.infer(imgs_noisy).mean(0)\n</code></pre>"},{"location":"tutorials/denoising/#noise2void-n2v-inference","title":"Noise2Void (N2V) Inference","text":"<pre><code>den_n2v = denoiser_n2v.infer(imgs_noisy).mean(0)\n</code></pre>"},{"location":"tutorials/denoising/#deep-image-prior-dip-inference","title":"Deep Image Prior (DIP) Inference","text":"<pre><code>den_dip = denoiser_dip.infer(inp_dip)\n</code></pre>"},{"location":"tutorials/denoising/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Finally, we will visualize the results of the different denoisers.</p> ImageCode <p></p> <pre><code>fig, axs = plt.subplots(2, 3, sharex=True, sharey=True)\naxs[0, 0].imshow(img_orig)\naxs[0, 0].set_title(\"Original image\")\naxs[0, 1].imshow(imgs_noisy[0])\naxs[0, 1].set_title(\"Noisy image\")\naxs[0, 2].imshow(den_sup)\naxs[0, 2].set_title(\"Denoised supervised\")\naxs[1, 0].imshow(den_n2n)\naxs[1, 0].set_title(\"Denoised N2N\")\naxs[1, 1].imshow(den_n2v)\naxs[1, 1].set_title(\"Denoised N2V\")\naxs[1, 2].imshow(den_dip)\naxs[1, 2].set_title(\"Denoised DIP\")\nfig.tight_layout()\nplt.show(block=False)\n</code></pre>"},{"location":"tutorials/denoising/#references","title":"References","text":"<ol> <li> O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Convolutional Networks for Biomedical Image Segmentation,\u201d in Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, 2015, pp. 234\u2013241. doi: 10.1007/978-3-319-24574-4_28.</li> <li> D. M. Pelt and J. A. Sethian, \u201cA mixed-scale dense convolutional neural network for image analysis,\u201d Proceedings of the National Academy of Sciences, vol. 115, no. 2, pp. 254\u2013259, 2018, doi: 10.1073/pnas.1715832114.</li> <li> K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, \u201cBeyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,\u201d IEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3142\u20133155, Jul. 2017, doi: 10.1109/TIP.2017.2662206.</li> <li> K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep Residual Learning for Image Recognition,\u201d in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Jun. 2016, pp. 770\u2013778. doi: 10.1109/CVPR.2016.90.</li> <li> J. Lehtinen et al., \u201cNoise2Noise: Learning Image Restoration without Clean Data,\u201d in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, Eds., in Proceedings of Machine Learning Research, vol. 80. PMLR, 2018, pp. 2965\u20132974. https://proceedings.mlr.press/v80/lehtinen18a.html.</li> <li> A. Krull, T.-O. Buchholz, and F. Jug, \u201cNoise2Void - Learning Denoising From Single Noisy Images,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Jun. 2019, pp. 2124\u20132132. doi: 10.1109/CVPR.2019.00223.</li> <li> C. Broaddus, A. Krull, M. Weigert, U. Schmidt, and G. Myers, \u201cRemoving Structured Noise with Self-Supervised Blind-Spot Networks,\u201d in 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE, Apr. 2020, pp. 159\u2013163. doi: 10.1109/ISBI45749.2020.9098336.</li> <li> V. Lempitsky, A. Vedaldi, and D. Ulyanov, \u201cDeep Image Prior,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, Jun. 2018, pp. 9446\u20139454. doi: 10.1109/CVPR.2018.00984.</li> </ol>"},{"location":"tutorials/models_io/","title":"Loading and storing models","text":"<p>Auto-denoise offers the ability to store models for later use. The reasons why one would want to do that include: reproducibility for batches of tests, pre-training, fine-tuning, etc. While Auto-denoise provides a handful of pre-implemented model types, you can always use your own model. In order to use your customized models in the provided algorithms, they need to inherit from <code>PyTorch</code>'s <code>nn.Module</code> class. At the end of this tutorial, we will explain how to make your own models compatible with the storing/loading mechanics of auto-denoise.</p>"},{"location":"tutorials/models_io/#creating-models","title":"Creating models","text":"<p>Auto-denoise offers two ways to create a pre-configured model: (a) model configuration class, and (b) instantiating the model directly: (a) Configuration class<pre><code>import autoden as ad\n\nmodel_def = ad.models.config.NetworkParamsUNet(n_features=32, n_levels=3)\nmodel = model_def.get_model()\n</code></pre> or simply: (b) Direct model instantiation<pre><code>import autoden as ad\n\nmodel = ad.models.unet.UNet(1, 1, n_features=32, n_levels=3)\n</code></pre></p> <p>While the second is more compact, it might be useful to use a model configuration class when we just want to pass around a description of the model architecture, without having to instantiate and initialize its weights.</p>"},{"location":"tutorials/models_io/#storing-models","title":"Storing models","text":"<p>Storing models to file can be done through the following function: <pre><code>ad.models.io.save_model(\"file_dest.pt\", model)\n</code></pre> Optionally, <code>save_model</code> can store the optimizer state and epoch number. The model weights, name, and architecture are all saved in the same file.</p>"},{"location":"tutorials/models_io/#loading-models","title":"Loading models","text":"<p>Stored models can be loaded with <code>PyTorch</code>'s <code>load</code> function, and then the <code>create_network</code> function from auto-denoise: <pre><code>from torch import load as load_model\n\nmodel_dict = load_model(\"file_dest.pt\")\nmodel = ad.models.config.create_network(model_dict)\n</code></pre></p>"},{"location":"tutorials/models_io/#making-your-model-compatible","title":"Making your model compatible","text":"<p>To make your custom model compatible with <code>auto-denoise</code>'s storing/loading mechanics, you need to implement the <code>SerializableModel</code> protocol from <code>ad.models.config</code>:</p> SerializableModel<pre><code>from collections.abc import Mapping\nfrom typing import Protocol, runtime_checkable\n\n\n@runtime_checkable\nclass SerializableModel(Protocol):\n    \"\"\"\n    Protocol for serializable models.\n\n    Provides a dictionary containing the initialization parameters of the model.\n    \"\"\"\n\n    init_params: Mapping\n</code></pre> <p>The <code>init_params</code> dictionary should contain the input arguments necessary to initialize your model. As an instructive example, the following is the implementation from the model <code>UNet</code>:</p> UNet initialization parameters storing<pre><code>class UNet(nn.Module):\n    \"\"\"U-net model.\"\"\"\n\n    def __init__(\n        self,\n        n_channels_in: int,\n        n_channels_out: int,\n        n_features: int = 32,\n        n_levels: int = 3,\n        n_channels_skip: int | None = None,\n        bilinear: bool = True,\n        pad_mode: str = \"replicate\",\n        device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n        verbose: bool = False,\n    ):\n        init_params = locals()\n        del init_params[\"self\"]\n        del init_params[\"__class__\"]\n\n        super().__init__()\n        self.init_params = init_params\n        ...\n</code></pre> <p>Currently, the <code>create_network</code> function only knows the pre-configured models, so for the time being you will have to patch that function too. In the future we will provide mechanics to register your model types.</p>"},{"location":"tutorials/noise2inverse/","title":"Tutorial: Using Auto-Denoise (autoden) for Noise2Inverse (Self-Supervised Denoising)","text":"<p>This tutorial demonstrates how to implement Noise2Inverse (self-supervised denoising) using the Auto-Denoise (autoden) library. Noise2Inverse is a self-supervised deep convolutional denoising method for tomography, as described in [1]. This tutorial is a walk-through of example number 03, from the <code>examples</code> directory.</p> <ul> <li>[1] A. A. Hendriksen, D. M. Pelt, and K. J. Batenburg, \"Noise2Inverse: Self-Supervised Deep Convolutional Denoising for Tomography,\" IEEE Transactions on Computational Imaging, vol. 6, pp. 1320\u20131335, 2020, doi: 10.1109/TCI.2020.3019647.</li> </ul>"},{"location":"tutorials/noise2inverse/#setting-up-the-data","title":"Setting Up the Data","text":"<p>First, we need to set up the data to be used for training and testing the denoisers. We will use the <code>skimage</code> library to generate a phantom image, create a sinogram, add noise to the sinogram, and then reconstruct the noisy sinogram.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport skimage.data as skd\nimport skimage.transform as skt\nfrom corrct.models import get_vol_geom_from_data, get_vol_geom_from_volume\nfrom corrct.processing.post import plot_frcs\nfrom corrct.projectors import ProjectorUncorrected\nfrom corrct.solvers import FBP, PDHG\nfrom corrct.regularizers import Regularizer_TV2D\nfrom corrct.testing import add_noise\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\nimport autoden as ad\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib widget\n\nprint(\"Creating phantom\")\nphantom = skd.shepp_logan_phantom()\nphantom = skt.downscale_local_mean(phantom, 4).astype(np.float32)\n\nprint(\"Creating sinogram\")\nvol_geom = get_vol_geom_from_volume(phantom)\nangles_rad = np.deg2rad(np.linspace(start=0, stop=180, num=int((180 * np.pi) // 2), endpoint=False))\nwith ProjectorUncorrected(vol_geom, angles_rad) as prj:\n    sino_clean = prj.fp(phantom)\n\nprint(\"Adding noise\")\nsino_noisy, _, _ = add_noise(sino_clean, num_photons=1, readout_noise_std=2)\n\nPIX_TO_INCH = 0.02\n\nfig_size = (sino_clean.shape[-1] * 2 * PIX_TO_INCH, sino_clean.shape[-2] * PIX_TO_INCH)\nfig, axes = plt.subplots(1, 2, figsize=fig_size)\naxes[0].imshow(sino_clean, cmap='gray')\naxes[0].set_title('Clean Sinogram')\naxes[1].imshow(sino_noisy, cmap='gray')\naxes[1].set_title('Noisy Sinogram')\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> </p>"},{"location":"tutorials/noise2inverse/#creating-noisy-reconstructions","title":"Creating Noisy Reconstructions","text":"<p>We split the sinograms into two sets and create noisy reconstructions using the Filtered Back Projection (FBP) method. This provides two statistically independent reconstructions of the same object. This will be used by N2I to train the denoiser model.</p> <pre><code>print(\"Creating noisy reconstructions (FBP)\")\nvol_geom = get_vol_geom_from_data(sino_clean)\nsolver = FBP()\nwith ProjectorUncorrected(vol_geom, angles_rad) as prj:\n    rec_noisy = solver(prj, sino_noisy)[0]\n\n# Splitting sinograms\nangles_rad_rec = [angles_rad[0::2], angles_rad[1::2]]\nsinos_noisy_rec = [sino_noisy[0::2], sino_noisy[1::2]]\n\nrecs_noisy = []\nfor sino, angles in zip(sinos_noisy_rec, angles_rad_rec):\n    with ProjectorUncorrected(vol_geom, angles) as prj:\n        recs_noisy.append(solver(prj, sino)[0])\n\nrecs_noisy_stack = np.stack(recs_noisy, axis=0)\n</code></pre>"},{"location":"tutorials/noise2inverse/#total-variation-minimization","title":"Total Variation Minimization","text":"<p>We perform the Total Variation (TV) minimization reconstruction of the entire noisy sinogram as reference.</p> <p><pre><code>reg = Regularizer_TV2D(1e1)\npdhg = PDHG(regularizer=reg, verbose=True)\n\nwith ProjectorUncorrected(vol_geom, angles_rad) as prj:\n    rec_tv = pdhg(prj, sino_noisy, iterations=500)[0]\n\nPIX_TO_INCH = 0.04\nfig_size = (phantom.shape[-1] * 3 * PIX_TO_INCH, phantom.shape[-2] * PIX_TO_INCH)\nfig, axs = plt.subplots(1, 3, sharex=True, sharey=True, figsize=fig_size)\naxs[0].imshow(phantom)\naxs[0].set_title(\"Phantom\")\naxs[1].imshow(rec_noisy, vmin=0.0, vmax=1.0)\naxs[1].set_title(\"Noisy reconstruction\")\naxs[2].imshow(rec_tv, vmin=0.0, vmax=1.0)\naxs[2].set_title(\"TV-min reconstruction\")\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> </p>"},{"location":"tutorials/noise2inverse/#training-the-noise2inverse-denoiser","title":"Training the Noise2Inverse Denoiser","text":"<p>We train the N2I denoiser using the split noisy reconstructions.</p> <pre><code>EPOCHS = 1024 * 1\nREG_TV_VAL = 3e-6\n\nprint(\"Denoising reconstructions with N2N\")\nnet_params = ad.NetworkParamsUNet(n_features=24)\ndenoiser_un = ad.N2N(model=net_params, reg_val=REG_TV_VAL)\ndenoiser_un.train_selfsupervised(recs_noisy_stack, epochs=EPOCHS)\n</code></pre> <p>The training algorithm sets aside a small portion of the images' pixels to test the quality of the denoised image. When the training is over, it will automatically select the model weights that exhibit the highest denoising performance (according to the MSE loss) on the said pixel leave-out set. </p>"},{"location":"tutorials/noise2inverse/#performing-inference","title":"Performing Inference","text":"<p>We use the trained N2I model to produce the denoised reconstruction.</p> <pre><code>rec_n2i = denoiser_un.infer(recs_noisy_stack).mean(0)\n</code></pre>"},{"location":"tutorials/noise2inverse/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Finally, we visualize the results of the different denoising methods.</p> <p><pre><code>PIX_TO_INCH = 0.04\nfig_size = (phantom.shape[-1] * 3 * PIX_TO_INCH, phantom.shape[-2] * PIX_TO_INCH)\nfig, axs = plt.subplots(1, 3, sharex=True, sharey=True, figsize=fig_size)\naxs[0].imshow(phantom)\naxs[0].set_title(\"Phantom\")\naxs[1].imshow(rec_noisy, vmin=0.0, vmax=1.0)\naxs[1].set_title(\"Noisy reconstruction\")\naxs[2].imshow(rec_n2i, vmin=0.0, vmax=1.0)\naxs[2].set_title(\"Noise2Inverse reconstruction\")\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> </p>"},{"location":"tutorials/noise2inverse/#evaluating-the-results","title":"Evaluating the Results","text":"<p>We now evaluate the results using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM).</p> <p><pre><code>all_recs = [rec_noisy, rec_tv, rec_n2i]\nall_labs = [\"FBP\", \"TV-min\", \"Noise2Inverse\"]\n\nprint(\"PSNR:\")\nfor rec, lab in zip(all_recs, all_labs):\n    print(f\"- {lab}: {psnr(phantom, rec, data_range=1.0):.3}\")\nprint(\"SSIM:\")\nfor rec, lab in zip(all_recs, all_labs):\n    print(f\"- {lab}: {ssim(phantom, rec, data_range=1.0):.3}\")\n\nplot_frcs([(phantom, rec) for rec in all_recs], all_labs)\n</code></pre> Resulting in: <pre><code>PSNR:\n- FBP: 21.5\n- TV-min: 24.8\n- Noise2Inverse: 25.2\nSSIM:\n- FBP: 0.388\n- TV-min: 0.558\n- Noise2Inverse: 0.707\n</code></pre> </p>"}]}