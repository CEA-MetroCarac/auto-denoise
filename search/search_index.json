{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Auto-Denoise","text":"<p>Unsupervised and self-supervised CNN denoising methods.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>It takes a few steps to setup Auto-Denoise on your machine. We recommend installing Anaconda package manager or the more compact Miniconda for Python 3.</p>"},{"location":"#installing-with-conda","title":"Installing with conda","text":"<p>Simply install with: <pre><code>conda install auto-denoise -c n-vigano\n</code></pre></p>"},{"location":"#installing-from-pypi","title":"Installing from PyPI","text":"<p>Simply install with: <pre><code>python3 -m pip install auto-denoise\n</code></pre></p> <p>If you are on jupyter, and don't have the rights to install packages system-wide, then you can install with: <pre><code>! pip install --user auto-denoise\n</code></pre></p>"},{"location":"#installing-from-source","title":"Installing from source","text":"<p>To install Auto-Denoise, simply clone this github.com project with either: <pre><code>git clone https://github.com/CEA-MetroCarac/auto-denoise.git auto-denoise\n</code></pre> or: <pre><code>git clone git@github.com:CEA-MetroCarac/auto-denoise.git auto-denoise\n</code></pre></p> <p>Then go to the cloned directory and run <code>pip</code> installer: <pre><code>cd auto-denoise\npip install -e .\n</code></pre></p>"},{"location":"#how-to-contribute","title":"How to contribute","text":"<p>Contributions are always welcome. Please submit pull requests against the <code>main</code> branch.</p> <p>If you have any issues, questions, or remarks, then please open an issue on github.com.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at nicola.vigano@cea.fr. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier!</p> <p>Fork and clone the repository, then:</p> <pre><code>cd auto-denoise\nmake setup\n</code></pre> <p>Note</p> <p>If it fails for some reason, you'll need to install Poetry manually.</p> <p>You can install it with:</p> <pre><code>python3 -m pip install --user pipx\npipx install poetry\n</code></pre> <p>Now you can try running <code>make setup</code> again, or simply <code>poetry install</code>.</p> <p>You now have the dependencies installed.</p> <p>Run <code>make help</code> to see all the available actions!</p>"},{"location":"contributing/#tasks","title":"Tasks","text":"<p>This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following:</p> <ol> <li><code>export PYTHON_VERSIONS=</code>: this will run the task    with only the current Python version</li> <li>run the task directly with <code>poetry run duty TASK</code>,    or <code>duty TASK</code> if the environment was already activated    through <code>poetry shell</code></li> </ol> <p>The Makefile detects if the Poetry environment is activated, so <code>make</code> will work the same with the virtualenv activated or not.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git checkout -b feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>If you updated the documentation or the project dependencies:</p> <ol> <li>run <code>make docs-regen</code></li> <li>run <code>make docs-serve</code>,    go to http://localhost:8000 and check that everything looks good</li> </ol> <p>Before committing:</p> <ol> <li>run <code>make format</code> to auto-format the code</li> <li>run <code>make check</code> to check everything (fix any warning)</li> <li>run <code>make test</code> to run the tests (fix any issue)</li> <li>follow our commit message convention</li> </ol> <p>If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review.</p> <p>Don't bother updating the changelog, we will take care of this.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit message convention","text":"<p>Commits messages must follow the Angular style:</p> <pre><code>&lt;type&gt;[(scope)]: Subject\n\n[Body]\n</code></pre> <p>Scope and body are optional. Type can be:</p> <ul> <li><code>build</code>: About packaging, building wheels, etc.</li> <li><code>chore</code>: About packaging or repo/files management.</li> <li><code>ci</code>: About Continuous Integration.</li> <li><code>docs</code>: About documentation.</li> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>perf</code>: About performance.</li> <li><code>refactor</code>: Changes which are not features nor bug fixes.</li> <li><code>style</code>: A change in code style/format.</li> <li><code>tests</code>: About tests.</li> </ul> <p>Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end:</p> <pre><code>Body.\n\nReferences: #10, #11.\nFixes #15.\n</code></pre>"},{"location":"contributing/#pull-requests-guidelines","title":"Pull requests guidelines","text":"<p>Link to any related issue in the Pull Request message.</p> <p>During review, we recommend using fixups:</p> <pre><code># SHA is the SHA of the commit you want to fix\ngit commit --fixup=SHA\n</code></pre> <p>Once all the changes are approved, you can squash your commits:</p> <pre><code>git rebase -i --autosquash master\n</code></pre> <p>And force-push:</p> <pre><code>git push -f\n</code></pre> <p>If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.</p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2023 Nicola VIGANO\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"reference/algorithms/","title":"algorithms.py","text":"<p>Implementation of various unsupervised and self-supervised denoising methods.</p>"},{"location":"reference/algorithms/#autoden.algorithms.DIP","title":"<code>DIP</code>","text":"<p>               Bases: <code>Denoiser</code></p> <p>Deep image prior.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>class DIP(Denoiser):\n    \"\"\"Deep image prior.\"\"\"\n\n    def train_unsupervised(\n        self, tgt: NDArray, epochs: int, inp: NDArray | None = None, num_tst_ratio: float = 0.2, algo: str = \"adam\"\n    ) -&gt; NDArray:\n        if inp is None:\n            tmp_inp = inp = np.random.normal(size=tgt.shape, scale=0.25).astype(tgt.dtype)\n            self.data_scaling_inp = 1.0\n            self.data_bias_inp = 0.0\n        else:\n            range_vals_inp = _get_normalization(inp, percentile=0.001)\n            self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n            self.data_bias_inp = range_vals_inp[2] * self.data_scaling_inp\n\n            # Rescale input\n            tmp_inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n        range_vals_tgt = _get_normalization(tgt, percentile=0.001)\n        self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n        self.data_bias_tgt = range_vals_tgt[2] * self.data_scaling_tgt\n\n        # Rescale target\n        tmp_tgt = tgt * self.data_scaling_tgt - self.data_bias_tgt\n\n        mask_trn = np.ones_like(tgt, dtype=bool)\n        rnd_inds = np.random.random_integers(low=0, high=mask_trn.size - 1, size=int(mask_trn.size * num_tst_ratio))\n        mask_trn[np.unravel_index(rnd_inds, shape=mask_trn.shape)] = False\n\n        losses_trn, losses_tst = self._train_dip(tmp_inp, tmp_tgt, mask_trn, epochs=epochs, algo=algo)\n\n        if self.verbose:\n            self._plot_loss_curves(losses_trn, losses_tst, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n\n        return inp\n\n    def _train_dip(\n        self, inp: NDArray, tgt: NDArray, mask_trn: NDArray, epochs: int, algo: str = \"adam\"\n    ) -&gt; tuple[NDArray, NDArray]:\n        losses_trn = []\n        losses_tst = []\n        # loss_trn_fn = models.MSELoss_TV(lambda_val=self.reg_tv_val, reduction=\"sum\")\n        loss_trn_fn = pt.nn.MSELoss(reduction=\"mean\")\n        loss_tst_fn = pt.nn.MSELoss(reduction=\"mean\")\n        optim = _create_optimizer(self.net, algo=algo)\n\n        best_epoch = -1\n        best_loss_tst = +np.inf\n        best_state = self.net.state_dict()\n        best_optim = optim.state_dict()\n\n        inp_t = pt.tensor(inp, device=self.device)[None, None, ...]\n        tgt_trn = pt.tensor(tgt[mask_trn], device=self.device)\n        tgt_tst = pt.tensor(tgt[np.logical_not(mask_trn)], device=self.device)\n\n        mask_trn_t = pt.tensor(mask_trn, device=self.device)\n        mask_tst_t = pt.tensor(np.logical_not(mask_trn), device=self.device)\n\n        self.net.train()\n        for epoch in tqdm(range(epochs), desc=f\"Training {algo.upper()}\"):\n            # Train\n            optim.zero_grad()\n            out_t = self.net(inp_t)\n            out_trn = out_t[0, 0][mask_trn_t]\n\n            loss_trn = loss_trn_fn(out_trn, tgt_trn)\n            if self.reg_val is not None:\n                loss_trn += losses.LossST(self.reg_val, reduction=\"mean\")(out_t)\n            loss_trn.backward()\n\n            losses_trn.append(loss_trn.item())\n            optim.step()\n\n            # Test\n            out_tst = out_t[0, 0][mask_tst_t]\n            loss_tst = loss_tst_fn(out_tst, tgt_tst)\n            losses_tst.append(loss_tst.item())\n\n            # Check improvement\n            if losses_tst[-1] &lt; best_loss_tst if losses_tst[-1] is not None else False:\n                best_loss_tst = losses_tst[-1]\n                best_epoch = epoch\n                best_state = cp.deepcopy(self.net.state_dict())\n                best_optim = cp.deepcopy(optim.state_dict())\n\n            # Save epoch\n            if self.save_epochs:\n                self._save_state(epoch, self.net.state_dict(), optim.state_dict())\n\n        print(f\"Best epoch: {best_epoch}, with tst_loss: {best_loss_tst:.5}\")\n        if self.save_epochs:\n            self._save_state(best_epoch, best_state, best_optim, is_final=True)\n\n        self.net.load_state_dict(best_state)\n\n        losses_trn = np.array(losses_trn)\n        losses_tst = np.array(losses_tst)\n\n        return losses_trn, losses_tst\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.DatasetSplit","title":"<code>DatasetSplit</code>","text":"<p>Store the dataset split indices, between training and validation.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>class DatasetSplit:\n    \"\"\"Store the dataset split indices, between training and validation.\"\"\"\n\n    trn_inds: NDArray[np.integer]\n    tst_inds: NDArray[np.integer] | None\n\n    def __init__(self, trn_inds: NDArray, tst_inds: NDArray | None = None) -&gt; None:\n        self.trn_inds = np.array(trn_inds)\n        self.tst_inds = np.array(tst_inds) if tst_inds is not None else None\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(\\n  Training indices: {self.trn_inds}\\n  Testing indices: {self.tst_inds}\\n)\"\n\n    @staticmethod\n    def create_sequential(num_trn_imgs: int, num_tst_imgs: int | None = None) -&gt; \"DatasetSplit\":\n        return DatasetSplit(\n            np.arange(num_trn_imgs), np.arange(num_trn_imgs, num_trn_imgs + num_tst_imgs) if num_tst_imgs is not None else None\n        )\n\n    @staticmethod\n    def create_random(num_trn_imgs: int, num_tst_imgs: int | None, tot_num_imgs: int | None = None) -&gt; \"DatasetSplit\":\n        if tot_num_imgs is None:\n            tot_num_imgs = num_trn_imgs + num_tst_imgs if num_tst_imgs is not None else 0\n        inds = np.arange(tot_num_imgs)\n        inds = np.random.permutation(inds)\n        return DatasetSplit(\n            inds[:num_trn_imgs], inds[num_trn_imgs : num_trn_imgs + num_tst_imgs] if num_tst_imgs is not None else None\n        )\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.Denoiser","title":"<code>Denoiser</code>","text":"<p>Denoising images.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>class Denoiser:\n    \"\"\"Denoising images.\"\"\"\n\n    dataset_name: str\n    n_channels: int\n\n    data_scaling_inp: float | NDArray\n    data_scaling_tgt: float | NDArray\n\n    data_bias_inp: float | NDArray\n    data_bias_tgt: float | NDArray\n\n    net: pt.nn.Module\n\n    device: str\n    save_epochs: bool\n\n    verbose: bool\n\n    def __init__(\n        self,\n        dataset_name: str,\n        network_type: str | NetworkParams,\n        network_state: Mapping | None = None,\n        data_scaling_inp: float | None = None,\n        data_scaling_tgt: float | None = None,\n        reg_tv_val: float | None = 1e-5,\n        batch_size: int = 8,\n        device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n        save_epochs: bool = True,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the noise2noise method.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Name of the dataset.\n        network_type : Union[str, NetworkParams]\n            Type of neural network to use\n        network_state : Union[Mapping, None], optional\n            Specific network state to load, by default None\n        data_scaling_inp : Union[float, None], optional\n            Scaling of the input data, by default None\n        data_scaling_tgt : Union[float, None], optional\n            Scaling of the output, by default None\n        reg_tv_val : Union[float, None], optional\n            Deep-image prior regularization value, by default 1e-5\n        batch_size : int, optional\n            Size of the batch, by default 8\n        device : str, optional\n            Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n        save_epochs : bool, optional\n            Whether to save network states at each epoch, by default True\n        verbose : bool, optional\n            Whether to produce verbose output, by default True\n        \"\"\"\n        self.dataset_name = dataset_name\n\n        if isinstance(network_type, str):\n            self.n_channels = 1\n        else:\n            self.n_channels = network_type.n_channels_in\n\n        self.net = _create_network(network_type, device=device)\n\n        if network_state is not None:\n            if isinstance(network_state, int):\n                self._load_state(network_state)\n            else:\n                self.net.load_state_dict(network_state)\n\n        if data_scaling_inp is not None:\n            self.data_scaling_inp = data_scaling_inp\n        else:\n            self.data_scaling_inp = 1\n        if data_scaling_tgt is not None:\n            self.data_scaling_tgt = data_scaling_tgt\n        else:\n            self.data_scaling_tgt = 1\n\n        self.data_bias_inp = 0\n        self.data_bias_tgt = 0\n\n        self.reg_val = reg_tv_val\n        self.batch_size = batch_size\n        self.device = device\n        self.save_epochs = save_epochs\n        self.verbose = verbose\n\n    def train_supervised(self, inp: NDArray, tgt: NDArray, epochs: int, dset_split: DatasetSplit, algo: str = \"adam\"):\n        \"\"\"Supervised training.\n\n        Parameters\n        ----------\n        inp : NDArray\n            The input images\n        tgt : NDArray\n            The target images\n        epochs : int\n            Number of training epochs\n        dset_split : DatasetSplit\n            How to split the dataset in training and validation set\n        algo : str, optional\n            Learning algorithm to use, by default \"adam\"\n        \"\"\"\n        if tgt.ndim == (inp.ndim - 1):\n            tgt = np.tile(tgt[None, ...], [inp.shape[0], *np.ones_like(tgt.shape)])\n\n        range_vals_inp = _get_normalization(inp, percentile=0.001)\n        range_vals_tgt = _get_normalization(tgt, percentile=0.001)\n\n        self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n        self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n\n        self.data_bias_inp = inp.mean() * self.data_scaling_inp\n        self.data_bias_tgt = tgt.mean() * self.data_scaling_tgt\n\n        # Rescale the datasets\n        inp = inp * self.data_scaling_inp - self.data_bias_inp\n        tgt = tgt * self.data_scaling_tgt - self.data_bias_tgt\n\n        # Create datasets\n        dset_trn = datasets.SupervisedDataset(inp[dset_split.trn_inds], tgt[dset_split.trn_inds], device=self.device)\n        dset_tst = datasets.SupervisedDataset(inp[dset_split.tst_inds], tgt[dset_split.tst_inds], device=self.device)\n\n        dl_trn = DataLoader(dset_trn, batch_size=self.batch_size)\n        dl_tst = DataLoader(dset_tst, batch_size=self.batch_size * 16)\n\n        loss_trn, loss_tst = self._train(dl_trn, dl_tst, epochs=epochs, algo=algo)\n\n        if self.verbose:\n            self._plot_loss_curves(loss_trn, loss_tst, f\"Supervised {algo.upper()}\")\n\n    def _train(self, dl_trn: DataLoader, dl_tst: DataLoader, epochs: int, algo: str = \"adam\") -&gt; tuple[NDArray, NDArray]:\n        losses_trn = []\n        losses_tst = []\n        loss_trn_fn = losses.MSELoss_TV(lambda_val=self.reg_val, reduction=\"sum\")\n        loss_tst_fn = pt.nn.MSELoss(reduction=\"sum\")\n        optim = _create_optimizer(self.net, algo=algo)\n\n        best_epoch = -1\n        best_loss_tst = +np.inf\n        best_state = self.net.state_dict()\n        best_optim = optim.state_dict()\n\n        dset_trn_size = len(dl_trn)\n        dset_tst_size = len(dl_tst)\n\n        for epoch in tqdm(range(epochs), desc=f\"Training {algo.upper()}\"):\n            # Train\n            self.net.train()\n            loss_trn_val = 0\n            for inp_trn, tgt_trn in dl_trn:\n                # inp_trn = inp_trn.to(self.device, non_blocking=True)\n                # tgt_trn = tgt_trn.to(self.device, non_blocking=True)\n\n                optim.zero_grad()\n                output = self.net(inp_trn)\n                loss_trn = loss_trn_fn(output, tgt_trn)\n                loss_trn.backward()\n\n                loss_trn_val += loss_trn.item()\n\n                optim.step()\n\n            losses_trn.append(loss_trn_val / dset_trn_size)\n\n            # Test\n            self.net.eval()\n            loss_tst_val = 0\n            with pt.inference_mode():\n                for inp_tst, tgt_tst in dl_tst:\n                    # inp_tst = inp_tst.to(self.device, non_blocking=True)\n                    # tgt_tst = tgt_tst.to(self.device, non_blocking=True)\n\n                    output = self.net(inp_tst)\n                    loss_tst = loss_tst_fn(output, tgt_tst)\n\n                    loss_tst_val += loss_tst.item()\n\n                losses_tst.append(loss_tst_val / dset_tst_size)\n\n            # Check improvement\n            if losses_tst[-1] &lt; best_loss_tst if losses_tst[-1] is not None else False:\n                best_loss_tst = losses_tst[-1]\n                best_epoch = epoch\n                best_state = cp.deepcopy(self.net.state_dict())\n                best_optim = cp.deepcopy(optim.state_dict())\n\n            # Save epoch\n            if self.save_epochs:\n                self._save_state(epoch, self.net.state_dict(), optim.state_dict())\n\n        print(f\"Best epoch: {best_epoch}, with tst_loss: {best_loss_tst:.5}\")\n        if self.save_epochs:\n            self._save_state(best_epoch, best_state, best_optim, is_final=True)\n\n        self.net.load_state_dict(best_state)\n\n        return np.array(losses_trn), np.array(losses_tst)\n\n    def _save_state(self, epoch_num: int, net_state: Mapping, optim_state: Mapping, is_final: bool = False) -&gt; None:\n        epochs_base_path = Path(self.dataset_name) / \"weights\"\n        epochs_base_path.mkdir(parents=True, exist_ok=True)\n\n        if is_final:\n            pt.save(\n                {\"epoch\": epoch_num, \"state_dict\": net_state, \"optimizer\": optim_state},\n                epochs_base_path / \"weights.pt\",\n            )\n        else:\n            pt.save(\n                {\"epoch\": epoch_num, \"state_dict\": net_state, \"optimizer\": optim_state},\n                epochs_base_path / f\"weights_epoch_{epoch_num}.pt\",\n            )\n\n    def _load_state(self, epoch_num: int | None = None) -&gt; None:\n        epochs_base_path = Path(self.dataset_name) / \"weights\"\n        if not epochs_base_path.exists():\n            raise ValueError(\"No state to load!\")\n\n        if epoch_num is None or epoch_num == -1:\n            state_path = epochs_base_path / \"weights.pt\"\n        else:\n            state_path = epochs_base_path / f\"weights_epoch_{epoch_num}.pt\"\n        print(f\"Loading state path: {state_path}\")\n        state_dict = pt.load(state_path)\n        self.net.load_state_dict(state_dict[\"state_dict\"])\n\n    def _plot_loss_curves(self, train_loss: NDArray, test_loss: NDArray, title: str | None = None) -&gt; None:\n        test_argmin = int(np.argmin(test_loss))\n        fig, axs = plt.subplots(1, 1, figsize=[7, 2.6])\n        if title is not None:\n            axs.set_title(title)\n        axs.semilogy(np.arange(train_loss.size), train_loss, label=\"training loss\")\n        axs.semilogy(np.arange(test_loss.size) + 1, test_loss, label=\"test loss\")\n        axs.stem(test_argmin + 1, test_loss[test_argmin], linefmt=\"C1--\", markerfmt=\"C1o\", label=f\"Best epoch: {test_argmin}\")\n        axs.legend()\n        axs.grid()\n        fig.tight_layout()\n        plt.show(block=False)\n\n    def infer(self, inp: NDArray) -&gt; NDArray:\n        \"\"\"Inference, given an initial stack of images.\n\n        Parameters\n        ----------\n        inp : NDArray\n            The input stack of images\n\n        Returns\n        -------\n        NDArray\n            The denoised stack of images\n        \"\"\"\n        # Rescale input\n        inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n        # Create datasets\n        dset = datasets.InferenceDataset(inp, device=self.device)\n\n        dtl = DataLoader(dset, batch_size=self.batch_size)\n\n        output = self._infer(dtl)\n\n        # Rescale output\n        return (output + self.data_bias_tgt) / self.data_scaling_tgt\n\n    def _infer(self, dtl: DataLoader) -&gt; NDArray:\n        self.net.eval()\n        output = []\n        with pt.inference_mode():\n            for inp in tqdm(dtl, desc=\"Inference\"):\n                inp = inp.to(self.device, non_blocking=True)\n\n                out = self.net(inp)\n                output.append(out.cpu().numpy())\n\n        output = np.concatenate(output, axis=0)\n        if output.shape[1] == 1:\n            output = np.squeeze(output, axis=1)\n        return output\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.Denoiser.__init__","title":"<code>__init__(dataset_name, network_type, network_state=None, data_scaling_inp=None, data_scaling_tgt=None, reg_tv_val=1e-05, batch_size=8, device='cuda' if pt.cuda.is_available() else 'cpu', save_epochs=True, verbose=True)</code>","text":"<p>Initialize the noise2noise method.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>network_type</code> <code>Union[str, NetworkParams]</code> <p>Type of neural network to use</p> required <code>network_state</code> <code>Union[Mapping, None]</code> <p>Specific network state to load, by default None</p> <code>None</code> <code>data_scaling_inp</code> <code>Union[float, None]</code> <p>Scaling of the input data, by default None</p> <code>None</code> <code>data_scaling_tgt</code> <code>Union[float, None]</code> <p>Scaling of the output, by default None</p> <code>None</code> <code>reg_tv_val</code> <code>Union[float, None]</code> <p>Deep-image prior regularization value, by default 1e-5</p> <code>1e-05</code> <code>batch_size</code> <code>int</code> <p>Size of the batch, by default 8</p> <code>8</code> <code>device</code> <code>str</code> <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> <code>'cuda' if is_available() else 'cpu'</code> <code>save_epochs</code> <code>bool</code> <p>Whether to save network states at each epoch, by default True</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to produce verbose output, by default True</p> <code>True</code> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    network_type: str | NetworkParams,\n    network_state: Mapping | None = None,\n    data_scaling_inp: float | None = None,\n    data_scaling_tgt: float | None = None,\n    reg_tv_val: float | None = 1e-5,\n    batch_size: int = 8,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    save_epochs: bool = True,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Name of the dataset.\n    network_type : Union[str, NetworkParams]\n        Type of neural network to use\n    network_state : Union[Mapping, None], optional\n        Specific network state to load, by default None\n    data_scaling_inp : Union[float, None], optional\n        Scaling of the input data, by default None\n    data_scaling_tgt : Union[float, None], optional\n        Scaling of the output, by default None\n    reg_tv_val : Union[float, None], optional\n        Deep-image prior regularization value, by default 1e-5\n    batch_size : int, optional\n        Size of the batch, by default 8\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs : bool, optional\n        Whether to save network states at each epoch, by default True\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    self.dataset_name = dataset_name\n\n    if isinstance(network_type, str):\n        self.n_channels = 1\n    else:\n        self.n_channels = network_type.n_channels_in\n\n    self.net = _create_network(network_type, device=device)\n\n    if network_state is not None:\n        if isinstance(network_state, int):\n            self._load_state(network_state)\n        else:\n            self.net.load_state_dict(network_state)\n\n    if data_scaling_inp is not None:\n        self.data_scaling_inp = data_scaling_inp\n    else:\n        self.data_scaling_inp = 1\n    if data_scaling_tgt is not None:\n        self.data_scaling_tgt = data_scaling_tgt\n    else:\n        self.data_scaling_tgt = 1\n\n    self.data_bias_inp = 0\n    self.data_bias_tgt = 0\n\n    self.reg_val = reg_tv_val\n    self.batch_size = batch_size\n    self.device = device\n    self.save_epochs = save_epochs\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.Denoiser.infer","title":"<code>infer(inp)</code>","text":"<p>Inference, given an initial stack of images.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>NDArray</code> <p>The input stack of images</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>The denoised stack of images</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n    # Create datasets\n    dset = datasets.InferenceDataset(inp, device=self.device)\n\n    dtl = DataLoader(dset, batch_size=self.batch_size)\n\n    output = self._infer(dtl)\n\n    # Rescale output\n    return (output + self.data_bias_tgt) / self.data_scaling_tgt\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.Denoiser.train_supervised","title":"<code>train_supervised(inp, tgt, epochs, dset_split, algo='adam')</code>","text":"<p>Supervised training.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>NDArray</code> <p>The input images</p> required <code>tgt</code> <code>NDArray</code> <p>The target images</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs</p> required <code>dset_split</code> <code>DatasetSplit</code> <p>How to split the dataset in training and validation set</p> required <code>algo</code> <code>str</code> <p>Learning algorithm to use, by default \"adam\"</p> <code>'adam'</code> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_supervised(self, inp: NDArray, tgt: NDArray, epochs: int, dset_split: DatasetSplit, algo: str = \"adam\"):\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    epochs : int\n        Number of training epochs\n    dset_split : DatasetSplit\n        How to split the dataset in training and validation set\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [inp.shape[0], *np.ones_like(tgt.shape)])\n\n    range_vals_inp = _get_normalization(inp, percentile=0.001)\n    range_vals_tgt = _get_normalization(tgt, percentile=0.001)\n\n    self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n\n    self.data_bias_inp = inp.mean() * self.data_scaling_inp\n    self.data_bias_tgt = tgt.mean() * self.data_scaling_tgt\n\n    # Rescale the datasets\n    inp = inp * self.data_scaling_inp - self.data_bias_inp\n    tgt = tgt * self.data_scaling_tgt - self.data_bias_tgt\n\n    # Create datasets\n    dset_trn = datasets.SupervisedDataset(inp[dset_split.trn_inds], tgt[dset_split.trn_inds], device=self.device)\n    dset_tst = datasets.SupervisedDataset(inp[dset_split.tst_inds], tgt[dset_split.tst_inds], device=self.device)\n\n    dl_trn = DataLoader(dset_trn, batch_size=self.batch_size)\n    dl_tst = DataLoader(dset_tst, batch_size=self.batch_size * 16)\n\n    loss_trn, loss_tst = self._train(dl_trn, dl_tst, epochs=epochs, algo=algo)\n\n    if self.verbose:\n        self._plot_loss_curves(loss_trn, loss_tst, f\"Supervised {algo.upper()}\")\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.N2N","title":"<code>N2N</code>","text":"<p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from pairs of images.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>class N2N(Denoiser):\n    \"\"\"Self-supervised denoising from pairs of images.\"\"\"\n\n    def train_selfsupervised(\n        self, inp: NDArray, epochs: int, dset_split: DatasetSplit, strategy: str = \"1:X\", algo: str = \"adam\"\n    ):\n        \"\"\"Self-supervised training.\n\n        Parameters\n        ----------\n        inp : NDArray\n            The input images, which will also be targets\n        epochs : int\n            Number of training epochs\n        dset_split : DatasetSplit\n            How to split the dataset in training and validation set\n        strategy : str, optional\n            The grouping strategy to use (either one-to-many, or many-to-one), by default \"1:X\"\n        algo : str, optional\n            Learning algorithm to use, by default \"adam\"\n        \"\"\"\n        range_vals_tgt = range_vals_inp = _get_normalization(inp, percentile=0.001)\n\n        self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n        self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n\n        self.data_bias_inp = inp.mean() * self.data_scaling_inp\n        self.data_bias_tgt = inp.mean() * self.data_scaling_tgt\n\n        # Rescale the datasets\n        inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n        inp_trn = inp[dset_split.trn_inds]\n        inp_tst = inp[dset_split.tst_inds]\n\n        list_dsets_trn = [datasets.NumpyDataset(x[None, ...], n_channels=self.n_channels) for x in inp_trn]\n        list_dsets_tst = [datasets.NumpyDataset(x[None, ...], n_channels=self.n_channels) for x in inp_tst]\n\n        # Create datasets\n        dset_trn = datasets.SelfsupervisedDataset(*list_dsets_trn, strategy=strategy, device=self.device)\n        dset_tst = datasets.SelfsupervisedDataset(*list_dsets_tst, strategy=strategy, device=self.device)\n\n        dl_trn = DataLoader(dset_trn, batch_size=self.batch_size)\n        dl_tst = DataLoader(dset_tst, batch_size=self.batch_size * 16)\n\n        loss_trn, loss_tst = self._train(dl_trn, dl_tst, epochs=epochs, algo=algo)\n\n        if self.verbose:\n            self._plot_loss_curves(loss_trn, loss_tst, f\"Self-supervised N2N {algo.upper()}\")\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.N2N.train_selfsupervised","title":"<code>train_selfsupervised(inp, epochs, dset_split, strategy='1:X', algo='adam')</code>","text":"<p>Self-supervised training.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>NDArray</code> <p>The input images, which will also be targets</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs</p> required <code>dset_split</code> <code>DatasetSplit</code> <p>How to split the dataset in training and validation set</p> required <code>strategy</code> <code>str</code> <p>The grouping strategy to use (either one-to-many, or many-to-one), by default \"1:X\"</p> <code>'1:X'</code> <code>algo</code> <code>str</code> <p>Learning algorithm to use, by default \"adam\"</p> <code>'adam'</code> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_selfsupervised(\n    self, inp: NDArray, epochs: int, dset_split: DatasetSplit, strategy: str = \"1:X\", algo: str = \"adam\"\n):\n    \"\"\"Self-supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images, which will also be targets\n    epochs : int\n        Number of training epochs\n    dset_split : DatasetSplit\n        How to split the dataset in training and validation set\n    strategy : str, optional\n        The grouping strategy to use (either one-to-many, or many-to-one), by default \"1:X\"\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    range_vals_tgt = range_vals_inp = _get_normalization(inp, percentile=0.001)\n\n    self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n\n    self.data_bias_inp = inp.mean() * self.data_scaling_inp\n    self.data_bias_tgt = inp.mean() * self.data_scaling_tgt\n\n    # Rescale the datasets\n    inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n    inp_trn = inp[dset_split.trn_inds]\n    inp_tst = inp[dset_split.tst_inds]\n\n    list_dsets_trn = [datasets.NumpyDataset(x[None, ...], n_channels=self.n_channels) for x in inp_trn]\n    list_dsets_tst = [datasets.NumpyDataset(x[None, ...], n_channels=self.n_channels) for x in inp_tst]\n\n    # Create datasets\n    dset_trn = datasets.SelfsupervisedDataset(*list_dsets_trn, strategy=strategy, device=self.device)\n    dset_tst = datasets.SelfsupervisedDataset(*list_dsets_tst, strategy=strategy, device=self.device)\n\n    dl_trn = DataLoader(dset_trn, batch_size=self.batch_size)\n    dl_tst = DataLoader(dset_tst, batch_size=self.batch_size * 16)\n\n    loss_trn, loss_tst = self._train(dl_trn, dl_tst, epochs=epochs, algo=algo)\n\n    if self.verbose:\n        self._plot_loss_curves(loss_trn, loss_tst, f\"Self-supervised N2N {algo.upper()}\")\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.N2V","title":"<code>N2V</code>","text":"<p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from single images.</p> Source code in <code>src/autoden/algorithms.py</code> <pre><code>class N2V(Denoiser):\n    \"Self-supervised denoising from single images.\"\n\n    def train_selfsupervised(\n        self,\n        inp: NDArray,\n        epochs: int,\n        dset_split: DatasetSplit,\n        mask_shape: int | Sequence[int] | NDArray = 1,\n        ratio_blind_spot: float = 0.015,\n        algo: str = \"adam\",\n    ):\n        \"\"\"Self-supervised training.\n\n        Parameters\n        ----------\n        inp : NDArray\n            The input images, which will also be targets\n        epochs : int\n            Number of training epochs\n        dset_split : DatasetSplit\n            How to split the dataset in training and validation set\n        mask_shape : int | Sequence[int] | NDArray\n            Shape of the blind spot mask, by default 1.\n        algo : str, optional\n            Learning algorithm to use, by default \"adam\"\n        \"\"\"\n        range_vals_tgt = range_vals_inp = _get_normalization(inp, percentile=0.001)\n\n        self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n        self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n\n        self.data_bias_inp = inp.mean() * self.data_scaling_inp\n        self.data_bias_tgt = inp.mean() * self.data_scaling_tgt\n\n        # Rescale the datasets\n        inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n        inp_trn = inp[dset_split.trn_inds]\n        inp_tst = inp[dset_split.tst_inds]\n\n        dsets_trn = datasets.NumpyDataset(inp_trn, n_channels=self.n_channels)\n        dsets_tst = datasets.NumpyDataset(inp_tst, n_channels=self.n_channels)\n\n        # Create datasets\n        dset_trn = datasets.InferenceDataset(dsets_trn, device=self.device)\n        dset_tst = datasets.InferenceDataset(dsets_tst, device=self.device)\n\n        dl_trn = DataLoader(dset_trn, batch_size=self.batch_size)\n        dl_tst = DataLoader(dset_tst, batch_size=self.batch_size * 16)\n\n        losses_trn, losses_tst = self._train_n2v(\n            dl_trn, dl_tst, epochs=epochs, mask_shape=mask_shape, ratio_blind_spot=ratio_blind_spot, algo=algo\n        )\n\n        self._plot_loss_curves(losses_trn, losses_tst, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n\n    def _train_n2v(\n        self,\n        dl_trn: DataLoader,\n        dl_tst: DataLoader,\n        epochs: int,\n        mask_shape: int | Sequence[int] | NDArray,\n        ratio_blind_spot: float,\n        algo: str = \"adam\",\n    ) -&gt; tuple[NDArray, NDArray]:\n        losses_trn = []\n        losses_tst = []\n        # loss_trn_fn = models.MSELoss_TV(lambda_val=self.reg_tv_val, reduction=\"sum\")\n        loss_trn_fn = pt.nn.MSELoss(reduction=\"sum\")\n        loss_tst_fn = pt.nn.MSELoss(reduction=\"sum\")\n        optim = _create_optimizer(self.net, algo=algo)\n\n        best_epoch = -1\n        best_loss_tst = +np.inf\n        best_state = self.net.state_dict()\n        best_optim = optim.state_dict()\n\n        dset_trn_size = len(dl_trn)\n        dset_tst_size = len(dl_tst)\n\n        for epoch in tqdm(range(epochs), desc=f\"Training {algo.upper()}\"):\n            # Train\n            self.net.train()\n            loss_trn_val = 0\n            for inp_trn in dl_trn:\n                inp_trn = pt.squeeze(inp_trn, dim=0).swapaxes(0, 1)\n                mask = _random_probe_mask(inp_trn.shape[-2:], mask_shape, ratio_blind_spots=ratio_blind_spot)\n                to_damage = np.where(mask &gt; 0)\n                to_check = np.where(mask &gt; 1)\n                inp_trn_damaged = pt.clone(inp_trn)\n                size_to_damage = inp_trn_damaged[:, :, to_damage[0], to_damage[1]].shape\n                inp_trn_damaged[:, :, to_damage[0], to_damage[1]] = pt.randn(\n                    size_to_damage, device=inp_trn.device, dtype=inp_trn.dtype\n                )\n\n                optim.zero_grad()\n                out_trn = self.net(inp_trn_damaged)\n                out_to_check = out_trn[:, :, to_check[0], to_check[1]].flatten()\n                ref_to_check = inp_trn[:, :, to_check[0], to_check[1]].flatten()\n                loss_trn = loss_trn_fn(out_to_check, ref_to_check)\n                if self.reg_val is not None:\n                    loss_trn += losses.LossTV(self.reg_val, reduction=\"sum\")(out_trn)\n                loss_trn.backward()\n\n                loss_trn_val += loss_trn.item()\n\n                optim.step()\n\n            losses_trn.append(loss_trn_val / dset_trn_size)\n\n            # Test\n            self.net.eval()\n            loss_tst_val = 0\n            with pt.inference_mode():\n                for inp_tst in dl_tst:\n                    inp_tst = pt.squeeze(inp_tst, dim=0).swapaxes(0, 1)\n                    mask = _random_probe_mask(inp_tst.shape[-2:], mask_shape, ratio_blind_spots=ratio_blind_spot)\n                    to_damage = np.where(mask &gt; 0)\n                    to_check = np.where(mask &gt; 1)\n                    inp_tst_damaged = pt.clone(inp_tst)\n                    size_to_damage = inp_tst_damaged[:, :, to_damage[0], to_damage[1]].shape\n                    inp_tst_damaged[:, :, to_damage[0], to_damage[1]] = pt.randn(\n                        size_to_damage, device=inp_tst.device, dtype=inp_tst.dtype\n                    )\n\n                    out_tst = self.net(inp_tst_damaged)\n                    out_to_check = out_tst[:, :, to_check[0], to_check[1]].flatten()\n                    ref_to_check = inp_tst[:, :, to_check[0], to_check[1]].flatten()\n                    loss_tst = loss_tst_fn(out_to_check, ref_to_check)\n\n                    loss_tst_val += loss_tst.item()\n\n                losses_tst.append(loss_tst_val / dset_tst_size)\n\n            # Check improvement\n            if losses_tst[-1] &lt; best_loss_tst if losses_tst[-1] is not None else False:\n                best_loss_tst = losses_tst[-1]\n                best_epoch = epoch\n                best_state = cp.deepcopy(self.net.state_dict())\n                best_optim = cp.deepcopy(optim.state_dict())\n\n            # Save epoch\n            if self.save_epochs:\n                self._save_state(epoch, self.net.state_dict(), optim.state_dict())\n\n        print(f\"Best epoch: {best_epoch}, with tst_loss: {best_loss_tst:.5}\")\n        if self.save_epochs:\n            self._save_state(best_epoch, best_state, best_optim, is_final=True)\n\n        self.net.load_state_dict(best_state)\n\n        losses_trn = np.array(losses_trn)\n        losses_tst = np.array(losses_tst)\n\n        return losses_trn, losses_tst\n</code></pre>"},{"location":"reference/algorithms/#autoden.algorithms.N2V.train_selfsupervised","title":"<code>train_selfsupervised(inp, epochs, dset_split, mask_shape=1, ratio_blind_spot=0.015, algo='adam')</code>","text":"<p>Self-supervised training.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>NDArray</code> <p>The input images, which will also be targets</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs</p> required <code>dset_split</code> <code>DatasetSplit</code> <p>How to split the dataset in training and validation set</p> required <code>mask_shape</code> <code>int | Sequence[int] | NDArray</code> <p>Shape of the blind spot mask, by default 1.</p> <code>1</code> <code>algo</code> <code>str</code> <p>Learning algorithm to use, by default \"adam\"</p> <code>'adam'</code> Source code in <code>src/autoden/algorithms.py</code> <pre><code>def train_selfsupervised(\n    self,\n    inp: NDArray,\n    epochs: int,\n    dset_split: DatasetSplit,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n):\n    \"\"\"Self-supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images, which will also be targets\n    epochs : int\n        Number of training epochs\n    dset_split : DatasetSplit\n        How to split the dataset in training and validation set\n    mask_shape : int | Sequence[int] | NDArray\n        Shape of the blind spot mask, by default 1.\n    algo : str, optional\n        Learning algorithm to use, by default \"adam\"\n    \"\"\"\n    range_vals_tgt = range_vals_inp = _get_normalization(inp, percentile=0.001)\n\n    self.data_scaling_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    self.data_scaling_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n\n    self.data_bias_inp = inp.mean() * self.data_scaling_inp\n    self.data_bias_tgt = inp.mean() * self.data_scaling_tgt\n\n    # Rescale the datasets\n    inp = inp * self.data_scaling_inp - self.data_bias_inp\n\n    inp_trn = inp[dset_split.trn_inds]\n    inp_tst = inp[dset_split.tst_inds]\n\n    dsets_trn = datasets.NumpyDataset(inp_trn, n_channels=self.n_channels)\n    dsets_tst = datasets.NumpyDataset(inp_tst, n_channels=self.n_channels)\n\n    # Create datasets\n    dset_trn = datasets.InferenceDataset(dsets_trn, device=self.device)\n    dset_tst = datasets.InferenceDataset(dsets_tst, device=self.device)\n\n    dl_trn = DataLoader(dset_trn, batch_size=self.batch_size)\n    dl_tst = DataLoader(dset_tst, batch_size=self.batch_size * 16)\n\n    losses_trn, losses_tst = self._train_n2v(\n        dl_trn, dl_tst, epochs=epochs, mask_shape=mask_shape, ratio_blind_spot=ratio_blind_spot, algo=algo\n    )\n\n    self._plot_loss_curves(losses_trn, losses_tst, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n</code></pre>"},{"location":"reference/cli/","title":"cli.py","text":"<p>Module that contains the command line application.</p>"},{"location":"reference/cli/#autoden.cli.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the CLI argument parser.</p> <p>Returns:     An argparse parser.</p> Source code in <code>src/autoden/cli.py</code> <pre><code>def get_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"\n    Return the CLI argument parser.\n\n    Returns:\n        An argparse parser.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"autoden\",\n        description=\"Denoise the given images, using deep-learning-based unsupervised or self-supervised algorithms.\",\n    )\n    parser.add_argument(\"algorithm\", choices=[\"N2N\", \"N2V\", \"DIP\"], help=\"Denoising algorithm to use\")\n    parser.add_argument(\n        \"--epochs\",\n        \"-e\",\n        type=int,\n        help=f\"Number of epochs to use, by default {DEFAULT_EPOCHS}.\",\n        metavar=\"E\",\n        default=DEFAULT_EPOCHS,\n    )\n    parser.add_argument(\n        \"--unet-levels\",\n        \"-l\",\n        type=int,\n        help=f\"Number of UNet levels to use, by default: {NetworkParamsUNet.DEFAULT_LEVELS}.\",\n        default=NetworkParamsUNet.DEFAULT_LEVELS,\n        metavar=\"L\",\n    )\n    parser.add_argument(\n        \"--unet-features\",\n        \"-f\",\n        type=int,\n        help=f\"Number of UNet features to use, by default: {NetworkParamsUNet.DEFAULT_FEATURES}.\",\n        default=NetworkParamsUNet.DEFAULT_FEATURES,\n        metavar=\"F\",\n    )\n    parser.add_argument(\n        \"--regularization\",\n        \"-r\",\n        type=float,\n        help=f\"Total Variation regularization value, by default: {DEFAULT_TV_VAL}.\",\n        default=DEFAULT_TV_VAL,\n        metavar=\"R\",\n    )\n    parser.add_argument(\"src_file\", nargs=\"+\", help=\"Path of each input image.\", type=argparse.FileType(\"rb\"))\n    parser.add_argument(\"dst_file\", help=\"Path of the output image.\", type=argparse.FileType(\"wb\"))\n    parser.add_argument(\"--version\", action=\"version\", version=f\"%(prog)s {__version__}\")\n    return parser\n</code></pre>"},{"location":"reference/cli/#autoden.cli.main","title":"<code>main(args=None)</code>","text":"<p>Run the main program.</p> <p>This function is executed when you type <code>autoden</code> or <code>python -m autoden</code>.</p> <p>Arguments:     args: Arguments passed from the command line.</p> <p>Returns:     An exit code.</p> Source code in <code>src/autoden/cli.py</code> <pre><code>def main(args: list[str] | None = None) -&gt; int:\n    \"\"\"\n    Run the main program.\n\n    This function is executed when you type `autoden` or `python -m autoden`.\n\n    Arguments:\n        args: Arguments passed from the command line.\n\n    Returns:\n        An exit code.\n    \"\"\"\n    parser = get_parser()\n    opts = parser.parse_args(args=args)\n    # print(opts)  # noqa: WPS421 (side-effect in main is fine)\n\n    inp_imgs = [iio.imread(f) for f in opts.src_file]\n    if any(x.ndim &gt; 2 for x in inp_imgs):\n        print(\"Color images not supported, yet.\")\n        return 1\n\n    net_pars = NetworkParamsUNet(n_levels=opts.unet_levels, n_features=opts.unet_features)\n    if opts.algorithm.upper() == \"DIP\":\n        algo = DIP(\"\", network_type=net_pars, save_epochs=False, reg_tv_val=opts.regularization)\n        inp_img = algo.train_unsupervised(np.stack(inp_imgs, axis=0), epochs=opts.epochs)\n        out_img = algo.infer(inp_img)\n        iio.imwrite(opts.dst_file, out_img)\n    else:\n        print(f\"Not implemented support for algorithm {opts.algorithm} in command-line, yet.\")\n        return 1\n    return 0\n</code></pre>"},{"location":"reference/datasets/","title":"datasets.py","text":"<p>Implement data handling classes.</p>"},{"location":"reference/datasets/#autoden.datasets.DataProxy","title":"<code>DataProxy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Provide base interface.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class DataProxy(ABC):\n    \"\"\"Provide base interface.\"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index: Any) -&gt; NDArray:\n        \"\"\"Return selected item.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return length of the dataset.\"\"\"\n\n    @property\n    @abstractmethod\n    def shape(self) -&gt; tuple:\n        \"\"\"Return shape of the dataset.\"\"\"\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.DataProxy.shape","title":"<code>shape: tuple</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return shape of the dataset.</p>"},{"location":"reference/datasets/#autoden.datasets.DataProxy.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>Return selected item.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, index: Any) -&gt; NDArray:\n    \"\"\"Return selected item.\"\"\"\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.DataProxy.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return length of the dataset.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return length of the dataset.\"\"\"\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>Provide base interface.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class Dataset(DatasetBase, ABC):\n    \"\"\"Provide base interface.\"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index):\n        \"\"\"Return selected item.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return length of the dataset.\"\"\"\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.Dataset.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>Return selected item.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, index):\n    \"\"\"Return selected item.\"\"\"\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.Dataset.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return length of the dataset.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return length of the dataset.\"\"\"\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.InferenceDataset","title":"<code>InferenceDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Provide input data for supervised training.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class InferenceDataset(Dataset):\n    \"\"\"Provide input data for supervised training.\"\"\"\n\n    def __init__(\n        self,\n        input_ds: NDArray | DataProxy,\n        device: str,\n        dtype: DTypeLike = np.float32,\n    ) -&gt; None:\n        super().__init__()\n        self.input_ds = input_ds\n        self.device = device\n        self.dtype = dtype\n\n        if isinstance(self.input_ds, (np.ndarray, pt.Tensor)) and self.input_ds.ndim == 2:\n            self.input_ds = self.input_ds[None, :]\n\n    def __getitem__(self, img_ind: int) -&gt; pt.Tensor:\n        output = np.array(self.input_ds[img_ind : img_ind + 1 :], dtype=self.dtype)\n        output = pt.tensor(output)\n        return output.to(self.device, non_blocking=True)\n\n    def __len__(self) -&gt; int:\n        return len(self.input_ds)\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.NumpyDataset","title":"<code>NumpyDataset</code>","text":"<p>               Bases: <code>DataProxy</code></p> <p>Handle in-memory datasets.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class NumpyDataset(DataProxy):\n    \"\"\"Handle in-memory datasets.\"\"\"\n\n    input_ds: NDArray\n\n    def __init__(self, input_ds: NDArray, n_channels: int = 1, dtype: DTypeLike = np.float32) -&gt; None:\n        super().__init__()\n\n        self.n_channels = n_channels\n\n        # Add channel dimensions if not present\n        if input_ds.ndim &lt; 2:\n            raise ValueError(\"Input data is one-dimentional, but a two-dimentional image (or stack of images) is expected.\")\n        elif input_ds.ndim == 2:\n            if self.n_channels &gt; 1:\n                raise ValueError(f\"Input image is single-channel, but it should have {self.n_channels} channels\")\n            input_ds = input_ds[None, None, ...]\n        elif input_ds.ndim &gt;= 3:\n            if input_ds.shape[-3] != self.n_channels and self.n_channels &gt; 1:\n                raise ValueError(f\"Input image should have {self.n_channels} channels, but it has {input_ds.shape[0]} instead\")\n            if input_ds.ndim == 3:\n                input_ds = input_ds[None, ...]\n\n        self.input_ds = input_ds\n        self.dtype = dtype\n\n    def __getitem__(self, ind: int) -&gt; NDArray:\n        return self.input_ds[ind].astype(self.dtype)\n\n    def __len__(self) -&gt; int:\n        return len(self.input_ds)\n\n    @property\n    def shape(self) -&gt; tuple:\n        return self.input_ds.shape\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.SelfsupervisedDataset","title":"<code>SelfsupervisedDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Provide input data for self-supervised training.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class SelfsupervisedDataset(Dataset):\n    \"\"\"Provide input data for self-supervised training.\"\"\"\n\n    STRATEGIES = (\"X:1\", \"1:X\", \"1:1\")\n\n    def __init__(\n        self,\n        *datasets: NDArray | DataProxy,\n        device: str,\n        do_flip: bool = True,\n        do_rotation: bool = True,\n        strategy: str = \"1:X\",\n        verbose: bool = False,\n    ) -&gt; None:\n        super().__init__()\n\n        self.device = device\n        self.do_flip = do_flip\n        self.do_rotation = do_rotation\n        if verbose:\n            print(f\"{self.do_flip = }\")\n            print(f\"{self.do_rotation = }\")\n\n        self.datasets = datasets\n\n        dset_shapes = [ds.shape for ds in self.datasets]\n        wrong_shapes = [np.array(ds_shape) != np.array(dset_shapes[0]) for ds_shape in dset_shapes[1:]]\n        if np.any(wrong_shapes):\n            raise ValueError(\n                f\"Dataset shapes should all be the same shape ({dset_shapes[0]}),\"\n                f\" but following datasets have different shapes: {np.where(wrong_shapes)[0]+1}\"\n            )\n\n        if dset_shapes[0][-2] != dset_shapes[0][-1]:\n            if self.do_rotation:\n                print(f\"WARNING: Rotations are disabled when the images are not square (dset.shape: {dset_shapes[0]})\")\n            self.do_rotation = False\n\n        if strategy == \"X:1\":\n            num_input = self.num_splits - 1\n        elif strategy in [\"1:X\", \"1:1\"]:\n            num_input = 1\n        else:\n            raise ValueError(f\"Strategy {strategy} not supported. It should be one of: {self.STRATEGIES}\")\n\n        if strategy in [\"X:1\", \"1:X\"]:\n            split_idxs = set(range(self.num_splits))\n            self.input_idxs = list(combinations(split_idxs, num_input))\n            self.target_idxs = [split_idxs - set(idxs) for idxs in self.input_idxs]\n        elif strategy == \"1:1\":\n            self.input_idxs = np.arange(self.num_splits)\n            self.target_idxs = np.stack((self.input_idxs[1::2], self.input_idxs[0::2]), axis=-1)\n            self.target_idxs = [{ii} for ii in self.target_idxs.flatten()]\n            self.input_idxs = [{ii} for ii in self.input_idxs]\n\n        if verbose:\n            print(f\"{self.input_idxs = }\")\n            print(f\"{self.target_idxs = }\")\n\n    @property\n    def num_splits(self) -&gt; int:\n        return len(self.datasets)\n\n    @property\n    def num_slices(self) -&gt; int:\n        return len(self.datasets[0])\n\n    def __getitem__(self, img_ind: int) -&gt; tuple[pt.Tensor, pt.Tensor]:\n        num_splits = self.num_splits\n        slice_idx = img_ind // num_splits\n        split_idx = img_ind % num_splits\n\n        input_idxs = self.input_idxs[split_idx]\n        target_idxs = self.target_idxs[split_idx]\n\n        slices = [pt.Tensor(ds[slice_idx]) for ds in self.datasets]\n        inputs = [slices[j] for j in input_idxs]\n        targets = [slices[j] for j in target_idxs]\n\n        inp = pt.mean(pt.stack(inputs), dim=0)\n        tgt = pt.mean(pt.stack(targets), dim=0)\n\n        inp = inp.to(self.device, non_blocking=True)\n        tgt = tgt.to(self.device, non_blocking=True)\n\n        if self.do_flip:\n            inp, tgt = _random_flip_images((inp, tgt))\n\n        if self.do_rotation:\n            inp, tgt = _random_rotate_images((inp, tgt))\n\n        return inp, tgt\n\n    def __len__(self) -&gt; int:\n        return self.num_splits * self.num_slices\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.SupervisedDataset","title":"<code>SupervisedDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Provide input data for supervised training.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class SupervisedDataset(Dataset):\n    \"\"\"Provide input data for supervised training.\"\"\"\n\n    def __init__(\n        self,\n        input_ds: NDArray | DataProxy,\n        target_ds: NDArray | DataProxy,\n        device: str,\n        do_flip: bool = True,\n        do_rotation: bool = True,\n        dtype: DTypeLike = np.float32,\n    ) -&gt; None:\n        super().__init__()\n        self.input_ds = input_ds\n        self.target_ds = target_ds\n        self.do_flip = do_flip\n        self.do_rotation = do_rotation\n        self.device = device\n        self.dtype = dtype\n\n        if len(input_ds) != len(target_ds):\n            raise ValueError(\n                f\"Input dataset length (n. images: {len(input_ds)}) should be equal\"\n                f\" to the target dataset length (n. images: {len(target_ds)})\"\n            )\n\n        if self.input_ds.shape[-2] != self.input_ds.shape[-1]:\n            if self.do_rotation:\n                print(f\"WARNING: Rotations are disabled when the images are not square (inp.shape: {self.input_ds.shape})\")\n            self.do_rotation = False\n\n    def __getitem__(self, img_ind: int) -&gt; tuple[pt.Tensor, pt.Tensor]:\n        inp = pt.tensor(np.array(self.input_ds[img_ind : img_ind + 1 :], dtype=self.dtype))\n        tgt = pt.tensor(np.array(self.target_ds[img_ind : img_ind + 1 :], dtype=self.dtype))\n\n        inp = inp.to(self.device, non_blocking=True)\n        tgt = tgt.to(self.device, non_blocking=True)\n\n        if self.do_flip:\n            inp, tgt = _random_flip_images((inp, tgt))\n\n        if self.do_rotation:\n            inp, tgt = _random_rotate_images((inp, tgt))\n\n        return inp, tgt\n\n    def __len__(self) -&gt; int:\n        return len(self.input_ds)\n</code></pre>"},{"location":"reference/datasets/#autoden.datasets.TiffDataset","title":"<code>TiffDataset</code>","text":"<p>               Bases: <code>DataProxy</code></p> <p>Handle on-disk TIFF datasets.</p> Source code in <code>src/autoden/datasets.py</code> <pre><code>class TiffDataset(DataProxy):\n    \"\"\"Handle on-disk TIFF datasets.\"\"\"\n\n    def __init__(self, files_pattern, dtype: DTypeLike = np.float32, verbose: bool = False) -&gt; None:\n        super().__init__()\n        files_pattern = Path(files_pattern)\n        self.paths = sorted(Path(files_pattern.parent).glob(files_pattern.name))\n        if verbose:\n            print(self.paths)\n        self.dtype = dtype\n\n        if len(self.paths) == 0:\n            raise ValueError(f\"No images found for path: {files_pattern}\")\n\n        self._shape = self[0].shape\n        if self.shape[0] == 1:\n            self._shape = (len(self), *self._shape[1:])\n        else:\n            self._shape = (len(self), *self._shape)\n\n    def __getitem__(self, img_ind: int) -&gt; NDArray:\n        img = np.float32(iio.imread(str(self.paths[img_ind])))\n\n        # Add channel dimension if not present\n        if img.ndim == 2:\n            img = img[None, ...]\n\n        return np.array(img, dtype=self.dtype)\n\n    def __len__(self) -&gt; int:\n        return len(self.paths)\n\n    @property\n    def shape(self) -&gt; tuple:\n        return self._shape\n</code></pre>"},{"location":"reference/losses/","title":"losses.py","text":"<p>Data losses definitions.</p>"},{"location":"reference/losses/#autoden.losses.LossST","title":"<code>LossST</code>","text":"<p>               Bases: <code>MSELoss</code></p> Source code in <code>src/autoden/losses.py</code> <pre><code>class LossST(nn.MSELoss):\n    def __init__(\n        self, lambda_val: float, size_average=None, reduce=None, reduction: str = \"mean\", isotropic: bool = True\n    ) -&gt; None:\n        super().__init__(size_average, reduce, reduction)\n        self.lambda_val = lambda_val\n        self.isotropic = isotropic\n\n    def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n        \"\"\"Compute total variation statistics on current batch.\"\"\"\n        if img.ndim != 4:\n            raise RuntimeError(f\"Expected input `img` to be an 3D tensor, but got {img.shape}\")\n        axes = [-3, -2, -1]\n\n        diff1 = _differentiate(img, dim=-1)\n        diff2 = _differentiate(img, dim=-2)\n        if self.isotropic:\n            tv_val = pt.sqrt(pt.pow(diff1, 2) + pt.pow(diff2, 2))\n        else:\n            tv_val = diff1.abs() + diff2.abs()\n\n        dd_11 = _differentiate(diff1, dim=-1)\n        dd_12 = _differentiate(diff1, dim=-2)\n        dd_21 = _differentiate(diff2, dim=-1)\n        dd_22 = _differentiate(diff2, dim=-2)\n\n        jac_val = pt.sqrt(pt.pow(dd_11, 2) + pt.pow(dd_12, 2) + pt.pow(dd_21, 2) + pt.pow(dd_22, 2))\n\n        return self.lambda_val * (tv_val.sum(axes).mean() + jac_val.sum(axes).mean() / 4)\n</code></pre>"},{"location":"reference/losses/#autoden.losses.LossST.forward","title":"<code>forward(img)</code>","text":"<p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    if img.ndim != 4:\n        raise RuntimeError(f\"Expected input `img` to be an 3D tensor, but got {img.shape}\")\n    axes = [-3, -2, -1]\n\n    diff1 = _differentiate(img, dim=-1)\n    diff2 = _differentiate(img, dim=-2)\n    if self.isotropic:\n        tv_val = pt.sqrt(pt.pow(diff1, 2) + pt.pow(diff2, 2))\n    else:\n        tv_val = diff1.abs() + diff2.abs()\n\n    dd_11 = _differentiate(diff1, dim=-1)\n    dd_12 = _differentiate(diff1, dim=-2)\n    dd_21 = _differentiate(diff2, dim=-1)\n    dd_22 = _differentiate(diff2, dim=-2)\n\n    jac_val = pt.sqrt(pt.pow(dd_11, 2) + pt.pow(dd_12, 2) + pt.pow(dd_21, 2) + pt.pow(dd_22, 2))\n\n    return self.lambda_val * (tv_val.sum(axes).mean() + jac_val.sum(axes).mean() / 4)\n</code></pre>"},{"location":"reference/losses/#autoden.losses.LossTV","title":"<code>LossTV</code>","text":"<p>               Bases: <code>MSELoss</code></p> Source code in <code>src/autoden/losses.py</code> <pre><code>class LossTV(nn.MSELoss):\n    def __init__(\n        self,\n        lambda_val: float,\n        size_average=None,\n        reduce=None,\n        reduction: str = \"mean\",\n        isotropic: bool = True,\n        diff_dims: Sequence[int] | None = None,\n    ) -&gt; None:\n        super().__init__(size_average, reduce, reduction)\n        self.lambda_val = lambda_val\n        self.isotropic = isotropic\n        if diff_dims is None:\n            diff_dims = [-2, -1]\n        self.diff_dims = diff_dims\n\n    def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n        \"\"\"Compute total variation statistics on current batch.\"\"\"\n        if img.ndim != 4:\n            raise RuntimeError(f\"Expected input `img` to be an 3D tensor, but got {img.shape}\")\n        axes = [-3, -2, -1]\n\n        diff1 = _differentiate(img, dim=-1)\n        diff2 = _differentiate(img, dim=-2)\n        if self.isotropic:\n            tv_val = pt.sqrt(pt.pow(diff1, 2) + pt.pow(diff2, 2))\n        else:\n            tv_val = diff1.abs() + diff2.abs()\n\n        return self.lambda_val * tv_val.sum(axes).mean()\n</code></pre>"},{"location":"reference/losses/#autoden.losses.LossTV.forward","title":"<code>forward(img)</code>","text":"<p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    if img.ndim != 4:\n        raise RuntimeError(f\"Expected input `img` to be an 3D tensor, but got {img.shape}\")\n    axes = [-3, -2, -1]\n\n    diff1 = _differentiate(img, dim=-1)\n    diff2 = _differentiate(img, dim=-2)\n    if self.isotropic:\n        tv_val = pt.sqrt(pt.pow(diff1, 2) + pt.pow(diff2, 2))\n    else:\n        tv_val = diff1.abs() + diff2.abs()\n\n    return self.lambda_val * tv_val.sum(axes).mean()\n</code></pre>"},{"location":"reference/models/config/","title":"config.py","text":"<p>High level definition of CNN architectures.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p>"},{"location":"reference/models/config/#autoden.models.config.NetworkParams","title":"<code>NetworkParams</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for storing network parameters.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>class NetworkParams(ABC):\n    \"\"\"Abstract base class for storing network parameters.\"\"\"\n\n    n_channels_in: int\n    n_channels_out: int\n    n_features: int\n\n    def __init__(self, n_features: int, n_channels_in: int = 1, n_channels_out: int = 1) -&gt; None:\n        self.n_channels_in = n_channels_in\n        self.n_channels_out = n_channels_out\n        self.n_features = n_features\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Produce the string representation of the object.\n\n        Returns\n        -------\n        str\n            The string representation.\n        \"\"\"\n        return self.__class__.__name__ + \" {\\n\" + \",\\n\".join([f\"  {k} = {v}\" for k, v in self.__dict__.items()]) + \"\\n}\"\n\n    @abstractmethod\n    def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n        \"\"\"Get the associated model with the selected parameters.\n\n        Parameters\n        ----------\n        device : str, optional\n            The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n        Returns\n        -------\n        Module\n            The model.\n        \"\"\"\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParams.__repr__","title":"<code>__repr__()</code>","text":"<p>Produce the string representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Produce the string representation of the object.\n\n    Returns\n    -------\n    str\n        The string representation.\n    \"\"\"\n    return self.__class__.__name__ + \" {\\n\" + \",\\n\".join([f\"  {k} = {v}\" for k, v in self.__dict__.items()]) + \"\\n}\"\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParams.get_model","title":"<code>get_model(device='cuda' if is_cuda_available() else 'cpu')</code>  <code>abstractmethod</code>","text":"<p>Get the associated model with the selected parameters.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> <code>'cuda' if is_available() else 'cpu'</code> <p>Returns:</p> Type Description <code>Module</code> <p>The model.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>@abstractmethod\ndef get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get the associated model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsDnCNN","title":"<code>NetworkParamsDnCNN</code>","text":"<p>               Bases: <code>NetworkParams</code></p> <p>Store DnCNN parameters.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>class NetworkParamsDnCNN(NetworkParams):\n    \"\"\"Store DnCNN parameters.\"\"\"\n\n    n_layers: int\n\n    def __init__(self, n_channels_in: int = 1, n_channels_out: int = 1, n_layers: int = 20, n_features: int = 64) -&gt; None:\n        \"\"\"Initialize the DnCNN network parameters definition.\n\n        Parameters\n        ----------\n        n_channels_in : int, optional\n            Number of input channels. Default is 1.\n        n_channels_out : int, optional\n            Number of output channels. Default is 1.\n        n_layers : int, optional\n            Number of layers. Default is 20.\n        n_features : int, optional\n            Number of features. Default is 64.\n        \"\"\"\n        super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n        self.n_layers = n_layers\n\n    def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n        \"\"\"Get a DnCNN model with the selected parameters.\n\n        Parameters\n        ----------\n        device : str, optional\n            The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n        Returns\n        -------\n        Module\n            The DnCNN model.\n        \"\"\"\n        return DnCNN(\n            n_channels_in=self.n_channels_in,\n            n_channels_out=self.n_channels_out,\n            n_layers=self.n_layers,\n            n_features=self.n_features,\n            device=device,\n        )\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsDnCNN.__init__","title":"<code>__init__(n_channels_in=1, n_channels_out=1, n_layers=20, n_features=64)</code>","text":"<p>Initialize the DnCNN network parameters definition.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels_in</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>n_channels_out</code> <code>int</code> <p>Number of output channels. Default is 1.</p> <code>1</code> <code>n_layers</code> <code>int</code> <p>Number of layers. Default is 20.</p> <code>20</code> <code>n_features</code> <code>int</code> <p>Number of features. Default is 64.</p> <code>64</code> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(self, n_channels_in: int = 1, n_channels_out: int = 1, n_layers: int = 20, n_features: int = 64) -&gt; None:\n    \"\"\"Initialize the DnCNN network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_layers : int, optional\n        Number of layers. Default is 20.\n    n_features : int, optional\n        Number of features. Default is 64.\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_layers = n_layers\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsDnCNN.get_model","title":"<code>get_model(device='cuda' if is_cuda_available() else 'cpu')</code>","text":"<p>Get a DnCNN model with the selected parameters.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> <code>'cuda' if is_available() else 'cpu'</code> <p>Returns:</p> Type Description <code>Module</code> <p>The DnCNN model.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a DnCNN model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The DnCNN model.\n    \"\"\"\n    return DnCNN(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsMSD","title":"<code>NetworkParamsMSD</code>","text":"<p>               Bases: <code>NetworkParams</code></p> <p>Store MS-D net parameters.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>class NetworkParamsMSD(NetworkParams):\n    \"\"\"Store MS-D net parameters.\"\"\"\n\n    dilations: Sequence[int] | NDArray[np.integer]\n    n_layers: int\n\n    def __init__(\n        self,\n        n_channels_in: int = 1,\n        n_channels_out: int = 1,\n        n_layers: int = 80,\n        n_features: int = 1,\n        dilations: Sequence[int] | NDArray[np.integer] = np.arange(1, 10),\n    ) -&gt; None:\n        \"\"\"Initialize the MS-D network parameters definition.\n\n        Parameters\n        ----------\n        n_channels_in : int, optional\n            Number of input channels, by default 1.\n        n_channels_out : int, optional\n            Number of output channels, by default 1.\n        n_layers : int, optional\n            Number of layers in the network, by default 80.\n        n_features : int, optional\n            Number of features, by default 1.\n        dilations : Sequence[int] | NDArray[np.integer], optional\n            Dilation values for the network, by default np.arange(1, 10).\n        \"\"\"\n        super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n        self.n_layers = n_layers\n        self.dilations = dilations\n\n    def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n        \"\"\"Get a MS-D net model with the selected parameters.\n\n        Parameters\n        ----------\n        device : str, optional\n            The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n        Returns\n        -------\n        Module\n            The model.\n        \"\"\"\n        return MSDnet(\n            self.n_channels_in,\n            self.n_channels_out,\n            n_layers=self.n_layers,\n            n_features=self.n_features,\n            dilations=list(self.dilations),\n            device=device,\n        )\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsMSD.__init__","title":"<code>__init__(n_channels_in=1, n_channels_out=1, n_layers=80, n_features=1, dilations=np.arange(1, 10))</code>","text":"<p>Initialize the MS-D network parameters definition.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels_in</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>n_channels_out</code> <code>int</code> <p>Number of output channels, by default 1.</p> <code>1</code> <code>n_layers</code> <code>int</code> <p>Number of layers in the network, by default 80.</p> <code>80</code> <code>n_features</code> <code>int</code> <p>Number of features, by default 1.</p> <code>1</code> <code>dilations</code> <code>Sequence[int] | NDArray[integer]</code> <p>Dilation values for the network, by default np.arange(1, 10).</p> <code>arange(1, 10)</code> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 80,\n    n_features: int = 1,\n    dilations: Sequence[int] | NDArray[np.integer] = np.arange(1, 10),\n) -&gt; None:\n    \"\"\"Initialize the MS-D network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels, by default 1.\n    n_channels_out : int, optional\n        Number of output channels, by default 1.\n    n_layers : int, optional\n        Number of layers in the network, by default 80.\n    n_features : int, optional\n        Number of features, by default 1.\n    dilations : Sequence[int] | NDArray[np.integer], optional\n        Dilation values for the network, by default np.arange(1, 10).\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_layers = n_layers\n    self.dilations = dilations\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsMSD.get_model","title":"<code>get_model(device='cuda' if is_cuda_available() else 'cpu')</code>","text":"<p>Get a MS-D net model with the selected parameters.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> <code>'cuda' if is_available() else 'cpu'</code> <p>Returns:</p> Type Description <code>Module</code> <p>The model.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a MS-D net model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n    return MSDnet(\n        self.n_channels_in,\n        self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        dilations=list(self.dilations),\n        device=device,\n    )\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsUNet","title":"<code>NetworkParamsUNet</code>","text":"<p>               Bases: <code>NetworkParams</code></p> <p>Store UNet parameters.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>class NetworkParamsUNet(NetworkParams):\n    \"\"\"Store UNet parameters.\"\"\"\n\n    n_levels: int\n\n    DEFAULT_LEVELS: int = 3\n    DEFAULT_FEATURES: int = 32\n\n    def __init__(\n        self,\n        n_channels_in: int = 1,\n        n_channels_out: int = 1,\n        n_levels: int = DEFAULT_LEVELS,\n        n_features: int = DEFAULT_FEATURES,\n        n_channels_skip: int | None = None,\n        bilinear: bool = True,\n        pad_mode: str = \"replicate\",\n    ) -&gt; None:\n        \"\"\"Initialize the UNet network parameters definition.\n\n        Parameters\n        ----------\n        n_channels_in : int, optional\n            Number of input channels. Default is 1.\n        n_channels_out : int, optional\n            Number of output channels. Default is 1.\n        n_levels : int, optional\n            Number of levels in the UNet. Default is 3.\n        n_features : int, optional\n            Number of features in the UNet. Default is 32.\n        n_channels_skip : int, optional\n            Number of skip connections channels. Default is None.\n        bilinear : bool, optional\n            Whether to use bilinear interpolation. Default is True.\n        pad_mode : str, optional\n            Padding mode for convolutional layers. Default is \"replicate\".\n        \"\"\"\n        super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n        self.n_levels = n_levels\n        self.n_channels_skip = n_channels_skip\n        self.bilinear = bilinear\n        self.pad_mode = pad_mode\n\n    def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n        \"\"\"Get a U-net model with the selected parameters.\n\n        Parameters\n        ----------\n        device : str, optional\n            The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n        Returns\n        -------\n        Module\n            The U-net model.\n        \"\"\"\n        return UNet(\n            n_channels_in=self.n_channels_in,\n            n_channels_out=self.n_channels_out,\n            n_features=self.n_features,\n            n_levels=self.n_levels,\n            n_channels_skip=self.n_channels_skip,\n            bilinear=self.bilinear,\n            pad_mode=self.pad_mode,\n            device=device,\n        )\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsUNet.__init__","title":"<code>__init__(n_channels_in=1, n_channels_out=1, n_levels=DEFAULT_LEVELS, n_features=DEFAULT_FEATURES, n_channels_skip=None, bilinear=True, pad_mode='replicate')</code>","text":"<p>Initialize the UNet network parameters definition.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels_in</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>n_channels_out</code> <code>int</code> <p>Number of output channels. Default is 1.</p> <code>1</code> <code>n_levels</code> <code>int</code> <p>Number of levels in the UNet. Default is 3.</p> <code>DEFAULT_LEVELS</code> <code>n_features</code> <code>int</code> <p>Number of features in the UNet. Default is 32.</p> <code>DEFAULT_FEATURES</code> <code>n_channels_skip</code> <code>int</code> <p>Number of skip connections channels. Default is None.</p> <code>None</code> <code>bilinear</code> <code>bool</code> <p>Whether to use bilinear interpolation. Default is True.</p> <code>True</code> <code>pad_mode</code> <code>str</code> <p>Padding mode for convolutional layers. Default is \"replicate\".</p> <code>'replicate'</code> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_levels: int = DEFAULT_LEVELS,\n    n_features: int = DEFAULT_FEATURES,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the UNet network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_levels : int, optional\n        Number of levels in the UNet. Default is 3.\n    n_features : int, optional\n        Number of features in the UNet. Default is 32.\n    n_channels_skip : int, optional\n        Number of skip connections channels. Default is None.\n    bilinear : bool, optional\n        Whether to use bilinear interpolation. Default is True.\n    pad_mode : str, optional\n        Padding mode for convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out)\n    self.n_levels = n_levels\n    self.n_channels_skip = n_channels_skip\n    self.bilinear = bilinear\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/models/config/#autoden.models.config.NetworkParamsUNet.get_model","title":"<code>get_model(device='cuda' if is_cuda_available() else 'cpu')</code>","text":"<p>Get a U-net model with the selected parameters.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> <code>'cuda' if is_available() else 'cpu'</code> <p>Returns:</p> Type Description <code>Module</code> <p>The U-net model.</p> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a U-net model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The U-net model.\n    \"\"\"\n    return UNet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_features=self.n_features,\n        n_levels=self.n_levels,\n        n_channels_skip=self.n_channels_skip,\n        bilinear=self.bilinear,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/models/dncnn/","title":"dncnn.py","text":""},{"location":"reference/models/dncnn/#autoden.models.dncnn.ConvBlock","title":"<code>ConvBlock</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>Convolution block: conv =&gt; BN =&gt; act.</p> Source code in <code>src/autoden/models/dncnn.py</code> <pre><code>class ConvBlock(nn.Sequential):\n    \"\"\"Convolution block: conv =&gt; BN =&gt; act.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, pad_mode: str = \"replicate\", last_block: bool = False):\n        pad_size = (kernel_size - 1) // 2\n        if last_block:\n            post_conv = []\n        else:\n            post_conv = [nn.BatchNorm2d(out_ch), nn.LeakyReLU(0.2, inplace=True)]\n        super().__init__(\n            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=False),\n            *post_conv,\n        )\n</code></pre>"},{"location":"reference/models/dncnn/#autoden.models.dncnn.DnCNN","title":"<code>DnCNN</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>Implementation of the DnCNN architecture from [1].</p> <p>[1] Zhang, et al., \"Beyond a Gaussian denoiser: Residual learning of deep CNN     for image denoising,\" IEEE Trans. on Image Processing, 2017.</p> Source code in <code>src/autoden/models/dncnn.py</code> <pre><code>class DnCNN(nn.Sequential):\n    \"\"\"Implementation of the DnCNN architecture from [1].\n\n    [1] Zhang, et al., \"Beyond a Gaussian denoiser: Residual learning of deep CNN\n        for image denoising,\" IEEE Trans. on Image Processing, 2017.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_channels_in: int,\n        n_channels_out: int,\n        n_layers: int = 20,\n        n_features: int = 32,\n        kernel_size: int = 3,\n        pad_mode: str = \"replicate\",\n        device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    ):\n        # From zhang-2017-beyon-gauss-denois:\n        #\n        #  Thus, for Gaussian denoising with a certain noise level, we\n        #  set the receptive field size of DnCNN to 35 \u00d7 35 with the\n        #  corresponding depth of 17. For other general image denoising\n        #  tasks, we adopt a larger receptive field and set the depth\n        #  to be 20.\n        #\n        # Hence, we set the standard depth to 20.\n        layers = [\n            ConvBlock(\n                n_channels_in if i_l == 0 else n_features,\n                n_channels_out if i_l == (n_layers - 1) else n_features,\n                kernel_size=kernel_size,\n                pad_mode=pad_mode,\n                last_block=(i_l == (n_layers - 1)),\n            )\n            for i_l in range(n_layers)\n        ]\n\n        super().__init__(*layers)\n        self.n_ch_in = n_channels_in\n        self.n_ch_out = n_channels_out\n        self.n_layers = n_layers\n        self.device = device\n\n        self.to(self.device)\n</code></pre>"},{"location":"reference/models/msd/","title":"msd.py","text":"<p>Module implmenting MS-D net.</p>"},{"location":"reference/models/msd/#autoden.models.msd.DilatedConvBlock","title":"<code>DilatedConvBlock</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>Dilated convolution block (dilated_conv =&gt; BN =&gt; ReLU).</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>class DilatedConvBlock(nn.Sequential):\n    \"\"\"Dilated convolution block (dilated_conv =&gt; BN =&gt; ReLU).\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, dilation: int = 1, pad_mode: str = \"replicate\") -&gt; None:\n        super().__init__(\n            nn.Conv2d(in_ch, out_ch, 3, padding=dilation, dilation=dilation, padding_mode=pad_mode),\n            nn.BatchNorm2d(out_ch),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n</code></pre>"},{"location":"reference/models/msd/#autoden.models.msd.MSDnet","title":"<code>MSDnet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Simple MS-D net implementation.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>class MSDnet(nn.Module):\n    \"\"\"Simple MS-D net implementation.\"\"\"\n\n    def __init__(\n        self,\n        n_channels_in: int = 1,\n        n_channels_out: int = 1,\n        n_layers: int = 12,\n        n_features: int = 1,\n        dilations: Sequence[int] = [1, 2, 3, 4],\n        device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    ) -&gt; None:\n        super().__init__()\n        self.n_ch_in = n_channels_in\n        self.n_ch_out = n_channels_out\n        self.dilations = dilations\n        self.n_layers = n_layers\n        self.n_nodes = n_features\n        self.device = device\n\n        convs = [\n            DilatedConvBlock(n_channels_in + n_features * ii, n_features, dilation=self._layer_dilation(ii))\n            for ii in range(n_layers)\n        ]\n        self.convs = nn.ModuleList(convs)\n        self.outc = nn.Conv2d(n_channels_in + n_features * n_layers, n_channels_out, kernel_size=1)\n\n        self.to(self.device)\n\n    def _layer_dilation(self, ind: int) -&gt; int:\n        return self.dilations[ind % len(self.dilations)]\n\n    def forward(self, x: pt.Tensor, return_latent: bool = False) -&gt; Union[pt.Tensor, tuple[pt.Tensor, pt.Tensor]]:\n        latent = [x]\n        for ii_layer in range(self.n_layers):\n            temp_x = pt.cat(latent, dim=1)\n            latent.append(self.convs[ii_layer](temp_x))\n\n        latent = pt.cat(latent, dim=1)\n        x = self.outc(latent)\n\n        if return_latent:\n            return x, latent\n        else:\n            return x\n</code></pre>"},{"location":"reference/models/unet/","title":"unet.py","text":"<p>Implementation of a flexible U-net.</p> <p>Originally inspired by: https://github.com/milesial/Pytorch-UNet</p>"},{"location":"reference/models/unet/#autoden.models.unet.ConvBlock","title":"<code>ConvBlock</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>Convolution block: conv =&gt; BN =&gt; act.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>class ConvBlock(nn.Sequential):\n    \"\"\"Convolution block: conv =&gt; BN =&gt; act.\"\"\"\n\n    def __init__(\n        self,\n        in_ch: int,\n        out_ch: int,\n        kernel_size: int,\n        stride: int = 1,\n        dilation: int = 1,\n        pad_mode: str = \"replicate\",\n        residual: bool = False,\n        bias: bool = True,\n        last_block: bool = False,\n    ):\n        pad_size = (kernel_size - 1) // 2 + (dilation - 1)\n        if last_block:\n            post_conv = []\n        else:\n            post_conv = [nn.BatchNorm2d(out_ch), nn.LeakyReLU(0.2, inplace=True)]\n        super().__init__(\n            nn.Conv2d(\n                in_ch,\n                out_ch,\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                padding=pad_size,\n                padding_mode=pad_mode,\n                bias=bias,\n            ),\n            *post_conv,\n        )\n        if residual and in_ch != out_ch:\n            print(f\"Warning: Residual connections not available when {in_ch=} is different from {out_ch=}\")\n            residual = False\n        self.residual = residual\n\n    def forward(self, inp: pt.Tensor) -&gt; pt.Tensor:\n        if self.residual:\n            return super().forward(inp) + inp\n        else:\n            return super().forward(inp)\n</code></pre>"},{"location":"reference/models/unet/#autoden.models.unet.DoubleConv","title":"<code>DoubleConv</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>Double convolution (conv =&gt; BN =&gt; ReLU) * 2.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>class DoubleConv(nn.Sequential):\n    \"\"\"Double convolution (conv =&gt; BN =&gt; ReLU) * 2.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, pad_mode: str = \"replicate\"):\n        super().__init__(\n            ConvBlock(in_ch, out_ch, kernel_size=3, pad_mode=pad_mode),\n            ConvBlock(out_ch, out_ch, kernel_size=1),\n        )\n</code></pre>"},{"location":"reference/models/unet/#autoden.models.unet.DownBlock","title":"<code>DownBlock</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>Down-scaling block.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>class DownBlock(nn.Sequential):\n    \"\"\"Down-scaling block.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = True, pad_mode: str = \"replicate\"):\n        if bilinear:\n            down_block = [nn.AvgPool2d(2)]\n        else:\n            down_block = [ConvBlock(in_ch, in_ch, kernel_size=2, stride=2)]\n        super().__init__(\n            *down_block,\n            DoubleConv(in_ch, out_ch, pad_mode=pad_mode),\n        )\n        self.pad_mode = pad_mode.lower()\n\n    def forward(self, inp: pt.Tensor) -&gt; pt.Tensor:\n        img_h, img_w = inp.shape[-2:]\n        pad_size = _get_alignment_padding((img_h, img_w))\n        pad_block = _get_padding_block(pad_size, self.pad_mode)\n        return super().forward(pad_block(inp))\n</code></pre>"},{"location":"reference/models/unet/#autoden.models.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>U-net model.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>class UNet(nn.Module):\n    \"\"\"U-net model.\"\"\"\n\n    def __init__(\n        self,\n        n_channels_in: int,\n        n_channels_out: int,\n        n_features: int = 32,\n        n_levels: int = 3,\n        n_channels_skip: Union[int, None] = None,\n        bilinear: bool = True,\n        pad_mode: str = \"replicate\",\n        device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n        verbose: bool = False,\n    ):\n        super().__init__()\n        self.n_ch_in = n_channels_in\n        self.n_ch_out = n_channels_out\n        self.device = device\n\n        if pad_mode.lower() not in PAD_MODES:\n            raise ValueError(f\"Padding mode {pad_mode} should be one of {PAD_MODES}\")\n        self.pad_mode = pad_mode.lower()\n\n        encoder, decoder = _compute_architecture(\n            n_levels=n_levels, n_features=n_features, n_skip=n_channels_skip, verbose=verbose\n        )\n\n        self.in_layer = DoubleConv(n_channels_in, n_features, pad_mode=pad_mode)\n        self.encoder_layers = nn.ModuleList([DownBlock(*lvl, bilinear=bilinear, pad_mode=pad_mode) for lvl in encoder])\n        self.decoder_layers = nn.ModuleList([UpBlock(*lvl, bilinear=bilinear, pad_mode=pad_mode) for lvl in decoder])\n        self.out_layer = ConvBlock(n_features, n_channels_out, kernel_size=1, last_block=True, pad_mode=pad_mode)\n\n        if verbose:\n            print(\n                f\"Model {self.__class__.__name__} - \"\n                f\"num. parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\"\n            )\n        self.to(self.device)\n\n    def forward(self, inp_x: pt.Tensor, return_latent: bool = False) -&gt; Union[pt.Tensor, tuple[pt.Tensor, pt.Tensor]]:\n        tmps: list[pt.Tensor] = [self.in_layer(inp_x)]\n        for d_l in self.encoder_layers:\n            tmps.append(d_l(tmps[-1]))\n\n        out_x = self.decoder_layers[0](tmps[-1], tmps[-2])\n        for ii_u, u_l in enumerate(self.decoder_layers[1:]):\n            out_x = u_l(out_x, tmps[-(ii_u + 3)])\n        out_x = self.out_layer(out_x)\n\n        if return_latent:\n            return out_x, pt.cat([tmp.flatten() for tmp in tmps])\n        else:\n            return out_x\n</code></pre>"},{"location":"reference/models/unet/#autoden.models.unet.UpBlock","title":"<code>UpBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Up-scaling block.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>class UpBlock(nn.Module):\n    \"\"\"Up-scaling block.\"\"\"\n\n    def __init__(\n        self, in_ch: int, skip_ch: Union[int, None], out_ch: int, bilinear: bool = False, pad_mode: str = \"replicate\"\n    ):\n        super().__init__()\n        self.skip_ch = skip_ch\n\n        # Bilinear up-sampling tends to give better results, and use fewer weights\n        if bilinear:\n            self.up_block = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        else:\n            self.up_block = nn.ConvTranspose2d(in_ch, in_ch, kernel_size=2, stride=2)\n\n        if skip_ch is not None:\n            n_skip = skip_ch\n            if skip_ch &gt; 0:\n                self.skip_block = ConvBlock(in_ch, skip_ch, kernel_size=1)\n        else:\n            n_skip = in_ch\n\n        self.conv_block = DoubleConv(in_ch + n_skip, out_ch, pad_mode=pad_mode)\n\n    def forward(self, x_lo_res: pt.Tensor, x_hi_res: pt.Tensor) -&gt; pt.Tensor:\n        x_lo2hi_res: pt.Tensor = self.up_block(x_lo_res)\n\n        if self.skip_ch is None:\n            x_comb = pt.cat([x_hi_res, x_lo2hi_res[..., : x_hi_res.shape[-2], : x_hi_res.shape[-1]]], dim=1)\n        elif self.skip_ch &gt; 0:\n            x_hi_res = self.skip_block(x_hi_res)\n\n            x_comb = pt.cat([x_hi_res, x_lo2hi_res[..., : x_hi_res.shape[-2], : x_hi_res.shape[-1]]], dim=1)\n        else:\n            x_comb = x_lo2hi_res\n\n        return self.conv_block(x_comb)\n</code></pre>"}]}