{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Auto-Denoise","text":"<p>Auto-denoise (autoden) provides implementations for a small selection of unsupervised and self-supervised CNN denoising methods. These methods currently include:</p> <ul> <li>Noise2Noise (N2N) - A self-supervised denoising method using pairs of images of the same object [1].</li> <li>Noise2Void (N2V) - A self-supervised denoising method capable of working with a single image [2]. We have also implemented a later development of the method that can work with structured noise [3].</li> <li>Deep Image Prior (DIP) - An unsupervised denoising/upsampling/deconvolution method that can also work with a single image [4].</li> </ul> <p>We also provide example implementations of supervised denoising methods, and the tomography specific Noise2Inverse (N2I) method [5].</p> <p>References:</p> <ul> <li>[1] J. Lehtinen et al., \u201cNoise2Noise: Learning Image Restoration without Clean Data,\u201d in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, Eds., in Proceedings of Machine Learning Research, vol. 80. PMLR, 2018, pp. 2965\u20132974. https://proceedings.mlr.press/v80/lehtinen18a.html</li> <li>[2] A. Krull, T.-O. Buchholz, and F. Jug, \u201cNoise2Void - Learning Denoising From Single Noisy Images,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Jun. 2019, pp. 2124\u20132132. doi: 10.1109/CVPR.2019.00223.</li> <li>[3] C. Broaddus, A. Krull, M. Weigert, U. Schmidt, and G. Myers, \u201cRemoving Structured Noise with Self-Supervised Blind-Spot Networks,\u201d in 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE, Apr. 2020, pp. 159\u2013163. doi: 10.1109/ISBI45749.2020.9098336.</li> <li>[4] V. Lempitsky, A. Vedaldi, and D. Ulyanov, \u201cDeep Image Prior,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, Jun. 2018, pp. 9446\u20139454. doi: 10.1109/CVPR.2018.00984.</li> <li>[5] A. A. Hendriksen, D. M. Pelt, and K. J. Batenburg, \"Noise2Inverse: Self-Supervised Deep Convolutional Denoising for Tomography,\" IEEE Transactions on Computational Imaging, vol. 6, pp. 1320\u20131335, 2020, doi: 10.1109/TCI.2020.3019647.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>It takes just a few steps to setup Auto-Denoise on your machine.</p>"},{"location":"#installing-with-conda","title":"Installing with conda","text":"<p>We recommend using Miniforge. Once installed <code>miniforge</code>, simply install <code>autoden</code> with: <pre><code>conda install auto-denoise -c n-vigano\n</code></pre></p>"},{"location":"#installing-from-pypi","title":"Installing from PyPI","text":"<p>Simply install with: <pre><code>python -m pip install auto-denoise\n</code></pre></p> <p>If you are on jupyter, and don't have the rights to install packages system-wide, then you can install with: <pre><code>! python -m pip install --user auto-denoise\n</code></pre></p>"},{"location":"#installing-from-source","title":"Installing from source","text":"<p>To install Auto-Denoise, simply clone this github.com project with either: <pre><code>git clone https://github.com/CEA-MetroCarac/auto-denoise.git auto-denoise\n</code></pre> or: <pre><code>git clone git@github.com:CEA-MetroCarac/auto-denoise.git auto-denoise\n</code></pre></p> <p>Then go to the cloned directory and run <code>pip</code> installer: <pre><code>cd auto-denoise\npip install -e .\n</code></pre></p>"},{"location":"#how-to-contribute","title":"How to contribute","text":"<p>Contributions are always welcome. Please submit pull requests against the <code>main</code> branch.</p> <p>If you have any issues, questions, or remarks, then please open an issue on github.com.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#v200-2025-09-15","title":"v2.0.0 - 2025-09-15","text":"<p>This is the second major release of Auto-denoise, providing:</p> <ul> <li>Initial support for 1D and 3D signals.</li> <li>Batching, data augmentation (flips), and learning rate scheduling for Supervised and Noise2Noise algorithms.</li> <li>Support for 2D models with 3D volumes in Noise2Noise.</li> </ul> <p>Compare with v1.0.0</p>"},{"location":"changelog/#v100-2025-05-06","title":"v1.0.0 - 2025-05-06","text":"<p>This is the first major release of Auto-denoise, providing initial implementations for a small selection of unsupervised and self-supervised CNN denoising methods. These methods currently include:</p> <ul> <li>Noise2Noise (N2N) - A self-supervised denoising method using pairs of images of the same object.</li> <li>Noise2Void (N2V) - A self-supervised denoising method capable of working with a single image. We have also implemented a later development of the method that can work with structured noise.</li> <li>Deep Image Prior (DIP) - An unsupervised denoising/upsampling/deconvolution method that can also work with a single image.</li> <li>Supervised denoising methods, and the tomography-specific Noise2Inverse (N2I) method.</li> </ul> <p>We also provide a small set of pre-configured models for these algorithms: U-net, MS-D net, DnCNN, and a custom ResNet implementation.</p> <p>Compare with first commit</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nicola.vigano@cea.fr. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier!</p> <p>Fork and clone the repository, then:</p> <pre><code>cd auto-denoise\nmake setup\n</code></pre>"},{"location":"contributing/#development","title":"Development","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git switch -c feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>Before committing:</p> <ol> <li>format the code with <code>black -l 127 &lt;your_file_path&gt;.py</code></li> <li>run the tests with <code>pytest tests/*</code> (fix any issue)</li> <li>if you updated the documentation or the project dependencies:<ol> <li>run <code>mkdocs serve</code></li> <li>go to http://localhost:8000 and check that everything looks good</li> </ol> </li> <li>follow our commit message convention</li> </ol> <p>If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review.</p> <p>Don't bother updating the changelog, we will take care of this.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit message convention","text":"<p>Commit messages must follow our convention based on the Angular style or the Karma convention:</p> <pre><code>&lt;type&gt;[(scope)]: Subject\n\n[Body]\n</code></pre> <p>Subject and body must be valid Markdown. Subject must have proper casing (uppercase for first letter if it makes sense), but no dot at the end, and no punctuation in general.</p> <p>Scope and body are optional. Type can be:</p> <ul> <li><code>build</code>: About packaging, building wheels, etc.</li> <li><code>chore</code>: About packaging or repo/files management.</li> <li><code>ci</code>: About Continuous Integration.</li> <li><code>deps</code>: Dependencies update.</li> <li><code>docs</code>: About documentation.</li> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>perf</code>: About performance.</li> <li><code>refactor</code>: Changes that are not features or bug fixes.</li> <li><code>style</code>: A change in code style/format.</li> <li><code>tests</code>: About tests.</li> </ul> <p>If you write a body, please add trailers at the end (for example issues and PR references, or co-authors), without relying on GitHub's flavored Markdown:</p> <pre><code>Body.\n\nIssue #10: https://github.com/namespace/project/issues/10\nRelated to PR namespace/other-project#15: https://github.com/namespace/other-project/pull/15\n</code></pre> <p>These \"trailers\" must appear at the end of the body, without any blank lines between them. The trailer title can contain any character except colons <code>:</code>. We expect a full URI for each trailer, not just GitHub autolinks (for example, full GitHub URLs for commits and issues, not the hash or the #issue-number).</p> <p>We do not enforce a line length on commit messages summary and body, but please avoid very long summaries, and very long lines in the body, unless they are part of code blocks that must not be wrapped.</p>"},{"location":"contributing/#pull-requests-guidelines","title":"Pull requests guidelines","text":"<p>Link to any related issue in the Pull Request message.</p> <p>During the review, we recommend using fixups:</p> <pre><code># SHA is the SHA of the commit you want to fix\ngit commit --fixup=SHA\n</code></pre> <p>Once all the changes are approved, you can squash your commits:</p> <pre><code>git rebase -i --autosquash main\n</code></pre> <p>And force-push:</p> <pre><code>git push -f\n</code></pre> <p>If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.</p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2023 Nicola VIGANO\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> autoden<ul> <li> algorithms<ul> <li> deep_image_prior</li> <li> denoiser</li> <li> noise2noise</li> <li> noise2void</li> <li> supervised</li> </ul> </li> <li> cli</li> <li> debug</li> <li> losses</li> <li> models<ul> <li> config</li> <li> dncnn</li> <li> io</li> <li> msd</li> <li> param_utils</li> <li> resnet</li> <li> unet</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/autoden/","title":"autoden","text":""},{"location":"reference/autoden/#autoden","title":"autoden","text":"<p>Auto-Denoise package.</p> <p>Unsupervised and self-supervised CNN denoising methods.</p> <p>Modules:</p> <ul> <li> <code>algorithms</code>           \u2013            <p>Algorithms sub-package.</p> </li> <li> <code>cli</code>           \u2013            <p>Module that contains the command line application.</p> </li> <li> <code>debug</code>           \u2013            <p>Debugging utilities.</p> </li> <li> <code>losses</code>           \u2013            <p>Data losses definitions.</p> </li> <li> <code>models</code>           \u2013            <p>Models sub-package.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>DIP</code>           \u2013            <p>Deep image prior.</p> </li> <li> <code>N2N</code>           \u2013            <p>Self-supervised denoising from pairs of images.</p> </li> <li> <code>N2V</code>           \u2013            <p>Self-supervised denoising from single images.</p> </li> <li> <code>Supervised</code>           \u2013            <p>Supervised denoising class.</p> </li> </ul>"},{"location":"reference/autoden/#autoden.DIP","title":"DIP","text":"<pre><code>DIP(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Deep image prior.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>prepare_data</code>             \u2013              <p>Prepare input data.</p> </li> <li> <code>train</code>             \u2013              <p>Train the model in an unsupervised manner.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.DIP.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/#autoden.DIP.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.DIP.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    tgt: NDArray,\n    num_tst_ratio: float = 0.2,\n    average_redundant: bool = False,\n) -&gt; tuple[NDArray, NDArray, NDArray]\n</code></pre> <p>Prepare input data.</p> <p>Parameters:</p> <ul> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target image array. The shape of the output noise array will match the spatial dimensions of this array.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the test set size to the total dataset size. Default is 0.2.</p> </li> <li> <code>average_redundant</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, average redundant realizations in the target array to match the expected number of dimensions. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, NDArray, NDArray]</code>           \u2013            <p>A tuple containing: - A random noise array with the same spatial dimensions as the target   image. - The target image array. - A mask array indicating the training pixels.</p> </li> </ul> Notes <p>This function generates a random noise array with the same spatial dimensions as the target image. The noise array is used as the initial input for the DIP algorithm. It also generates a mask array indicating the training pixels based on the provided ratio.</p> Source code in <code>src/autoden/algorithms/deep_image_prior.py</code> <pre><code>def prepare_data(\n    self, tgt: NDArray, num_tst_ratio: float = 0.2, average_redundant: bool = False\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"\n    Prepare input data.\n\n    Parameters\n    ----------\n    tgt : NDArray\n        The target image array. The shape of the output noise array will match\n        the spatial dimensions of this array.\n    num_tst_ratio : float, optional\n        The ratio of the test set size to the total dataset size.\n        Default is 0.2.\n    average_redundant : bool, optional\n        If True, average redundant realizations in the target array to match the\n        expected number of dimensions. Default is False.\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        A tuple containing:\n        - A random noise array with the same spatial dimensions as the target\n          image.\n        - The target image array.\n        - A mask array indicating the training pixels.\n\n    Notes\n    -----\n    This function generates a random noise array with the same spatial dimensions\n    as the target image. The noise array is used as the initial input for the DIP\n    algorithm. It also generates a mask array indicating the training pixels based\n    on the provided ratio.\n    \"\"\"\n    if tgt.ndim &lt; self.n_dims:\n        raise ValueError(f\"Target data should at least be of {self.n_dims} dimensions, but its shape is {tgt.shape}\")\n    if average_redundant and tgt.ndim &gt; self.n_dims:\n        tgt = tgt.mean(axis=tuple(np.arange(-tgt.ndim, -self.n_dims)))\n\n    inp = np.random.normal(size=tgt.shape[-self.n_dims :], scale=0.25).astype(tgt.dtype)\n    mask_trn = get_random_pixel_mask(tgt.shape, mask_pixel_ratio=num_tst_ratio)\n    return inp, tgt, mask_trn\n</code></pre>"},{"location":"reference/autoden/#autoden.DIP.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_trn: NDArray,\n    epochs: int,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Train the model in an unsupervised manner.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input image.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target image to be denoised.</p> </li> <li> <code>pixel_mask_trn</code>               (<code>NDArray</code>)           \u2013            <p>The mask array indicating the training pixels.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of training epochs.</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to use. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, NDArray]</code>           \u2013            <p>A dictionary containing the training losses.</p> </li> </ul> Notes <p>This method trains the model using the deep image prior approach in an unsupervised manner. It uses a random initialization for the input image if not provided and applies a scaling and bias transformation to the input and target images. It then trains the model using the specified optimization algorithm and the provided mask array indicating the training pixels.</p> Source code in <code>src/autoden/algorithms/deep_image_prior.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_trn: NDArray,\n    epochs: int,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Train the model in an unsupervised manner.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input image.\n    tgt : NDArray\n        The target image to be denoised.\n    pixel_mask_trn : NDArray\n        The mask array indicating the training pixels.\n    epochs : int\n        The number of training epochs.\n    optimizer : str, optional\n        The optimization algorithm to use. Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n\n    Returns\n    -------\n    dict[str, NDArray]\n        A dictionary containing the training losses.\n\n    Notes\n    -----\n    This method trains the model using the deep image prior approach in an unsupervised manner.\n    It uses a random initialization for the input image if not provided and applies a scaling and bias\n    transformation to the input and target images. It then trains the model using the specified optimization\n    algorithm and the provided mask array indicating the training pixels.\n    \"\"\"\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    tmp_inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tmp_tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_small(\n        tmp_inp, tmp_tgt, pixel_mask_trn, epochs=epochs, optimizer=optimizer, regularizer=reg, lower_limit=lower_limit\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Unsupervised {self.__class__.__name__} {optimizer.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N","title":"N2N","text":"<pre><code>N2N(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from pairs of images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Perform inference on the input data.</p> </li> <li> <code>prepare_data</code>             \u2013              <p>Prepare input data for training.</p> </li> <li> <code>train</code>             \u2013              <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/#autoden.N2N.infer","title":"infer","text":"<pre><code>infer(inp: NDArray, average_splits: bool = True) -&gt; NDArray\n</code></pre> <p>Perform inference on the input data.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to perform inference on. It is expected to have an extra dimension including the different splits.</p> </li> <li> <code>average_splits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the splits are averaged. Default is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The inferred output data. If <code>average_splits</code> is True, the splits are averaged.</p> </li> </ul> Notes <p>If <code>self.batch_size</code> is set, the input data is processed in batches to avoid memory issues.</p> Source code in <code>src/autoden/algorithms/noise2noise.py</code> <pre><code>def infer(self, inp: NDArray, average_splits: bool = True) -&gt; NDArray:\n    \"\"\"\n    Perform inference on the input data.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to perform inference on. It is expected to have an extra dimension including the different splits.\n    average_splits : bool, optional\n        If True, the splits are averaged. Default is True.\n\n    Returns\n    -------\n    NDArray\n        The inferred output data. If `average_splits` is True, the splits are averaged.\n\n    Notes\n    -----\n    If `self.batch_size` is set, the input data is processed in batches to avoid memory issues.\n    \"\"\"\n    inp_shape = inp.shape\n    inp = inp.reshape([inp_shape[0] * inp_shape[1], *inp_shape[2:]])\n    if self.batch_size is not None:\n        out = []\n        for b in tqdm(range(0, inp.shape[0], self.batch_size), desc=\"Inference batch\"):\n            out.append(super().infer(inp[b : b + self.batch_size]))\n        out = np.concatenate(out, axis=0)\n    else:\n        out = super().infer(inp)\n    out = out.reshape(inp_shape)\n    if average_splits:\n        realization_batch_axis = -self.n_dims - 1\n        out = out.mean(axis=realization_batch_axis)\n    return out\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    inp: NDArray,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"1:X\",\n) -&gt; tuple[NDArray, NDArray, NDArray]\n</code></pre> <p>Prepare input data for training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the input data to be used for testing. The remaining data will be used for training. Default is 0.2.</p> </li> <li> <code>strategy</code>               (<code>str</code>, default:                   <code>'1:X'</code> )           \u2013            <p>The strategy to be used for creating input-target pairs. The available strategies are: - \"1:X\": Use the mean of the remaining samples as the target for each sample. - \"X:1\": Use the mean of the remaining samples as the input for each sample. Default is \"1:X\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, NDArray, NDArray]</code>           \u2013            <p>A tuple containing: - The input data array. - The target data array. - The mask array indicating the training pixels.</p> </li> </ul> Notes <p>This function generates input-target pairs based on the specified strategy. It also generates a mask array indicating the training pixels based on the provided ratio.</p> Source code in <code>src/autoden/algorithms/noise2noise.py</code> <pre><code>def prepare_data(\n    self, inp: NDArray, num_tst_ratio: float = 0.2, strategy: str = \"1:X\"\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"\n    Prepare input data for training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    num_tst_ratio : float, optional\n        The ratio of the input data to be used for testing. The remaining data will be used for training.\n        Default is 0.2.\n    strategy : str, optional\n        The strategy to be used for creating input-target pairs. The available strategies are:\n        - \"1:X\": Use the mean of the remaining samples as the target for each sample.\n        - \"X:1\": Use the mean of the remaining samples as the input for each sample.\n        Default is \"1:X\".\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        A tuple containing:\n        - The input data array.\n        - The target data array.\n        - The mask array indicating the training pixels.\n\n    Notes\n    -----\n    This function generates input-target pairs based on the specified strategy. It also generates a mask array\n    indicating the training pixels based on the provided ratio.\n    \"\"\"\n    if inp.ndim &lt; self.n_dims + 1:\n        raise ValueError(f\"Target data should at least be of {self.n_dims + 1} dimensions, but its shape is {inp.shape}\")\n\n    realizations_batch_axis = inp.ndim - self.n_dims - 1\n\n    inp_x = np.stack([np.delete(inp, obj=ii, axis=0).mean(axis=0) for ii in range(len(inp))], axis=realizations_batch_axis)\n    inp = inp.swapaxes(0, realizations_batch_axis)\n\n    if strategy.upper() == \"1:X\":\n        tmp_inp = inp\n        tmp_tgt = inp_x\n    elif strategy.upper() == \"X:1\":\n        tmp_inp = inp_x\n        tmp_tgt = inp\n    else:\n        raise ValueError(f\"Strategy {strategy} not implemented. Please choose one of: ['1:X', 'X:1']\")\n\n    mask_trn = get_random_pixel_mask(inp.shape, mask_pixel_ratio=num_tst_ratio)\n\n    return tmp_inp, tmp_tgt, mask_trn\n</code></pre>"},{"location":"reference/autoden/#autoden.N2N.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_tst: NDArray,\n    epochs: int,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>pixel_mask_tst</code>               (<code>NDArray</code>)           \u2013            <p>The mask array indicating the test pixels.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of epochs to train the model.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The learning rate for the optimizer. Default is 1e-3.</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to be used for training. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> <li> <code>restarts</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing of the learning rate will be restarted the specified number of times. Default is None.</p> </li> <li> <code>accum_grads</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple batches before updating the model parameters. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, NDArray]</code>           \u2013            <p>A dictionary containing the training losses.</p> </li> </ul> Notes <p>This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to generate target data based on the specified strategy. The training process involves creating pairs of input and target data and then training the model to minimize the difference between the predicted and target data.</p> Source code in <code>src/autoden/algorithms/noise2noise.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_tst: NDArray,\n    epochs: int,\n    learning_rate: float = 1e-3,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Train the denoiser using the Noise2Noise self-supervised approach.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    tgt : NDArray\n        The target data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    pixel_mask_tst : NDArray\n        The mask array indicating the test pixels.\n    epochs : int\n        The number of epochs to train the model.\n    learning_rate : float, optional\n        The learning rate for the optimizer. Default is 1e-3.\n    optimizer : str, optional\n        The optimization algorithm to be used for training. Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n    restarts : int | None, optional\n        The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing\n        of the learning rate will be restarted the specified number of times. Default is None.\n    accum_grads : bool, optional\n        Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple\n        batches before updating the model parameters. Default is False.\n\n    Returns\n    -------\n    dict[str, NDArray]\n        A dictionary containing the training losses.\n\n    Notes\n    -----\n    This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to\n    generate target data based on the specified strategy. The training process involves creating pairs of input\n    and target data and then training the model to minimize the difference between the predicted and target data.\n    \"\"\"\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    tmp_inp = inp.astype(np.float32)\n    tmp_tgt = tgt.astype(np.float32)\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_batched(\n        tmp_inp,\n        tmp_tgt,\n        pixel_mask_tst,\n        epochs=epochs,\n        learning_rate=learning_rate,\n        optimizer=optimizer,\n        regularizer=reg,\n        lower_limit=lower_limit,\n        restarts=restarts,\n        accum_grads=accum_grads,\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {optimizer.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V","title":"N2V","text":"<pre><code>N2V(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from single images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train</code>             \u2013              <p>Self-supervised training.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/#autoden.N2V.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.N2V.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Self-supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images, which will also be targets</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>mask_shape</code>               (<code>int | Sequence[int] | NDArray</code>, default:                   <code>1</code> )           \u2013            <p>Shape of the blind spot mask, by default 1.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Optimizer algorithm to use, by default \"adam\"</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> </ul> Source code in <code>src/autoden/algorithms/noise2void.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]:\n    \"\"\"Self-supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images, which will also be targets\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    mask_shape : int | Sequence[int] | NDArray\n        Shape of the blind spot mask, by default 1.\n    algo : str, optional\n        Optimizer algorithm to use, by default \"adam\"\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_trn = inp[trn_inds]\n    inp_tst = inp[tst_inds]\n\n    reg = self._get_regularization()\n    losses = self._train_n2v_pixelmask_small(\n        inp_trn,\n        inp_tst,\n        epochs=epochs,\n        mask_shape=mask_shape,\n        ratio_blind_spot=ratio_blind_spot,\n        algo=algo,\n        regularizer=reg,\n        lower_limit=lower_limit,\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/#autoden.Supervised","title":"Supervised","text":"<pre><code>Supervised(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Supervised denoising class.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>prepare_data</code>             \u2013              <p>Prepare input data for training.</p> </li> <li> <code>train</code>             \u2013              <p>Supervised training.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/#autoden.Supervised.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/#autoden.Supervised.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/#autoden.Supervised.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    inp: NDArray,\n    tgt: NDArray,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"pixel-mask\",\n) -&gt; tuple[NDArray, NDArray, NDArray | list[int]]\n</code></pre> <p>Prepare input data for training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the number of samples, and D, H and W are the depth, height and width of each sample, respectively.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the number of samples, and D, H and W are the depth, height and width of each sample, respectively.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the input data to be used for testing. The remaining data will be used for training. Default is 0.2.</p> </li> <li> <code>strategy</code>               (<code>str</code>, default:                   <code>'pixel-mask'</code> )           \u2013            <p>The strategy to be used for creating training and testing sets. The available strategies are: - \"pixel-mask\": Use randomly chosen pixels in the images as test set. - \"self-similar\": Use entire randomly chosen images as test set. Default is \"pixel-mask\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, NDArray, NDArray]</code>           \u2013            <p>A tuple containing: - The input data array. - The target data array. - Either the mask array indicating the testing pixels or the list of test indices.</p> </li> </ul> Source code in <code>src/autoden/algorithms/supervised.py</code> <pre><code>def prepare_data(\n    self, inp: NDArray, tgt: NDArray, num_tst_ratio: float = 0.2, strategy: str = \"pixel-mask\"\n) -&gt; tuple[NDArray, NDArray, NDArray | list[int]]:\n    \"\"\"\n    Prepare input data for training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the\n        number of samples, and D, H and W are the depth, height and width of each sample, respectively.\n    tgt : NDArray\n        The target data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the\n        number of samples, and D, H and W are the depth, height and width of each sample, respectively.\n    num_tst_ratio : float, optional\n        The ratio of the input data to be used for testing. The remaining data will be used for training.\n        Default is 0.2.\n    strategy : str, optional\n        The strategy to be used for creating training and testing sets. The available strategies are:\n        - \"pixel-mask\": Use randomly chosen pixels in the images as test set.\n        - \"self-similar\": Use entire randomly chosen images as test set.\n        Default is \"pixel-mask\".\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        A tuple containing:\n        - The input data array.\n        - The target data array.\n        - Either the mask array indicating the testing pixels or the list of test indices.\n    \"\"\"\n    if inp.ndim &lt; self.n_dims:\n        raise ValueError(f\"Target data should at least be of {self.n_dims} dimensions, but its shape is {inp.shape}\")\n\n    num_imgs = inp.shape[0]\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if inp.shape != tgt.shape:\n        raise ValueError(\n            f\"Input and target data must have the same shape. Input shape: {inp.shape}, Target shape: {tgt.shape}\"\n        )\n\n    if strategy.lower() == \"pixel-mask\":\n        mask_tst = get_random_pixel_mask(inp.shape, mask_pixel_ratio=num_tst_ratio)\n    elif strategy.lower() == \"self-similar\":\n        mask_tst = get_random_image_indices(num_imgs, num_tst_ratio=num_tst_ratio)\n    else:\n        raise ValueError(f\"Strategy {strategy} not implemented. Please choose one of: ['pixel-mask', 'self-similar']\")\n\n    return inp, tgt, mask_tst\n</code></pre>"},{"location":"reference/autoden/#autoden.Supervised.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    tgt: NDArray,\n    tst_inds: Sequence[int] | NDArray,\n    epochs: int,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices (either image indices if Sequence[int] or pixel indices if NDArray)</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The learning rate for the optimizer. Default is 1e-3.</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to be used for training. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> <li> <code>restarts</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing of the learning rate will be restarted the specified number of times. Default is None.</p> </li> <li> <code>accum_grads</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple batches before updating the model parameters. Default is False.</p> </li> </ul> Source code in <code>src/autoden/algorithms/supervised.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    tst_inds: Sequence[int] | NDArray,\n    epochs: int,\n    learning_rate: float = 1e-3,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]:\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices (either image indices if Sequence[int] or pixel indices if NDArray)\n    epochs : int\n        Number of training epochs\n    learning_rate : float, optional\n        The learning rate for the optimizer. Default is 1e-3.\n    optimizer : str, optional\n        The optimization algorithm to be used for training. Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n    restarts : int | None, optional\n        The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing\n        of the learning rate will be restarted the specified number of times. Default is None.\n    accum_grads : bool, optional\n        Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple\n        batches before updating the model parameters. Default is False.\n    \"\"\"\n    num_imgs = inp.shape[0]\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    inp = inp.astype(np.float32)\n    tgt = tgt.astype(np.float32)\n\n    reg = self._get_regularization()\n\n    if isinstance(tst_inds, Sequence):\n        tst_inds = np.array(tst_inds, dtype=int)\n        if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n            raise ValueError(\n                \"Each cross-validation index should be greater or equal than 0,\"\n                f\" and less than the number of images {num_imgs}\"\n            )\n        trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n        # Create datasets\n        dset_trn = (inp[trn_inds], tgt[trn_inds])\n        dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n        losses = self._train_selfsimilar_batched(\n            dset_trn,\n            dset_tst,\n            epochs=epochs,\n            learning_rate=learning_rate,\n            optimizer=optimizer,\n            regularizer=reg,\n            lower_limit=lower_limit,\n            restarts=restarts,\n            accum_grads=accum_grads,\n        )\n    elif isinstance(tst_inds, np.ndarray):\n        losses = self._train_pixelmask_batched(\n            inp,\n            tgt,\n            tst_inds,\n            epochs=epochs,\n            learning_rate=learning_rate,\n            optimizer=optimizer,\n            regularizer=reg,\n            lower_limit=lower_limit,\n            restarts=restarts,\n            accum_grads=accum_grads,\n        )\n    else:\n        raise ValueError(\n            \"`tst_inds` should either be a Sequence[int] or NDArray. Please use the the `prepare_data` function if unsure.\"\n        )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {optimizer.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/cli/","title":"autoden.cli","text":""},{"location":"reference/autoden/cli/#autoden.cli","title":"cli","text":"<p>Module that contains the command line application.</p> <p>Functions:</p> <ul> <li> <code>get_parser</code>             \u2013              <p>Return the CLI argument parser.</p> </li> <li> <code>main</code>             \u2013              <p>Run the main program.</p> </li> </ul>"},{"location":"reference/autoden/cli/#autoden.cli.get_parser","title":"get_parser","text":"<pre><code>get_parser() -&gt; ArgumentParser\n</code></pre> <p>Return the CLI argument parser.</p> <p>Returns:</p> <ul> <li> <code>ArgumentParser</code>           \u2013            <p>An argparse parser.</p> </li> </ul> Source code in <code>src/autoden/cli.py</code> <pre><code>def get_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"\n    Return the CLI argument parser.\n\n    Returns\n    -------\n    argparse.ArgumentParser\n        An argparse parser.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"autoden\",\n        description=\"Denoise the given images, using deep-learning-based unsupervised or self-supervised algorithms.\",\n    )\n    parser.add_argument(\"algorithm\", choices=[\"N2N\", \"N2V\", \"DIP\"], help=\"Denoising algorithm to use\")\n    parser.add_argument(\n        \"--epochs\",\n        \"-e\",\n        type=int,\n        help=f\"Number of epochs to use, by default {DEFAULT_EPOCHS}.\",\n        metavar=\"E\",\n        default=DEFAULT_EPOCHS,\n    )\n    parser.add_argument(\n        \"--unet-levels\",\n        \"-l\",\n        type=int,\n        help=f\"Number of UNet levels to use, by default: {NetworkParamsUNet.DEFAULT_LEVELS}.\",\n        default=NetworkParamsUNet.DEFAULT_LEVELS,\n        metavar=\"L\",\n    )\n    parser.add_argument(\n        \"--unet-features\",\n        \"-f\",\n        type=int,\n        help=f\"Number of UNet features to use, by default: {NetworkParamsUNet.DEFAULT_FEATURES}.\",\n        default=NetworkParamsUNet.DEFAULT_FEATURES,\n        metavar=\"F\",\n    )\n    parser.add_argument(\n        \"--regularization\",\n        \"-r\",\n        type=float,\n        help=f\"Total Variation regularization value, by default: {DEFAULT_TV_VAL}.\",\n        default=DEFAULT_TV_VAL,\n        metavar=\"R\",\n    )\n    parser.add_argument(\"src_file\", nargs=\"+\", help=\"Path of each input image.\", type=argparse.FileType(\"rb\"))\n    parser.add_argument(\"dst_file\", help=\"Path of the output image.\", type=argparse.FileType(\"wb\"))\n    parser.add_argument(\"-V\", \"--version\", action=\"version\", version=f\"%(prog)s {debug.get_version()}\")\n    parser.add_argument(\"--debug-info\", action=_DebugInfo, help=\"Print debug information.\")\n    return parser\n</code></pre>"},{"location":"reference/autoden/cli/#autoden.cli.main","title":"main","text":"<pre><code>main(args: list[str] | None = None) -&gt; int\n</code></pre> <p>Run the main program.</p> <p>This function is executed when you type <code>autoden</code> or <code>python -m autoden</code>.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Arguments passed from the command line, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>An exit code.</p> </li> </ul> Source code in <code>src/autoden/cli.py</code> <pre><code>def main(args: list[str] | None = None) -&gt; int:\n    \"\"\"\n    Run the main program.\n\n    This function is executed when you type `autoden` or `python -m autoden`.\n\n    Parameters\n    ----------\n    args : list[str] | None\n        Arguments passed from the command line, by default None.\n\n    Returns\n    -------\n    int\n        An exit code.\n    \"\"\"\n    parser = get_parser()\n    opts = parser.parse_args(args=args)\n    # print(opts)  # noqa: WPS421 (side-effect in main is fine)\n\n    inp_imgs = [iio.imread(f) for f in opts.src_file]\n    if any(x.ndim &gt; 2 for x in inp_imgs):\n        print(\"Color images not supported, yet.\")\n        return 1\n    inp_imgs_stack = np.stack(inp_imgs, axis=0)\n\n    net_pars = NetworkParamsUNet(n_levels=opts.unet_levels, n_features=opts.unet_features)\n\n    if opts.algorithm.upper() == \"DIP\":\n        algo = DIP(model=net_pars, reg_val=opts.regularization)\n        inp_img = algo.train_unsupervised(inp_imgs_stack, epochs=opts.epochs)\n        out_img = algo.infer(inp_img)\n    elif opts.algorithm.upper() == \"N2N\":\n        if len(inp_imgs) &lt; 2:\n            print(f\"Not enough input images, only {len(inp_imgs)} were passed.\")\n            return 1\n\n        algo = N2N(model=net_pars, reg_val=opts.regularization)\n        algo.train_selfsupervised(inp_imgs_stack, epochs=opts.epochs)\n        out_img = algo.infer(inp_imgs_stack)\n    else:\n        print(f\"Not implemented support for algorithm {opts.algorithm} in command-line, yet.\")\n        return 1\n\n    iio.imwrite(opts.dst_file, out_img)\n    return 0\n</code></pre>"},{"location":"reference/autoden/debug/","title":"autoden.debug","text":""},{"location":"reference/autoden/debug/#autoden.debug","title":"debug","text":"<p>Debugging utilities.</p> <p>Classes:</p> <ul> <li> <code>Environment</code>           \u2013            <p>Dataclass to store environment information.</p> </li> <li> <code>Package</code>           \u2013            <p>Dataclass describing a Python package.</p> </li> <li> <code>Variable</code>           \u2013            <p>Dataclass describing an environment variable.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_debug_info</code>             \u2013              <p>Get debug/environment information.</p> </li> <li> <code>get_version</code>             \u2013              <p>Get version of the given distribution.</p> </li> <li> <code>print_debug_info</code>             \u2013              <p>Print debug/environment information.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Environment","title":"Environment  <code>dataclass</code>","text":"<pre><code>Environment(\n    interpreter_name: str,\n    interpreter_version: str,\n    interpreter_path: str,\n    platform: str,\n    packages: list[Package],\n    variables: list[Variable],\n)\n</code></pre> <p>Dataclass to store environment information.</p> <p>Attributes:</p> <ul> <li> <code>interpreter_name</code>               (<code>str</code>)           \u2013            <p>Python interpreter name.</p> </li> <li> <code>interpreter_path</code>               (<code>str</code>)           \u2013            <p>Path to Python executable.</p> </li> <li> <code>interpreter_version</code>               (<code>str</code>)           \u2013            <p>Python interpreter version.</p> </li> <li> <code>packages</code>               (<code>list[Package]</code>)           \u2013            <p>Installed packages.</p> </li> <li> <code>platform</code>               (<code>str</code>)           \u2013            <p>Operating System.</p> </li> <li> <code>variables</code>               (<code>list[Variable]</code>)           \u2013            <p>Environment variables.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.interpreter_name","title":"interpreter_name  <code>instance-attribute</code>","text":"<pre><code>interpreter_name: str\n</code></pre> <p>Python interpreter name.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.interpreter_path","title":"interpreter_path  <code>instance-attribute</code>","text":"<pre><code>interpreter_path: str\n</code></pre> <p>Path to Python executable.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.interpreter_version","title":"interpreter_version  <code>instance-attribute</code>","text":"<pre><code>interpreter_version: str\n</code></pre> <p>Python interpreter version.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages: list[Package]\n</code></pre> <p>Installed packages.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.platform","title":"platform  <code>instance-attribute</code>","text":"<pre><code>platform: str\n</code></pre> <p>Operating System.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Environment.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: list[Variable]\n</code></pre> <p>Environment variables.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Package","title":"Package  <code>dataclass</code>","text":"<pre><code>Package(name: str, version: str)\n</code></pre> <p>Dataclass describing a Python package.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Package name.</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Package version.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Package.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Package name.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Package.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str\n</code></pre> <p>Package version.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Variable","title":"Variable  <code>dataclass</code>","text":"<pre><code>Variable(name: str, value: str)\n</code></pre> <p>Dataclass describing an environment variable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Variable name.</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>Variable value.</p> </li> </ul>"},{"location":"reference/autoden/debug/#autoden.debug.Variable.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Variable name.</p>"},{"location":"reference/autoden/debug/#autoden.debug.Variable.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: str\n</code></pre> <p>Variable value.</p>"},{"location":"reference/autoden/debug/#autoden.debug.get_debug_info","title":"get_debug_info","text":"<pre><code>get_debug_info() -&gt; Environment\n</code></pre> <p>Get debug/environment information.</p> <p>Returns:     Environment information.</p> Source code in <code>src/autoden/debug.py</code> <pre><code>def get_debug_info() -&gt; Environment:\n    \"\"\"Get debug/environment information.\n\n    Returns:\n        Environment information.\n    \"\"\"\n    py_name, py_version = _interpreter_name_version()\n    packages = [\"auto-denoise\"]\n    variables = [\"PYTHONPATH\", *[var for var in os.environ if var.startswith(\"AUTO_DENOISE\")]]\n    return Environment(\n        interpreter_name=py_name,\n        interpreter_version=py_version,\n        interpreter_path=sys.executable,\n        platform=platform.platform(),\n        variables=[Variable(var, val) for var in variables if (val := os.getenv(var))],\n        packages=[Package(pkg, get_version(pkg)) for pkg in packages],\n    )\n</code></pre>"},{"location":"reference/autoden/debug/#autoden.debug.get_version","title":"get_version","text":"<pre><code>get_version(dist: str = 'auto-denoise') -&gt; str\n</code></pre> <p>Get version of the given distribution.</p> <p>Parameters:     dist: A distribution name.</p> <p>Returns:     A version number.</p> Source code in <code>src/autoden/debug.py</code> <pre><code>def get_version(dist: str = \"auto-denoise\") -&gt; str:\n    \"\"\"Get version of the given distribution.\n\n    Parameters:\n        dist: A distribution name.\n\n    Returns:\n        A version number.\n    \"\"\"\n    try:\n        return metadata.version(dist)\n    except metadata.PackageNotFoundError:\n        return \"0.0.0\"\n</code></pre>"},{"location":"reference/autoden/debug/#autoden.debug.print_debug_info","title":"print_debug_info","text":"<pre><code>print_debug_info() -&gt; None\n</code></pre> <p>Print debug/environment information.</p> Source code in <code>src/autoden/debug.py</code> <pre><code>def print_debug_info() -&gt; None:\n    \"\"\"Print debug/environment information.\"\"\"\n    info = get_debug_info()\n    print(f\"- __System__: {info.platform}\")\n    print(f\"- __Python__: {info.interpreter_name} {info.interpreter_version} ({info.interpreter_path})\")\n    print(\"- __Environment variables__:\")\n    for var in info.variables:\n        print(f\"  - `{var.name}`: `{var.value}`\")\n    print(\"- __Installed packages__:\")\n    for pkg in info.packages:\n        print(f\"  - `{pkg.name}` v{pkg.version}\")\n</code></pre>"},{"location":"reference/autoden/losses/","title":"autoden.losses","text":""},{"location":"reference/autoden/losses/#autoden.losses","title":"losses","text":"<p>Data losses definitions.</p> <p>Classes:</p> <ul> <li> <code>LossRegularizer</code>           \u2013            <p>Base class for the regularizer losses.</p> </li> <li> <code>LossSWTN</code>           \u2013            <p>Multi-level n-dimensional stationary wavelet transform loss function.</p> </li> <li> <code>LossTGV</code>           \u2013            <p>Total Generalized Variation loss function.</p> </li> <li> <code>LossTV</code>           \u2013            <p>Total Variation loss function.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_nd_wl_filters</code>             \u2013              <p>Generate all possible N-D separable wavelet filters.</p> </li> <li> <code>swt_nd</code>             \u2013              <p>Perform N-dimensional Stationary Wavelet Transform (SWT).</p> </li> </ul>"},{"location":"reference/autoden/losses/#autoden.losses.LossRegularizer","title":"LossRegularizer","text":"<p>               Bases: <code>MSELoss</code></p> <p>Base class for the regularizer losses.</p>"},{"location":"reference/autoden/losses/#autoden.losses.LossSWTN","title":"LossSWTN","text":"<pre><code>LossSWTN(\n    wl_dec_lo: Tensor,\n    wl_dec_hi: Tensor,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    levels: int = 2,\n    n_dims: int = 2,\n    min_approx: bool = False,\n)\n</code></pre> <p>               Bases: <code>LossRegularizer</code></p> <p>Multi-level n-dimensional stationary wavelet transform loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute wavelet decomposition on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    wl_dec_lo: pt.Tensor,\n    wl_dec_hi: pt.Tensor,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    levels: int = 2,\n    n_dims: int = 2,\n    min_approx: bool = False,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.wl_dec_lo = wl_dec_lo\n    self.wl_dec_hi = wl_dec_hi\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.levels = levels\n    self.n_dims = n_dims\n    self.min_approx = min_approx\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossSWTN.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute wavelet decomposition on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute wavelet decomposition on current batch.\"\"\"\n    _check_input_tensor(img, self.n_dims)\n    axes = list(range(-(self.n_dims + 1), 0))\n\n    coeffs = swt_nd(img, wl_dec_lo=self.wl_dec_lo, wl_dec_hi=self.wl_dec_hi, level=self.levels, normalize=\"scale\")\n\n    wl_val = []\n    first_ind = int(not self.min_approx)\n    for lvl_c in coeffs[first_ind:]:\n        coeff = pt.stack(lvl_c, dim=0)\n\n        if self.isotropic:\n            wl_val.append(pt.sqrt(pt.pow(coeff, 2).sum(dim=0)).sum(axes))\n        else:\n            wl_val.append(coeff.abs().sum(dim=0).sum(axes))\n\n    return self.lambda_val * pt.stack(wl_val, dim=0).sum(dim=0).mean() / ((self.levels + self.min_approx) ** 0.5)\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTGV","title":"LossTGV","text":"<pre><code>LossTGV(\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>LossTV</code></p> <p>Total Generalized Variation loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute total variation statistics on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    n_dims: int = 2,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.n_dims = n_dims\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTGV.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    _check_input_tensor(img, self.n_dims)\n    axes = list(range(-(self.n_dims + 1), 0))\n\n    diffs = [_differentiate(img, dim=dim, position=\"post\") for dim in range(-self.n_dims, 0)]\n    diffdiffs = [_differentiate(d, dim=dim, position=\"pre\") for dim in range(-self.n_dims, 0) for d in diffs]\n\n    if self.isotropic:\n        tv_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffs], dim=0).sum(dim=0))\n        jac_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffdiffs], dim=0).sum(dim=0))\n    else:\n        tv_val = pt.stack([d.abs() for d in diffs], dim=0).sum(dim=0)\n        jac_val = pt.stack([d.abs() for d in diffdiffs], dim=0).sum(dim=0)\n\n    return self.lambda_val * (tv_val.sum(axes).mean() + jac_val.sum(axes).mean() / 4)\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTV","title":"LossTV","text":"<pre><code>LossTV(\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>LossRegularizer</code></p> <p>Total Variation loss function.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Compute total variation statistics on current batch.</p> </li> </ul> Source code in <code>src/autoden/losses.py</code> <pre><code>def __init__(\n    self,\n    lambda_val: float,\n    size_average=None,\n    reduce=None,\n    reduction: str = \"mean\",\n    isotropic: bool = True,\n    n_dims: int = 2,\n) -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n    self.lambda_val = lambda_val\n    self.isotropic = isotropic\n    self.n_dims = n_dims\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.LossTV.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; Tensor\n</code></pre> <p>Compute total variation statistics on current batch.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def forward(self, img: pt.Tensor) -&gt; pt.Tensor:\n    \"\"\"Compute total variation statistics on current batch.\"\"\"\n    _check_input_tensor(img, self.n_dims)\n    axes = list(range(-(self.n_dims + 1), 0))\n\n    diffs = [_differentiate(img, dim=dim, position=\"post\") for dim in range(-self.n_dims, 0)]\n    diffs = pt.stack(diffs, dim=0)\n\n    if self.isotropic:\n        # tv_val = pt.sqrt(pt.stack([pt.pow(d, 2) for d in diffs], dim=0).sum(dim=0))\n        tv_val = pt.sqrt(pt.pow(diffs, 2).sum(dim=0))\n    else:\n        # tv_val = pt.stack([d.abs() for d in diffs], dim=0).sum(dim=0)\n        tv_val = diffs.abs().sum(dim=0)\n\n    return self.lambda_val * tv_val.sum(axes).mean()\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.get_nd_wl_filters","title":"get_nd_wl_filters","text":"<pre><code>get_nd_wl_filters(\n    wl_lo: Tensor, wl_hi: Tensor, ndim: int\n) -&gt; list[Tensor]\n</code></pre> <p>Generate all possible N-D separable wavelet filters.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def get_nd_wl_filters(wl_lo: pt.Tensor, wl_hi: pt.Tensor, ndim: int) -&gt; list[pt.Tensor]:\n    \"\"\"\n    Generate all possible N-D separable wavelet filters.\n    \"\"\"\n    filters: list[pt.Tensor] = [wl_lo] + [wl_hi] * ndim\n    for _ in range(ndim - 1):\n        filters[0] = pt.outer(filters[0], wl_lo)\n    for ii in range(ndim):\n        new_shape = [1] * ndim\n        new_shape[ii] = -1\n        filters[ii + 1] = filters[ii + 1].reshape(new_shape)\n    return filters\n</code></pre>"},{"location":"reference/autoden/losses/#autoden.losses.swt_nd","title":"swt_nd","text":"<pre><code>swt_nd(\n    x: Tensor,\n    wl_dec_lo: Tensor,\n    wl_dec_hi: Tensor,\n    level: int = 1,\n    normalize: str | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Perform N-dimensional Stationary Wavelet Transform (SWT).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape (B, 1, *dims) where dims can be 1D, 2D, or 3D.</p> </li> <li> <code>wl_dec_lo</code>               (<code>Tensor</code>)           \u2013            <p>Low-pass wavelet decomposition filter.</p> </li> <li> <code>wl_dec_hi</code>               (<code>Tensor</code>)           \u2013            <p>High-pass wavelet decomposition filter.</p> </li> <li> <code>level</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of decomposition levels (default is 1).</p> </li> <li> <code>normalize</code>               (<code>str or None</code>, default:                   <code>None</code> )           \u2013            <p>Normalization method ('none', 'energy', or 'scale'). If None, no normalization is applied (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list of list of pt.Tensor</code>           \u2013            <p>List like [[approx], [detail_vols], ..., [detail_vols]].</p> </li> </ul> Notes <p>The function performs the SWT on the input tensor <code>x</code> using the specified wavelet filters and decomposition level. The output is a list of lists, where each inner list contains the decomposition volumes. The first inner list contains the approximation coefficients, and the subsequent inner lists contain the detail coefficients for each level.</p> Source code in <code>src/autoden/losses.py</code> <pre><code>def swt_nd(\n    x: pt.Tensor, wl_dec_lo: pt.Tensor, wl_dec_hi: pt.Tensor, level: int = 1, normalize: str | None = None\n) -&gt; list[list[pt.Tensor]]:\n    \"\"\"\n    Perform N-dimensional Stationary Wavelet Transform (SWT).\n\n    Parameters\n    ----------\n    x : pt.Tensor\n        Input tensor of shape (B, 1, *dims) where dims can be 1D, 2D, or 3D.\n    wl_dec_lo : pt.Tensor\n        Low-pass wavelet decomposition filter.\n    wl_dec_hi : pt.Tensor\n        High-pass wavelet decomposition filter.\n    level : int, optional\n        Number of decomposition levels (default is 1).\n    normalize : str or None, optional\n        Normalization method ('none', 'energy', or 'scale'). If None, no normalization is applied (default is None).\n\n    Returns\n    -------\n    list of list of pt.Tensor\n        List like [[approx], [detail_vols], ..., [detail_vols]].\n\n    Notes\n    -----\n    The function performs the SWT on the input tensor `x` using the specified wavelet filters and decomposition level.\n    The output is a list of lists, where each inner list contains the decomposition volumes. The first inner list contains\n    the approximation coefficients, and the subsequent inner lists contain the detail coefficients for each level.\n    \"\"\"\n    dims = x.shape[2:]\n    ndim = len(dims)\n    output = []\n    current = x\n\n    base_filters = get_nd_wl_filters(\n        wl_dec_lo.to(dtype=pt.float32, device=x.device), wl_dec_hi.to(dtype=pt.float32, device=x.device), ndim\n    )\n    for l in range(1, level + 1):\n        dilation = 2 ** (l - 1)\n\n        res_l = []\n        for filt in base_filters:\n            filt = _normalize_wl_filter(filt, l, normalize)\n            filt = filt.unsqueeze(0).unsqueeze(0)  # shape (1, 1, ...)\n\n            # Calculate padding for each dimension\n            filt_span_shape = (pt.tensor(filt.shape[2:]).flip(dims=[0]) - 1) * dilation\n            pad = [pt.tensor([k // 2, k - k // 2]) for k in filt_span_shape]\n            pad = pt.concatenate(pad)\n            padded = F.pad(current, pad.tolist(), mode='replicate')\n\n            if ndim == 1:\n                out = F.conv1d(padded, filt, dilation=dilation)\n            elif ndim == 2:\n                out = F.conv2d(padded, filt, dilation=dilation)\n            elif ndim == 3:\n                out = F.conv3d(padded, filt, dilation=dilation)\n            else:\n                raise ValueError(\"Only 1D, 2D, 3D supported\")\n\n            res_l.append(out)\n\n        # Split into approximation and details\n        current = res_l[0]  # recurse on approximation\n        output.append(res_l[1:])\n\n    output.append([current])\n\n    return list(reversed(output))\n</code></pre>"},{"location":"reference/autoden/algorithms/","title":"autoden.algorithms","text":""},{"location":"reference/autoden/algorithms/#autoden.algorithms","title":"algorithms","text":"<p>Algorithms sub-package.</p> <p>Implementation of denoising algorithms like Noise2Noise, Noise2Void, and Deep Image Prior.</p> <p>Modules:</p> <ul> <li> <code>deep_image_prior</code>           \u2013            <p>Unsupervised denoiser implementation, based on the Deep Image Prior.</p> </li> <li> <code>denoiser</code>           \u2013            <p>Base class and functions for all denoising algorithms.</p> </li> <li> <code>noise2noise</code>           \u2013            <p>Self-supervised denoiser implementation, based on Noise2Noise.</p> </li> <li> <code>noise2void</code>           \u2013            <p>Self-supervised denoiser implementation, based on Noise2Void.</p> </li> <li> <code>supervised</code>           \u2013            <p>Supervised denoiser implementation.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/deep_image_prior/","title":"autoden.algorithms.deep_image_prior","text":""},{"location":"reference/autoden/algorithms/deep_image_prior/#autoden.algorithms.deep_image_prior","title":"deep_image_prior","text":"<p>Unsupervised denoiser implementation, based on the Deep Image Prior.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>DIP</code>           \u2013            <p>Deep image prior.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/deep_image_prior/#autoden.algorithms.deep_image_prior.DIP","title":"DIP","text":"<pre><code>DIP(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Deep image prior.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>prepare_data</code>             \u2013              <p>Prepare input data.</p> </li> <li> <code>train</code>             \u2013              <p>Train the model in an unsupervised manner.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/deep_image_prior/#autoden.algorithms.deep_image_prior.DIP.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/deep_image_prior/#autoden.algorithms.deep_image_prior.DIP.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/deep_image_prior/#autoden.algorithms.deep_image_prior.DIP.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    tgt: NDArray,\n    num_tst_ratio: float = 0.2,\n    average_redundant: bool = False,\n) -&gt; tuple[NDArray, NDArray, NDArray]\n</code></pre> <p>Prepare input data.</p> <p>Parameters:</p> <ul> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target image array. The shape of the output noise array will match the spatial dimensions of this array.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the test set size to the total dataset size. Default is 0.2.</p> </li> <li> <code>average_redundant</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, average redundant realizations in the target array to match the expected number of dimensions. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, NDArray, NDArray]</code>           \u2013            <p>A tuple containing: - A random noise array with the same spatial dimensions as the target   image. - The target image array. - A mask array indicating the training pixels.</p> </li> </ul> Notes <p>This function generates a random noise array with the same spatial dimensions as the target image. The noise array is used as the initial input for the DIP algorithm. It also generates a mask array indicating the training pixels based on the provided ratio.</p> Source code in <code>src/autoden/algorithms/deep_image_prior.py</code> <pre><code>def prepare_data(\n    self, tgt: NDArray, num_tst_ratio: float = 0.2, average_redundant: bool = False\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"\n    Prepare input data.\n\n    Parameters\n    ----------\n    tgt : NDArray\n        The target image array. The shape of the output noise array will match\n        the spatial dimensions of this array.\n    num_tst_ratio : float, optional\n        The ratio of the test set size to the total dataset size.\n        Default is 0.2.\n    average_redundant : bool, optional\n        If True, average redundant realizations in the target array to match the\n        expected number of dimensions. Default is False.\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        A tuple containing:\n        - A random noise array with the same spatial dimensions as the target\n          image.\n        - The target image array.\n        - A mask array indicating the training pixels.\n\n    Notes\n    -----\n    This function generates a random noise array with the same spatial dimensions\n    as the target image. The noise array is used as the initial input for the DIP\n    algorithm. It also generates a mask array indicating the training pixels based\n    on the provided ratio.\n    \"\"\"\n    if tgt.ndim &lt; self.n_dims:\n        raise ValueError(f\"Target data should at least be of {self.n_dims} dimensions, but its shape is {tgt.shape}\")\n    if average_redundant and tgt.ndim &gt; self.n_dims:\n        tgt = tgt.mean(axis=tuple(np.arange(-tgt.ndim, -self.n_dims)))\n\n    inp = np.random.normal(size=tgt.shape[-self.n_dims :], scale=0.25).astype(tgt.dtype)\n    mask_trn = get_random_pixel_mask(tgt.shape, mask_pixel_ratio=num_tst_ratio)\n    return inp, tgt, mask_trn\n</code></pre>"},{"location":"reference/autoden/algorithms/deep_image_prior/#autoden.algorithms.deep_image_prior.DIP.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_trn: NDArray,\n    epochs: int,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Train the model in an unsupervised manner.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input image.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target image to be denoised.</p> </li> <li> <code>pixel_mask_trn</code>               (<code>NDArray</code>)           \u2013            <p>The mask array indicating the training pixels.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of training epochs.</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to use. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, NDArray]</code>           \u2013            <p>A dictionary containing the training losses.</p> </li> </ul> Notes <p>This method trains the model using the deep image prior approach in an unsupervised manner. It uses a random initialization for the input image if not provided and applies a scaling and bias transformation to the input and target images. It then trains the model using the specified optimization algorithm and the provided mask array indicating the training pixels.</p> Source code in <code>src/autoden/algorithms/deep_image_prior.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_trn: NDArray,\n    epochs: int,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Train the model in an unsupervised manner.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input image.\n    tgt : NDArray\n        The target image to be denoised.\n    pixel_mask_trn : NDArray\n        The mask array indicating the training pixels.\n    epochs : int\n        The number of training epochs.\n    optimizer : str, optional\n        The optimization algorithm to use. Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n\n    Returns\n    -------\n    dict[str, NDArray]\n        A dictionary containing the training losses.\n\n    Notes\n    -----\n    This method trains the model using the deep image prior approach in an unsupervised manner.\n    It uses a random initialization for the input image if not provided and applies a scaling and bias\n    transformation to the input and target images. It then trains the model using the specified optimization\n    algorithm and the provided mask array indicating the training pixels.\n    \"\"\"\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    tmp_inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tmp_tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_small(\n        tmp_inp, tmp_tgt, pixel_mask_trn, epochs=epochs, optimizer=optimizer, regularizer=reg, lower_limit=lower_limit\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Unsupervised {self.__class__.__name__} {optimizer.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/","title":"autoden.algorithms.denoiser","text":""},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser","title":"denoiser","text":"<p>Base class and functions for all denoising algorithms.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>Data scale and bias.</p> </li> <li> <code>Denoiser</code>           \u2013            <p>Base denoising class.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>compute_scaling_selfsupervised</code>             \u2013              <p>Compute input data scaling and bias for self-supervised learning.</p> </li> <li> <code>compute_scaling_supervised</code>             \u2013              <p>Compute input and target data scaling and bias for supervised learning.</p> </li> <li> <code>data_to_tensor</code>             \u2013              <p>Convert a NumPy array to a PyTorch tensor.</p> </li> <li> <code>get_flip_dims</code>             \u2013              <p>Generate all possible combinations of dimensions to flip for a given number of dimensions.</p> </li> <li> <code>get_normalization_range</code>             \u2013              <p>Calculate the normalization range for a given volume.</p> </li> <li> <code>get_random_image_indices</code>             \u2013              <p>Return a list of random indices from 0 to num_imgs - 1.</p> </li> <li> <code>get_random_pixel_mask</code>             \u2013              <p>Generate a random pixel mask for a given data shape.</p> </li> <li> <code>random_flips</code>             \u2013              <p>Randomly flip images.</p> </li> <li> <code>random_rotations</code>             \u2013              <p>Randomly rotate images.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.DataScaleBias","title":"DataScaleBias  <code>dataclass</code>","text":"<pre><code>DataScaleBias(\n    scale_inp: float | NDArray = 1.0,\n    scale_out: float | NDArray = 1.0,\n    scale_tgt: float | NDArray = 1.0,\n    bias_inp: float | NDArray = 0.0,\n    bias_out: float | NDArray = 0.0,\n    bias_tgt: float | NDArray = 0.0,\n)\n</code></pre> <p>Data scale and bias.</p>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.Denoiser","title":"Denoiser","text":"<pre><code>Denoiser(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base denoising class.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train</code>             \u2013              <p>Training of the model, given the required input.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.Denoiser.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.Denoiser.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.Denoiser.train","title":"train  <code>abstractmethod</code>","text":"<pre><code>train(*args: Any, **kwds: Any) -&gt; dict[str, NDArray]\n</code></pre> <p>Training of the model, given the required input.</p> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>@abstractmethod\ndef train(self, *args: Any, **kwds: Any) -&gt; dict[str, NDArray]:\n    \"\"\"Training of the model, given the required input.\"\"\"\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.compute_scaling_selfsupervised","title":"compute_scaling_selfsupervised","text":"<pre><code>compute_scaling_selfsupervised(\n    inp: NDArray,\n) -&gt; DataScaleBias\n</code></pre> <p>Compute input data scaling and bias for self-supervised learning.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>Input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>An instance of DataScaleBias containing the computed scaling and bias values.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def compute_scaling_selfsupervised(inp: NDArray) -&gt; DataScaleBias:\n    \"\"\"\n    Compute input data scaling and bias for self-supervised learning.\n\n    Parameters\n    ----------\n    inp : NDArray\n        Input data.\n\n    Returns\n    -------\n    DataScaleBias\n        An instance of DataScaleBias containing the computed scaling and bias values.\n    \"\"\"\n    range_vals_inp = get_normalization_range(inp, percentile=0.001)\n\n    sb = DataScaleBias()\n    sb.scale_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    sb.scale_out = sb.scale_tgt = sb.scale_inp\n\n    sb.bias_inp = range_vals_inp[2] * sb.scale_inp\n    sb.bias_out = sb.bias_tgt = sb.bias_inp\n\n    return sb\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.compute_scaling_supervised","title":"compute_scaling_supervised","text":"<pre><code>compute_scaling_supervised(\n    inp: NDArray, tgt: NDArray\n) -&gt; DataScaleBias\n</code></pre> <p>Compute input and target data scaling and bias for supervised learning.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>Input data.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>Target data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataScaleBias</code>           \u2013            <p>An instance of DataScaleBias containing the computed scaling and bias values.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def compute_scaling_supervised(inp: NDArray, tgt: NDArray) -&gt; DataScaleBias:\n    \"\"\"\n    Compute input and target data scaling and bias for supervised learning.\n\n    Parameters\n    ----------\n    inp : NDArray\n        Input data.\n    tgt : NDArray\n        Target data.\n\n    Returns\n    -------\n    DataScaleBias\n        An instance of DataScaleBias containing the computed scaling and bias values.\n    \"\"\"\n    range_vals_inp = get_normalization_range(inp, percentile=0.001)\n    range_vals_tgt = get_normalization_range(tgt, percentile=0.001)\n\n    sb = DataScaleBias()\n    sb.scale_inp = 1 / (range_vals_inp[1] - range_vals_inp[0])\n    sb.scale_tgt = 1 / (range_vals_tgt[1] - range_vals_tgt[0])\n    sb.scale_out = sb.scale_tgt\n\n    sb.bias_inp = range_vals_inp[2] * sb.scale_inp\n    sb.bias_tgt = range_vals_tgt[2] * sb.scale_tgt\n    sb.bias_out = sb.bias_tgt\n\n    return sb\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.data_to_tensor","title":"data_to_tensor","text":"<pre><code>data_to_tensor(\n    data: NDArray,\n    device: str,\n    n_dims: int = 2,\n    spectral_axis: int | None = None,\n    dtype: DTypeLike | None = float32,\n) -&gt; Tensor\n</code></pre> <p>Convert a NumPy array to a PyTorch tensor.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be converted to a tensor.</p> </li> <li> <code>device</code>               (<code>str</code>)           \u2013            <p>The device to which the tensor should be moved (e.g., 'cpu', 'cuda').</p> </li> <li> <code>n_dims</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>The number of dimensions to consider for the data shape, by default 2.</p> </li> <li> <code>spectral_axis</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>The axis along which the spectral data is located, by default None.</p> </li> <li> <code>dtype</code>               (<code>DTypeLike or None</code>, default:                   <code>float32</code> )           \u2013            <p>The data type to which the data should be converted, by default np.float32.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The converted PyTorch tensor.</p> </li> </ul> Notes <p>If <code>spectral_axis</code> is provided, the data is moved to the specified axis. Otherwise, the data is expanded to include an additional dimension. The data is then reshaped and converted to the specified data type before being converted to a PyTorch tensor and moved to the specified device.</p> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def data_to_tensor(\n    data: NDArray, device: str, n_dims: int = 2, spectral_axis: int | None = None, dtype: DTypeLike | None = np.float32\n) -&gt; pt.Tensor:\n    \"\"\"\n    Convert a NumPy array to a PyTorch tensor.\n\n    Parameters\n    ----------\n    data : NDArray\n        The input data to be converted to a tensor.\n    device : str\n        The device to which the tensor should be moved (e.g., 'cpu', 'cuda').\n    n_dims : int, optional\n        The number of dimensions to consider for the data shape, by default 2.\n    spectral_axis : int or None, optional\n        The axis along which the spectral data is located, by default None.\n    dtype : DTypeLike or None, optional\n        The data type to which the data should be converted, by default np.float32.\n\n    Returns\n    -------\n    pt.Tensor\n        The converted PyTorch tensor.\n\n    Notes\n    -----\n    If `spectral_axis` is provided, the data is moved to the specified axis.\n    Otherwise, the data is expanded to include an additional dimension.\n    The data is then reshaped and converted to the specified data type before\n    being converted to a PyTorch tensor and moved to the specified device.\n    \"\"\"\n    if spectral_axis is not None:\n        num_channels = data.shape[spectral_axis]\n        data = np.moveaxis(data, spectral_axis, -n_dims - 1)\n    else:\n        num_channels = 1\n        data = np.expand_dims(data, -n_dims - 1)\n    data_shape = data.shape[-n_dims:]\n    data = data.reshape([-1, num_channels, *data_shape])\n    if dtype is not None:\n        data = data.astype(dtype)\n    return pt.tensor(data, device=device)\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.get_flip_dims","title":"get_flip_dims","text":"<pre><code>get_flip_dims(n_dims: int) -&gt; Sequence[tuple[int, ...]]\n</code></pre> <p>Generate all possible combinations of dimensions to flip for a given number of dimensions.</p> <p>Parameters:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>The number of dimensions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[tuple[int, ...]]</code>           \u2013            <p>A sequence of tuples, where each tuple represents a combination of dimensions to flip. The dimensions are represented by negative indices, ranging from -n_dims to -1.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _get_flip_dims(2)\n[(), (-2,), (-1,), (-2, -1)]\n</code></pre> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def get_flip_dims(n_dims: int) -&gt; Sequence[tuple[int, ...]]:\n    \"\"\"\n    Generate all possible combinations of dimensions to flip for a given number of dimensions.\n\n    Parameters\n    ----------\n    n_dims : int\n        The number of dimensions.\n\n    Returns\n    -------\n    Sequence[tuple[int, ...]]\n        A sequence of tuples, where each tuple represents a combination of dimensions to flip.\n        The dimensions are represented by negative indices, ranging from -n_dims to -1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; _get_flip_dims(2)\n    [(), (-2,), (-1,), (-2, -1)]\n    \"\"\"\n    return sum([[*combinations(range(-n_dims, 0), d)] for d in range(n_dims + 1)], [])\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.get_normalization_range","title":"get_normalization_range","text":"<pre><code>get_normalization_range(\n    vol: NDArray, percentile: float | None = None\n) -&gt; tuple[float, float, float]\n</code></pre> <p>Calculate the normalization range for a given volume.</p> <p>Parameters:</p> <ul> <li> <code>vol</code>               (<code>NDArray</code>)           \u2013            <p>The input volume as a NumPy array.</p> </li> <li> <code>percentile</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The percentile to use for calculating the normalization range. If None, the minimum, maximum, and mean of the entire volume are used. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, float, float]</code>           \u2013            <p>A tuple containing the minimum, maximum, and mean values of the volume within the specified percentile range. If <code>percentile</code> is None, the minimum, maximum, and mean of the entire volume are returned.</p> </li> </ul> Notes <p>If <code>percentile</code> is provided, the function calculates the indices for the minimum and maximum values based on the specified percentile. The mean value is then calculated from the range between these indices.</p> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def get_normalization_range(vol: NDArray, percentile: float | None = None) -&gt; tuple[float, float, float]:\n    \"\"\"\n    Calculate the normalization range for a given volume.\n\n    Parameters\n    ----------\n    vol : NDArray\n        The input volume as a NumPy array.\n    percentile : float, optional\n        The percentile to use for calculating the normalization range. If None, the\n        minimum, maximum, and mean of the entire volume are used. Default is None.\n\n    Returns\n    -------\n    tuple[float, float, float]\n        A tuple containing the minimum, maximum, and mean values of the volume within\n        the specified percentile range. If `percentile` is None, the minimum, maximum,\n        and mean of the entire volume are returned.\n\n    Notes\n    -----\n    If `percentile` is provided, the function calculates the indices for the minimum\n    and maximum values based on the specified percentile. The mean value is then\n    calculated from the range between these indices.\n    \"\"\"\n    if percentile is not None:\n        vol_sort = np.sort(vol.flatten())\n        ind_min = int(np.fmax(vol_sort.size * percentile, 0))\n        ind_max = int(np.fmin(vol_sort.size * (1 - percentile), vol_sort.size - 1))\n        return vol_sort[ind_min], vol_sort[ind_max], vol_sort[ind_min : ind_max + 1].mean()\n    else:\n        return vol.min(), vol.max(), vol.mean()\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.get_random_image_indices","title":"get_random_image_indices","text":"<pre><code>get_random_image_indices(\n    num_imgs: int, num_tst_ratio: float\n) -&gt; list\n</code></pre> <p>Return a list of random indices from 0 to num_imgs - 1.</p> <p>Parameters:</p> <ul> <li> <code>num_imgs</code>               (<code>int</code>)           \u2013            <p>Total number of images.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>)           \u2013            <p>Ratio of images to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>           \u2013            <p>List of random indices.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def get_random_image_indices(num_imgs: int, num_tst_ratio: float) -&gt; list:\n    \"\"\"Return a list of random indices from 0 to num_imgs - 1.\n\n    Parameters\n    ----------\n    num_imgs : int\n        Total number of images.\n    num_tst_ratio : float\n        Ratio of images to select.\n\n    Returns\n    -------\n    list\n        List of random indices.\n    \"\"\"\n    num_tst_imgs = int(num_imgs * num_tst_ratio)\n    return list(np.random.choice(num_imgs, size=num_tst_imgs, replace=False))\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.get_random_pixel_mask","title":"get_random_pixel_mask","text":"<pre><code>get_random_pixel_mask(\n    data_shape: Sequence[int] | NDArray,\n    mask_pixel_ratio: float,\n) -&gt; NDArray\n</code></pre> <p>Generate a random pixel mask for a given data shape.</p> <p>This function creates a mask where a specified ratio of pixels are set to True, effectively masking those pixels. The remaining pixels are set to True.</p> <p>Parameters:</p> <ul> <li> <code>data_shape</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The shape of the data array for which the mask is to be generated.</p> </li> <li> <code>mask_pixel_ratio</code>               (<code>float</code>)           \u2013            <p>The ratio of pixels to be masked (set to True). Must be between 0 and 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>A boolean array of the same shape as <code>data_shape</code> with the specified ratio of pixels set to True.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data_shape = (10, 10)\n&gt;&gt;&gt; mask_pixel_ratio = 0.1\n&gt;&gt;&gt; mask = get_random_pixel_mask(data_shape, mask_pixel_ratio)\n&gt;&gt;&gt; print(mask)\n</code></pre> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def get_random_pixel_mask(data_shape: Sequence[int] | NDArray, mask_pixel_ratio: float) -&gt; NDArray:\n    \"\"\"\n    Generate a random pixel mask for a given data shape.\n\n    This function creates a mask where a specified ratio of pixels are set to True,\n    effectively masking those pixels. The remaining pixels are set to True.\n\n    Parameters\n    ----------\n    data_shape : Sequence[int] | NDArray\n        The shape of the data array for which the mask is to be generated.\n    mask_pixel_ratio : float\n        The ratio of pixels to be masked (set to True). Must be between 0 and 1.\n\n    Returns\n    -------\n    NDArray\n        A boolean array of the same shape as `data_shape` with the specified ratio\n        of pixels set to True.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data_shape = (10, 10)\n    &gt;&gt;&gt; mask_pixel_ratio = 0.1\n    &gt;&gt;&gt; mask = get_random_pixel_mask(data_shape, mask_pixel_ratio)\n    &gt;&gt;&gt; print(mask)\n    \"\"\"\n    data_mask = np.zeros(data_shape, dtype=bool)\n    rnd_inds = np.random.randint(low=0, high=data_mask.size, size=int(data_mask.size * mask_pixel_ratio))\n    data_mask.flat[rnd_inds] = True\n    return data_mask\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.random_flips","title":"random_flips","text":"<pre><code>random_flips(\n    *imgs: Tensor,\n    flips: Sequence[tuple[int, ...]] | None = None\n) -&gt; Sequence[Tensor]\n</code></pre> <p>Randomly flip images.</p> <p>Parameters:</p> <ul> <li> <code>*imgs</code>               (<code>Tensor</code>, default:                   <code>()</code> )           \u2013            <p>The input images</p> </li> <li> <code>flips</code>               (<code>Sequence[tuple[int, ...]] | None</code>, default:                   <code>None</code> )           \u2013            <p>If None, it will call _get_flip_dims on the ndim of the first image. The flips to be selected from, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Tensor]</code>           \u2013            <p>The flipped images.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def random_flips(*imgs: pt.Tensor, flips: Sequence[tuple[int, ...]] | None = None) -&gt; Sequence[pt.Tensor]:\n    \"\"\"Randomly flip images.\n\n    Parameters\n    ----------\n    *imgs : torch.Tensor\n        The input images\n    flips : Sequence[tuple[int, ...]] | None, optional\n        If None, it will call _get_flip_dims on the ndim of the first image.\n        The flips to be selected from, by default None.\n\n    Returns\n    -------\n    Sequence[torch.Tensor]\n        The flipped images.\n    \"\"\"\n    if flips is None:\n        flips = get_flip_dims(imgs[0].ndim - 2)\n    rand_val = np.random.randint(len(flips))\n\n    flip = flips[rand_val]\n    return [pt.flip(im, flip) for im in imgs]\n</code></pre>"},{"location":"reference/autoden/algorithms/denoiser/#autoden.algorithms.denoiser.random_rotations","title":"random_rotations","text":"<pre><code>random_rotations(\n    *imgs: Tensor, dims: tuple[int, int] = (-2, -1)\n) -&gt; Sequence[Tensor]\n</code></pre> <p>Randomly rotate images.</p> <p>Parameters:</p> <ul> <li> <code>*imgs</code>               (<code>Tensor</code>, default:                   <code>()</code> )           \u2013            <p>The input images</p> </li> <li> <code>dims</code>               (<code>tuple[int, int]</code>, default:                   <code>(-2, -1)</code> )           \u2013            <p>The dimensions to rotate, by default (-2, -1)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Tensor]</code>           \u2013            <p>The rotated images.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def random_rotations(*imgs: pt.Tensor, dims: tuple[int, int] = (-2, -1)) -&gt; Sequence[pt.Tensor]:\n    \"\"\"Randomly rotate images.\n\n    Parameters\n    ----------\n    *imgs : torch.Tensor\n        The input images\n    dims : tuple[int, int], optional\n        The dimensions to rotate, by default (-2, -1)\n\n    Returns\n    -------\n    Sequence[torch.Tensor]\n        The rotated images.\n    \"\"\"\n    rand_val = np.random.randint(4)\n\n    if rand_val &gt; 0:\n        return [pt.rot90(im, k=rand_val, dims=dims) for im in imgs]\n    else:\n        return imgs\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2noise/","title":"autoden.algorithms.noise2noise","text":""},{"location":"reference/autoden/algorithms/noise2noise/#autoden.algorithms.noise2noise","title":"noise2noise","text":"<p>Self-supervised denoiser implementation, based on Noise2Noise.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>N2N</code>           \u2013            <p>Self-supervised denoising from pairs of images.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/noise2noise/#autoden.algorithms.noise2noise.N2N","title":"N2N","text":"<pre><code>N2N(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from pairs of images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Perform inference on the input data.</p> </li> <li> <code>prepare_data</code>             \u2013              <p>Prepare input data for training.</p> </li> <li> <code>train</code>             \u2013              <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2noise/#autoden.algorithms.noise2noise.N2N.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/noise2noise/#autoden.algorithms.noise2noise.N2N.infer","title":"infer","text":"<pre><code>infer(inp: NDArray, average_splits: bool = True) -&gt; NDArray\n</code></pre> <p>Perform inference on the input data.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to perform inference on. It is expected to have an extra dimension including the different splits.</p> </li> <li> <code>average_splits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the splits are averaged. Default is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The inferred output data. If <code>average_splits</code> is True, the splits are averaged.</p> </li> </ul> Notes <p>If <code>self.batch_size</code> is set, the input data is processed in batches to avoid memory issues.</p> Source code in <code>src/autoden/algorithms/noise2noise.py</code> <pre><code>def infer(self, inp: NDArray, average_splits: bool = True) -&gt; NDArray:\n    \"\"\"\n    Perform inference on the input data.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to perform inference on. It is expected to have an extra dimension including the different splits.\n    average_splits : bool, optional\n        If True, the splits are averaged. Default is True.\n\n    Returns\n    -------\n    NDArray\n        The inferred output data. If `average_splits` is True, the splits are averaged.\n\n    Notes\n    -----\n    If `self.batch_size` is set, the input data is processed in batches to avoid memory issues.\n    \"\"\"\n    inp_shape = inp.shape\n    inp = inp.reshape([inp_shape[0] * inp_shape[1], *inp_shape[2:]])\n    if self.batch_size is not None:\n        out = []\n        for b in tqdm(range(0, inp.shape[0], self.batch_size), desc=\"Inference batch\"):\n            out.append(super().infer(inp[b : b + self.batch_size]))\n        out = np.concatenate(out, axis=0)\n    else:\n        out = super().infer(inp)\n    out = out.reshape(inp_shape)\n    if average_splits:\n        realization_batch_axis = -self.n_dims - 1\n        out = out.mean(axis=realization_batch_axis)\n    return out\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2noise/#autoden.algorithms.noise2noise.N2N.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    inp: NDArray,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"1:X\",\n) -&gt; tuple[NDArray, NDArray, NDArray]\n</code></pre> <p>Prepare input data for training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the input data to be used for testing. The remaining data will be used for training. Default is 0.2.</p> </li> <li> <code>strategy</code>               (<code>str</code>, default:                   <code>'1:X'</code> )           \u2013            <p>The strategy to be used for creating input-target pairs. The available strategies are: - \"1:X\": Use the mean of the remaining samples as the target for each sample. - \"X:1\": Use the mean of the remaining samples as the input for each sample. Default is \"1:X\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, NDArray, NDArray]</code>           \u2013            <p>A tuple containing: - The input data array. - The target data array. - The mask array indicating the training pixels.</p> </li> </ul> Notes <p>This function generates input-target pairs based on the specified strategy. It also generates a mask array indicating the training pixels based on the provided ratio.</p> Source code in <code>src/autoden/algorithms/noise2noise.py</code> <pre><code>def prepare_data(\n    self, inp: NDArray, num_tst_ratio: float = 0.2, strategy: str = \"1:X\"\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"\n    Prepare input data for training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    num_tst_ratio : float, optional\n        The ratio of the input data to be used for testing. The remaining data will be used for training.\n        Default is 0.2.\n    strategy : str, optional\n        The strategy to be used for creating input-target pairs. The available strategies are:\n        - \"1:X\": Use the mean of the remaining samples as the target for each sample.\n        - \"X:1\": Use the mean of the remaining samples as the input for each sample.\n        Default is \"1:X\".\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        A tuple containing:\n        - The input data array.\n        - The target data array.\n        - The mask array indicating the training pixels.\n\n    Notes\n    -----\n    This function generates input-target pairs based on the specified strategy. It also generates a mask array\n    indicating the training pixels based on the provided ratio.\n    \"\"\"\n    if inp.ndim &lt; self.n_dims + 1:\n        raise ValueError(f\"Target data should at least be of {self.n_dims + 1} dimensions, but its shape is {inp.shape}\")\n\n    realizations_batch_axis = inp.ndim - self.n_dims - 1\n\n    inp_x = np.stack([np.delete(inp, obj=ii, axis=0).mean(axis=0) for ii in range(len(inp))], axis=realizations_batch_axis)\n    inp = inp.swapaxes(0, realizations_batch_axis)\n\n    if strategy.upper() == \"1:X\":\n        tmp_inp = inp\n        tmp_tgt = inp_x\n    elif strategy.upper() == \"X:1\":\n        tmp_inp = inp_x\n        tmp_tgt = inp\n    else:\n        raise ValueError(f\"Strategy {strategy} not implemented. Please choose one of: ['1:X', 'X:1']\")\n\n    mask_trn = get_random_pixel_mask(inp.shape, mask_pixel_ratio=num_tst_ratio)\n\n    return tmp_inp, tmp_tgt, mask_trn\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2noise/#autoden.algorithms.noise2noise.N2N.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_tst: NDArray,\n    epochs: int,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Train the denoiser using the Noise2Noise self-supervised approach.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the number of samples, and H and W are the height and width of each sample, respectively.</p> </li> <li> <code>pixel_mask_tst</code>               (<code>NDArray</code>)           \u2013            <p>The mask array indicating the test pixels.</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>The number of epochs to train the model.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The learning rate for the optimizer. Default is 1e-3.</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to be used for training. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> <li> <code>restarts</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing of the learning rate will be restarted the specified number of times. Default is None.</p> </li> <li> <code>accum_grads</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple batches before updating the model parameters. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, NDArray]</code>           \u2013            <p>A dictionary containing the training losses.</p> </li> </ul> Notes <p>This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to generate target data based on the specified strategy. The training process involves creating pairs of input and target data and then training the model to minimize the difference between the predicted and target data.</p> Source code in <code>src/autoden/algorithms/noise2noise.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    pixel_mask_tst: NDArray,\n    epochs: int,\n    learning_rate: float = 1e-3,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Train the denoiser using the Noise2Noise self-supervised approach.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    tgt : NDArray\n        The target data to be used for training. This should be a NumPy array of shape (N, H, W), where N is the\n        number of samples, and H and W are the height and width of each sample, respectively.\n    pixel_mask_tst : NDArray\n        The mask array indicating the test pixels.\n    epochs : int\n        The number of epochs to train the model.\n    learning_rate : float, optional\n        The learning rate for the optimizer. Default is 1e-3.\n    optimizer : str, optional\n        The optimization algorithm to be used for training. Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n    restarts : int | None, optional\n        The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing\n        of the learning rate will be restarted the specified number of times. Default is None.\n    accum_grads : bool, optional\n        Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple\n        batches before updating the model parameters. Default is False.\n\n    Returns\n    -------\n    dict[str, NDArray]\n        A dictionary containing the training losses.\n\n    Notes\n    -----\n    This method uses the Noise2Noise self-supervised approach to train the denoiser. The input data is used to\n    generate target data based on the specified strategy. The training process involves creating pairs of input\n    and target data and then training the model to minimize the difference between the predicted and target data.\n    \"\"\"\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    tmp_inp = inp.astype(np.float32)\n    tmp_tgt = tgt.astype(np.float32)\n\n    reg = self._get_regularization()\n    losses = self._train_pixelmask_batched(\n        tmp_inp,\n        tmp_tgt,\n        pixel_mask_tst,\n        epochs=epochs,\n        learning_rate=learning_rate,\n        optimizer=optimizer,\n        regularizer=reg,\n        lower_limit=lower_limit,\n        restarts=restarts,\n        accum_grads=accum_grads,\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {optimizer.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2void/","title":"autoden.algorithms.noise2void","text":""},{"location":"reference/autoden/algorithms/noise2void/#autoden.algorithms.noise2void","title":"noise2void","text":"<p>Self-supervised denoiser implementation, based on Noise2Void.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>N2V</code>           \u2013            <p>Self-supervised denoising from single images.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/noise2void/#autoden.algorithms.noise2void.N2V","title":"N2V","text":"<pre><code>N2V(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Self-supervised denoising from single images.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>train</code>             \u2013              <p>Self-supervised training.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2void/#autoden.algorithms.noise2void.N2V.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/noise2void/#autoden.algorithms.noise2void.N2V.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/noise2void/#autoden.algorithms.noise2void.N2V.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Self-supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images, which will also be targets</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices</p> </li> <li> <code>mask_shape</code>               (<code>int | Sequence[int] | NDArray</code>, default:                   <code>1</code> )           \u2013            <p>Shape of the blind spot mask, by default 1.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>Optimizer algorithm to use, by default \"adam\"</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> </ul> Source code in <code>src/autoden/algorithms/noise2void.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    epochs: int,\n    tst_inds: Sequence[int] | NDArray,\n    mask_shape: int | Sequence[int] | NDArray = 1,\n    ratio_blind_spot: float = 0.015,\n    algo: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n) -&gt; dict[str, NDArray]:\n    \"\"\"Self-supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images, which will also be targets\n    epochs : int\n        Number of training epochs\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices\n    mask_shape : int | Sequence[int] | NDArray\n        Shape of the blind spot mask, by default 1.\n    algo : str, optional\n        Optimizer algorithm to use, by default \"adam\"\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n    \"\"\"\n    num_imgs = inp.shape[0]\n    tst_inds = np.array(tst_inds, dtype=int)\n    if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n        raise ValueError(\n            f\"Each cross-validation index should be greater or equal than 0, and less than the number of images {num_imgs}\"\n        )\n    trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_selfsupervised(inp)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_trn = inp[trn_inds]\n    inp_tst = inp[tst_inds]\n\n    reg = self._get_regularization()\n    losses = self._train_n2v_pixelmask_small(\n        inp_trn,\n        inp_tst,\n        epochs=epochs,\n        mask_shape=mask_shape,\n        ratio_blind_spot=ratio_blind_spot,\n        algo=algo,\n        regularizer=reg,\n        lower_limit=lower_limit,\n    )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Self-supervised {self.__class__.__name__} {algo.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/algorithms/supervised/","title":"autoden.algorithms.supervised","text":""},{"location":"reference/autoden/algorithms/supervised/#autoden.algorithms.supervised","title":"supervised","text":"<p>Supervised denoiser implementation.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>Supervised</code>           \u2013            <p>Supervised denoising class.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/supervised/#autoden.algorithms.supervised.Supervised","title":"Supervised","text":"<pre><code>Supervised(\n    model: int | str | NetworkParams | Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>Denoiser</code></p> <p>Supervised denoising class.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Module | Mapping | None</code>)           \u2013            <p>Type of neural network to use or a specific network (or state) to use</p> </li> <li> <code>data_scale_bias</code>               (<code>DataScaleBias | None</code>, default:                   <code>None</code> )           \u2013            <p>Scale and bias of the input data, by default None</p> </li> <li> <code>reg_val</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Regularization value, by default 1e-5</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"</p> </li> <li> <code>save_epochs_dir</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory where to save network states at each epoch. If None disabled, by default None</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to produce verbose output, by default True</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>infer</code>             \u2013              <p>Inference, given an initial stack of images.</p> </li> <li> <code>prepare_data</code>             \u2013              <p>Prepare input data for training.</p> </li> <li> <code>train</code>             \u2013              <p>Supervised training.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Returns the expected signal dimensions.</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def __init__(\n    self,\n    model: int | str | NetworkParams | pt.nn.Module | Mapping,\n    data_scale_bias: DataScaleBias | None = None,\n    reg_val: float | LossRegularizer | None = None,\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    batch_size: int | None = None,\n    augmentation: str | Sequence[str] | None = None,\n    save_epochs_dir: str | None = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the noise2noise method.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | pt.nn.Module | Mapping | None\n        Type of neural network to use or a specific network (or state) to use\n    data_scale_bias : DataScaleBias | None, optional\n        Scale and bias of the input data, by default None\n    reg_val : float | None, optional\n        Regularization value, by default 1e-5\n    device : str, optional\n        Device to use, by default \"cuda\" if cuda is available, otherwise \"cpu\"\n    save_epochs_dir : str | None, optional\n        Directory where to save network states at each epoch.\n        If None disabled, by default None\n    verbose : bool, optional\n        Whether to produce verbose output, by default True\n    \"\"\"\n    if isinstance(model, int):\n        if self.save_epochs_dir is None:\n            raise ValueError(\"Directory for saving epochs not specified\")\n\n        model = load_model_state(self.save_epochs_dir, epoch_num=model)\n\n    if isinstance(model, (str, NetworkParams, Mapping, pt.nn.Module)):\n        self.model = create_network(model, device=device)\n    else:\n        raise ValueError(f\"Invalid model {type(model)}\")\n    if verbose:\n        get_num_parameters(self.model, verbose=True)\n\n    if augmentation is None:\n        augmentation = []\n    elif isinstance(augmentation, str):\n        augmentation = [augmentation.lower()]\n    elif isinstance(augmentation, Sequence):\n        augmentation = [str(a).lower() for a in augmentation]\n\n    self.data_sb = data_scale_bias\n\n    self.reg_val = reg_val\n    self.device = device\n    self.batch_size = batch_size\n    self.augmentation = augmentation\n    self.save_epochs_dir = save_epochs_dir\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/autoden/algorithms/supervised/#autoden.algorithms.supervised.Supervised.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the expected signal dimensions.</p> <p>If the model is an instance of <code>SerializableModel</code> and has an <code>init_params</code> attribute containing the key <code>\"n_dims\"</code>, this property returns the value associated with <code>\"n_dims\"</code>. Otherwise, it defaults to 2.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The expected signal dimensions.</p> </li> </ul>"},{"location":"reference/autoden/algorithms/supervised/#autoden.algorithms.supervised.Supervised.infer","title":"infer","text":"<pre><code>infer(inp: NDArray) -&gt; NDArray\n</code></pre> <p>Inference, given an initial stack of images.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input stack of images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>The denoised stack of images</p> </li> </ul> Source code in <code>src/autoden/algorithms/denoiser.py</code> <pre><code>def infer(self, inp: NDArray) -&gt; NDArray:\n    \"\"\"Inference, given an initial stack of images.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input stack of images\n\n    Returns\n    -------\n    NDArray\n        The denoised stack of images\n    \"\"\"\n    # Rescale input\n    if self.data_sb is not None:\n        inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n\n    inp_t = data_to_tensor(inp, device=self.device, n_dims=self.n_dims)\n\n    self.model.eval()\n    with pt.inference_mode():\n        out_t: pt.Tensor = self.model(inp_t)\n        output = out_t.squeeze(dim=(0, 1)).to(\"cpu\").numpy()\n\n    # Rescale output\n    if self.data_sb is not None:\n        output = (output + self.data_sb.bias_out) / self.data_sb.scale_out\n\n    return output\n</code></pre>"},{"location":"reference/autoden/algorithms/supervised/#autoden.algorithms.supervised.Supervised.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    inp: NDArray,\n    tgt: NDArray,\n    num_tst_ratio: float = 0.2,\n    strategy: str = \"pixel-mask\",\n) -&gt; tuple[NDArray, NDArray, NDArray | list[int]]\n</code></pre> <p>Prepare input data for training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the number of samples, and D, H and W are the depth, height and width of each sample, respectively.</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the number of samples, and D, H and W are the depth, height and width of each sample, respectively.</p> </li> <li> <code>num_tst_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The ratio of the input data to be used for testing. The remaining data will be used for training. Default is 0.2.</p> </li> <li> <code>strategy</code>               (<code>str</code>, default:                   <code>'pixel-mask'</code> )           \u2013            <p>The strategy to be used for creating training and testing sets. The available strategies are: - \"pixel-mask\": Use randomly chosen pixels in the images as test set. - \"self-similar\": Use entire randomly chosen images as test set. Default is \"pixel-mask\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, NDArray, NDArray]</code>           \u2013            <p>A tuple containing: - The input data array. - The target data array. - Either the mask array indicating the testing pixels or the list of test indices.</p> </li> </ul> Source code in <code>src/autoden/algorithms/supervised.py</code> <pre><code>def prepare_data(\n    self, inp: NDArray, tgt: NDArray, num_tst_ratio: float = 0.2, strategy: str = \"pixel-mask\"\n) -&gt; tuple[NDArray, NDArray, NDArray | list[int]]:\n    \"\"\"\n    Prepare input data for training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the\n        number of samples, and D, H and W are the depth, height and width of each sample, respectively.\n    tgt : NDArray\n        The target data to be used for training. This should be a NumPy array of shape (N, [D, H], W), where N is the\n        number of samples, and D, H and W are the depth, height and width of each sample, respectively.\n    num_tst_ratio : float, optional\n        The ratio of the input data to be used for testing. The remaining data will be used for training.\n        Default is 0.2.\n    strategy : str, optional\n        The strategy to be used for creating training and testing sets. The available strategies are:\n        - \"pixel-mask\": Use randomly chosen pixels in the images as test set.\n        - \"self-similar\": Use entire randomly chosen images as test set.\n        Default is \"pixel-mask\".\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        A tuple containing:\n        - The input data array.\n        - The target data array.\n        - Either the mask array indicating the testing pixels or the list of test indices.\n    \"\"\"\n    if inp.ndim &lt; self.n_dims:\n        raise ValueError(f\"Target data should at least be of {self.n_dims} dimensions, but its shape is {inp.shape}\")\n\n    num_imgs = inp.shape[0]\n    if tgt.ndim == (inp.ndim - 1):\n        tgt = np.tile(tgt[None, ...], [num_imgs, *np.ones_like(tgt.shape)])\n\n    if inp.shape != tgt.shape:\n        raise ValueError(\n            f\"Input and target data must have the same shape. Input shape: {inp.shape}, Target shape: {tgt.shape}\"\n        )\n\n    if strategy.lower() == \"pixel-mask\":\n        mask_tst = get_random_pixel_mask(inp.shape, mask_pixel_ratio=num_tst_ratio)\n    elif strategy.lower() == \"self-similar\":\n        mask_tst = get_random_image_indices(num_imgs, num_tst_ratio=num_tst_ratio)\n    else:\n        raise ValueError(f\"Strategy {strategy} not implemented. Please choose one of: ['pixel-mask', 'self-similar']\")\n\n    return inp, tgt, mask_tst\n</code></pre>"},{"location":"reference/autoden/algorithms/supervised/#autoden.algorithms.supervised.Supervised.train","title":"train","text":"<pre><code>train(\n    inp: NDArray,\n    tgt: NDArray,\n    tst_inds: Sequence[int] | NDArray,\n    epochs: int,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]\n</code></pre> <p>Supervised training.</p> <p>Parameters:</p> <ul> <li> <code>inp</code>               (<code>NDArray</code>)           \u2013            <p>The input images</p> </li> <li> <code>tgt</code>               (<code>NDArray</code>)           \u2013            <p>The target images</p> </li> <li> <code>tst_inds</code>               (<code>Sequence[int] | NDArray</code>)           \u2013            <p>The validation set indices (either image indices if Sequence[int] or pixel indices if NDArray)</p> </li> <li> <code>epochs</code>               (<code>int</code>)           \u2013            <p>Number of training epochs</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The learning rate for the optimizer. Default is 1e-3.</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The optimization algorithm to be used for training. Default is \"adam\".</p> </li> <li> <code>lower_limit</code>               (<code>float | NDArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The lower limit for the input data. If provided, the input data will be clipped to this limit. Default is None.</p> </li> <li> <code>restarts</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing of the learning rate will be restarted the specified number of times. Default is None.</p> </li> <li> <code>accum_grads</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple batches before updating the model parameters. Default is False.</p> </li> </ul> Source code in <code>src/autoden/algorithms/supervised.py</code> <pre><code>def train(\n    self,\n    inp: NDArray,\n    tgt: NDArray,\n    tst_inds: Sequence[int] | NDArray,\n    epochs: int,\n    learning_rate: float = 1e-3,\n    optimizer: str = \"adam\",\n    lower_limit: float | NDArray | None = None,\n    restarts: int | None = None,\n    accum_grads: bool = False,\n) -&gt; dict[str, NDArray]:\n    \"\"\"Supervised training.\n\n    Parameters\n    ----------\n    inp : NDArray\n        The input images\n    tgt : NDArray\n        The target images\n    tst_inds : Sequence[int] | NDArray\n        The validation set indices (either image indices if Sequence[int] or pixel indices if NDArray)\n    epochs : int\n        Number of training epochs\n    learning_rate : float, optional\n        The learning rate for the optimizer. Default is 1e-3.\n    optimizer : str, optional\n        The optimization algorithm to be used for training. Default is \"adam\".\n    lower_limit : float | NDArray | None, optional\n        The lower limit for the input data. If provided, the input data will be clipped to this limit.\n        Default is None.\n    restarts : int | None, optional\n        The number of times to restart the cosine annealing of the learning rate. If provided, the cosine annealing\n        of the learning rate will be restarted the specified number of times. Default is None.\n    accum_grads : bool, optional\n        Whether to accumulate gradients over multiple batches. If True, gradients will be accumulated over multiple\n        batches before updating the model parameters. Default is False.\n    \"\"\"\n    num_imgs = inp.shape[0]\n\n    if self.data_sb is None:\n        self.data_sb = compute_scaling_supervised(inp, tgt)\n\n    # Rescale the datasets\n    inp = inp * self.data_sb.scale_inp - self.data_sb.bias_inp\n    tgt = tgt * self.data_sb.scale_tgt - self.data_sb.bias_tgt\n\n    inp = inp.astype(np.float32)\n    tgt = tgt.astype(np.float32)\n\n    reg = self._get_regularization()\n\n    if isinstance(tst_inds, Sequence):\n        tst_inds = np.array(tst_inds, dtype=int)\n        if np.any(tst_inds &lt; 0) or np.any(tst_inds &gt;= num_imgs):\n            raise ValueError(\n                \"Each cross-validation index should be greater or equal than 0,\"\n                f\" and less than the number of images {num_imgs}\"\n            )\n        trn_inds = np.delete(np.arange(num_imgs), obj=tst_inds)\n\n        # Create datasets\n        dset_trn = (inp[trn_inds], tgt[trn_inds])\n        dset_tst = (inp[tst_inds], tgt[tst_inds])\n\n        losses = self._train_selfsimilar_batched(\n            dset_trn,\n            dset_tst,\n            epochs=epochs,\n            learning_rate=learning_rate,\n            optimizer=optimizer,\n            regularizer=reg,\n            lower_limit=lower_limit,\n            restarts=restarts,\n            accum_grads=accum_grads,\n        )\n    elif isinstance(tst_inds, np.ndarray):\n        losses = self._train_pixelmask_batched(\n            inp,\n            tgt,\n            tst_inds,\n            epochs=epochs,\n            learning_rate=learning_rate,\n            optimizer=optimizer,\n            regularizer=reg,\n            lower_limit=lower_limit,\n            restarts=restarts,\n            accum_grads=accum_grads,\n        )\n    else:\n        raise ValueError(\n            \"`tst_inds` should either be a Sequence[int] or NDArray. Please use the the `prepare_data` function if unsure.\"\n        )\n\n    if self.verbose:\n        self._plot_loss_curves(losses, f\"Supervised {optimizer.upper()}\")\n\n    return losses\n</code></pre>"},{"location":"reference/autoden/models/","title":"autoden.models","text":""},{"location":"reference/autoden/models/#autoden.models","title":"models","text":"<p>Models sub-package.</p> <p>Implementation of models like DnCNN, MS-D net, UNet and a custom ResNet.</p> <p>Adapted from: https://github.com/ahendriksen/noise2inverse</p> <p>Modules:</p> <ul> <li> <code>config</code>           \u2013            <p>High level definition of CNN architectures.</p> </li> <li> <code>dncnn</code>           \u2013            </li> <li> <code>io</code>           \u2013            <p>IO module.</p> </li> <li> <code>msd</code>           \u2013            <p>Module implementing MS-D net.</p> </li> <li> <code>param_utils</code>           \u2013            <p>This module provides utility functions for handling PyTorch models, including</p> </li> <li> <code>resnet</code>           \u2013            </li> <li> <code>unet</code>           \u2013            <p>Implementation of a flexible U-net.</p> </li> </ul>"},{"location":"reference/autoden/models/config/","title":"autoden.models.config","text":""},{"location":"reference/autoden/models/config/#autoden.models.config","title":"config","text":"<p>High level definition of CNN architectures.</p> <p>@author: Nicola VIGAN\u00d2, CEA-MEM, Grenoble, France</p> <p>Classes:</p> <ul> <li> <code>NetworkParams</code>           \u2013            <p>Abstract base class for storing network parameters.</p> </li> <li> <code>NetworkParamsDnCNN</code>           \u2013            <p>Store DnCNN parameters.</p> </li> <li> <code>NetworkParamsMSD</code>           \u2013            <p>Store MS-D net parameters.</p> </li> <li> <code>NetworkParamsResnet</code>           \u2013            <p>Store Resnet parameters.</p> </li> <li> <code>NetworkParamsUNet</code>           \u2013            <p>Store UNet parameters.</p> </li> <li> <code>SerializableModel</code>           \u2013            <p>Protocol for serializable models.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_network</code>             \u2013              <p>Create and return a neural network model based on the provided network configuration.</p> </li> <li> <code>create_optimizer</code>             \u2013              <p>Instantiates the desired optimizer for the given model.</p> </li> </ul>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParams","title":"NetworkParams","text":"<pre><code>NetworkParams(\n    n_features: int,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for storing network parameters.</p> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get the associated model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(self, n_features: int, n_channels_in: int = 1, n_channels_out: int = 1, n_dims: int = 2) -&gt; None:\n    self.n_channels_in = n_channels_in\n    self.n_channels_out = n_channels_out\n    self.n_features = n_features\n    self.n_dims = n_dims\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParams.get_model","title":"get_model  <code>abstractmethod</code>","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get the associated model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>@abstractmethod\ndef get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get the associated model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsDnCNN","title":"NetworkParamsDnCNN","text":"<pre><code>NetworkParamsDnCNN(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 20,\n    n_features: int = 64,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store DnCNN parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels. Default is 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels. Default is 1.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Number of layers. Default is 20.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Number of features. Default is 64.</p> </li> <li> <code>n_dims</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of dimensions of the signals/convolutions, by default 2.</p> </li> <li> <code>kernel_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Size of the convolutional kernel. Default is 3.</p> </li> <li> <code>pad_mode</code>               (<code>str</code>, default:                   <code>'replicate'</code> )           \u2013            <p>Padding mode for the convolutional layers. Default is \"replicate\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a DnCNN model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 20,\n    n_features: int = 64,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the DnCNN network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_layers : int, optional\n        Number of layers. Default is 20.\n    n_features : int, optional\n        Number of features. Default is 64.\n    n_dims : int, optional\n        Number of dimensions of the signals/convolutions, by default 2.\n    kernel_size : int, optional\n        Size of the convolutional kernel. Default is 3.\n    pad_mode : str, optional\n        Padding mode for the convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out, n_dims=n_dims)\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsDnCNN.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a DnCNN model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The DnCNN model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a DnCNN model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The DnCNN model.\n    \"\"\"\n    return DnCNN(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        n_dims=self.n_dims,\n        kernel_size=self.kernel_size,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsMSD","title":"NetworkParamsMSD","text":"<pre><code>NetworkParamsMSD(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    n_dims: int = 2,\n    dilations: Sequence[int] | NDArray[integer] = arange(\n        1, 4\n    ),\n    use_dilations: bool = True,\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store MS-D net parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels, by default 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels, by default 1.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>12</code> )           \u2013            <p>Number of layers in the network, by default 12.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of features, by default 1.</p> </li> <li> <code>n_dims</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of dimensions of the signals/convolutions, by default 2.</p> </li> <li> <code>dilations</code>               (<code>Sequence[int] | NDArray[integer]</code>, default:                   <code>arange(1, 4)</code> )           \u2013            <p>Dilation values for the network, by default np.arange(1, 4).</p> </li> <li> <code>use_dilations</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use dilations in the network, by default True.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a MS-D net model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    n_dims: int = 2,\n    dilations: Sequence[int] | NDArray[np.integer] = np.arange(1, 4),\n    use_dilations: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the MS-D network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels, by default 1.\n    n_channels_out : int, optional\n        Number of output channels, by default 1.\n    n_layers : int, optional\n        Number of layers in the network, by default 12.\n    n_features : int, optional\n        Number of features, by default 1.\n    n_dims : int, optional\n        Number of dimensions of the signals/convolutions, by default 2.\n    dilations : Sequence[int] | NDArray[np.integer], optional\n        Dilation values for the network, by default np.arange(1, 4).\n    use_dilations : bool, optional\n        Whether to use dilations in the network, by default True.\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out, n_dims=n_dims)\n    self.n_layers = n_layers\n    self.dilations = dilations\n    self.use_dilations = use_dilations\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsMSD.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a MS-D net model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a MS-D net model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The model.\n    \"\"\"\n    return MSDnet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        dilations=list(self.dilations),\n        device=device,\n        use_dilations=self.use_dilations,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsResnet","title":"NetworkParamsResnet","text":"<pre><code>NetworkParamsResnet(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 10,\n    n_features: int = 24,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store Resnet parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels. Default is 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels. Default is 1.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of layers. Default is 10.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>24</code> )           \u2013            <p>Number of features. Default is 24.</p> </li> <li> <code>n_dims</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of dimensions of the signals/convolutions, by default 2.</p> </li> <li> <code>kernel_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Size of the convolutional kernel. Default is 3.</p> </li> <li> <code>pad_mode</code>               (<code>str</code>, default:                   <code>'replicate'</code> )           \u2013            <p>Padding mode for the convolutional layers. Default is \"replicate\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a Resnet model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 10,\n    n_features: int = 24,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the Resnet network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_layers : int, optional\n        Number of layers. Default is 10.\n    n_features : int, optional\n        Number of features. Default is 24.\n    n_dims : int, optional\n        Number of dimensions of the signals/convolutions, by default 2.\n    kernel_size : int, optional\n        Size of the convolutional kernel. Default is 3.\n    pad_mode : str, optional\n        Padding mode for the convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out, n_dims=n_dims)\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsResnet.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a Resnet model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The Resnet model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a Resnet model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The Resnet model.\n    \"\"\"\n    return Resnet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_layers=self.n_layers,\n        n_features=self.n_features,\n        n_dims=self.n_dims,\n        kernel_size=self.kernel_size,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsUNet","title":"NetworkParamsUNet","text":"<pre><code>NetworkParamsUNet(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_levels: int = DEFAULT_LEVELS,\n    n_features: int = DEFAULT_FEATURES,\n    n_channels_skip: int | None = None,\n    n_dims: int = 2,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>NetworkParams</code></p> <p>Store UNet parameters.</p> <p>Parameters:</p> <ul> <li> <code>n_channels_in</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of input channels. Default is 1.</p> </li> <li> <code>n_channels_out</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of output channels. Default is 1.</p> </li> <li> <code>n_levels</code>               (<code>int</code>, default:                   <code>DEFAULT_LEVELS</code> )           \u2013            <p>Number of levels in the UNet. Default is 3.</p> </li> <li> <code>n_features</code>               (<code>int</code>, default:                   <code>DEFAULT_FEATURES</code> )           \u2013            <p>Number of features in the UNet. Default is 32.</p> </li> <li> <code>n_channels_skip</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of skip connections channels. Default is None.</p> </li> <li> <code>n_dims</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of dimensions of the signals/convolutions, by default 2.</p> </li> <li> <code>bilinear</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bilinear interpolation. Default is True.</p> </li> <li> <code>pad_mode</code>               (<code>str</code>, default:                   <code>'replicate'</code> )           \u2013            <p>Padding mode for convolutional layers. Default is \"replicate\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_model</code>             \u2013              <p>Get a U-net model with the selected parameters.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_levels: int = DEFAULT_LEVELS,\n    n_features: int = DEFAULT_FEATURES,\n    n_channels_skip: int | None = None,\n    n_dims: int = 2,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n) -&gt; None:\n    \"\"\"Initialize the UNet network parameters definition.\n\n    Parameters\n    ----------\n    n_channels_in : int, optional\n        Number of input channels. Default is 1.\n    n_channels_out : int, optional\n        Number of output channels. Default is 1.\n    n_levels : int, optional\n        Number of levels in the UNet. Default is 3.\n    n_features : int, optional\n        Number of features in the UNet. Default is 32.\n    n_channels_skip : int, optional\n        Number of skip connections channels. Default is None.\n    n_dims : int, optional\n        Number of dimensions of the signals/convolutions, by default 2.\n    bilinear : bool, optional\n        Whether to use bilinear interpolation. Default is True.\n    pad_mode : str, optional\n        Padding mode for convolutional layers. Default is \"replicate\".\n    \"\"\"\n    super().__init__(n_features=n_features, n_channels_in=n_channels_in, n_channels_out=n_channels_out, n_dims=n_dims)\n    self.n_levels = n_levels\n    self.n_channels_skip = n_channels_skip\n    self.bilinear = bilinear\n    self.pad_mode = pad_mode\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.NetworkParamsUNet.get_model","title":"get_model","text":"<pre><code>get_model(\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Get a U-net model with the selected parameters.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The U-net model.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def get_model(self, device: str = \"cuda\" if is_cuda_available() else \"cpu\") -&gt; Module:\n    \"\"\"Get a U-net model with the selected parameters.\n\n    Parameters\n    ----------\n    device : str, optional\n        The device that the the model should run on, by default \"cuda\" if cuda is available, otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The U-net model.\n    \"\"\"\n    return UNet(\n        n_channels_in=self.n_channels_in,\n        n_channels_out=self.n_channels_out,\n        n_features=self.n_features,\n        n_levels=self.n_levels,\n        n_channels_skip=self.n_channels_skip,\n        n_dims=self.n_dims,\n        bilinear=self.bilinear,\n        pad_mode=self.pad_mode,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.SerializableModel","title":"SerializableModel","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for serializable models.</p> <p>Provides a dictionary containing the initialization parameters of the model.</p>"},{"location":"reference/autoden/models/config/#autoden.models.config.create_network","title":"create_network","text":"<pre><code>create_network(\n    model: str | NetworkParams | Mapping | Module,\n    init_params: Mapping | None = None,\n    state_dict: Mapping | None = None,\n    device: str = \"cuda\" if is_available() else \"cpu\",\n) -&gt; Module\n</code></pre> <p>Create and return a neural network model based on the provided network configuration.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | NetworkParams | Mapping | Module</code>)           \u2013            <p>The network configuration. It can be a string specifying the network type, an instance of <code>NetworkParams</code>, or an already instantiated <code>Module</code>. If a string is provided, it must be one of the supported network types: \"msd\", \"unet\", or \"dncnn\".</p> </li> <li> <code>state_dict</code>               (<code>Mapping | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing the state dictionary of the model. If provided, the model's parameters will be loaded from this dictionary. Default is None.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda' if is_available() else 'cpu'</code> )           \u2013            <p>The device to which the model should be moved. Default is \"cuda\" if CUDA is available, otherwise \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The created neural network model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the provided network name is invalid or the network type is not supported.</p> </li> </ul> Notes <p>The function supports the following network types: - \"msd\": Multi-Scale Dense Network. - \"unet\": U-Net. - \"dncnn\": Denoising Convolutional Neural Network. - \"resnet\": Residual Neural Network.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net = create_network(\"unet\")\n&gt;&gt;&gt; print(net)\nModel UNet - num. parameters: 1234567\n</code></pre> Source code in <code>src/autoden/models/config.py</code> <pre><code>def create_network(\n    model: str | NetworkParams | Mapping | Module,\n    init_params: Mapping | None = None,\n    state_dict: Mapping | None = None,\n    device: str = \"cuda\" if is_cuda_available() else \"cpu\",\n) -&gt; Module:\n    \"\"\"\n    Create and return a neural network model based on the provided network configuration.\n\n    Parameters\n    ----------\n    model : str | NetworkParams | Mapping | Module\n        The network configuration. It can be a string specifying the network type,\n        an instance of `NetworkParams`, or an already instantiated `Module`.\n        If a string is provided, it must be one of the supported network types:\n        \"msd\", \"unet\", or \"dncnn\".\n    state_dict : Mapping | None, optional\n        A dictionary containing the state dictionary of the model. If provided,\n        the model's parameters will be loaded from this dictionary. Default is None.\n    device : str, optional\n        The device to which the model should be moved. Default is \"cuda\" if CUDA is available,\n        otherwise \"cpu\".\n\n    Returns\n    -------\n    Module\n        The created neural network model.\n\n    Raises\n    ------\n    ValueError\n        If the provided network name is invalid or the network type is not supported.\n\n    Notes\n    -----\n    The function supports the following network types:\n    - \"msd\": Multi-Scale Dense Network.\n    - \"unet\": U-Net.\n    - \"dncnn\": Denoising Convolutional Neural Network.\n    - \"resnet\": Residual Neural Network.\n\n    Examples\n    --------\n    &gt;&gt;&gt; net = create_network(\"unet\")\n    &gt;&gt;&gt; print(net)\n    Model UNet - num. parameters: 1234567\n    \"\"\"\n    if isinstance(model, Mapping):\n        if not all(key in model for key in (\"model_class\", \"init_params\", \"state_dict\")):\n            raise ValueError(\n                \"Malformed model state dictionary. Expected mandatory fields: 'model_class', 'init_params', and 'state_dict'\"\n            )\n        state_dict = model[\"state_dict\"]\n        init_params = model[\"init_params\"]\n        model = model[\"model_class\"]\n\n    if init_params is None:\n        init_params = dict()\n    else:\n        init_params = dict(**init_params)\n\n    for par in (\"device\", \"verbose\"):\n        if par in init_params:\n            del init_params[par]\n\n    if isinstance(model, str):\n        if model.lower() in (\"msd\", MSDnet.__name__.lower()):\n            model = NetworkParamsMSD(**init_params)\n        elif model.lower() == UNet.__name__.lower():\n            model = NetworkParamsUNet(**init_params)\n        elif model.lower() == DnCNN.__name__.lower():\n            model = NetworkParamsDnCNN(**init_params)\n        elif model.lower() == Resnet.__name__.lower():\n            model = NetworkParamsResnet(**init_params)\n        else:\n            raise ValueError(f\"Invalid model name: {model}\")\n\n    if isinstance(model, NetworkParams):\n        net = model.get_model(device)\n    elif isinstance(model, Module):\n        net = model.to(device=device)\n    else:\n        raise ValueError(f\"Invalid model type: {type(model)}\")\n\n    if state_dict is not None:\n        net.load_state_dict(state_dict)\n        net.to(device)  # Needed to ensure that the model lives in the correct device\n\n    print(f\"Model {net.__class__.__name__} - num. parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad)}\")\n    return net\n</code></pre>"},{"location":"reference/autoden/models/config/#autoden.models.config.create_optimizer","title":"create_optimizer","text":"<pre><code>create_optimizer(\n    network: Module,\n    algo: str = \"adam\",\n    learning_rate: float = 0.001,\n    weight_decay: float = 0.01,\n    optim_state: Mapping | None = None,\n) -&gt; Optimizer\n</code></pre> <p>Instantiates the desired optimizer for the given model.</p> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>Module</code>)           \u2013            <p>The network to train.</p> </li> <li> <code>algo</code>               (<code>str</code>, default:                   <code>'adam'</code> )           \u2013            <p>The requested optimizer, by default \"adam\".</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The desired learning rate, by default 1e-3.</p> </li> <li> <code>weight_decay</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The desired weight decay, by default 1e-2.</p> </li> <li> <code>optim_state</code>               (<code>Mapping | None</code>, default:                   <code>None</code> )           \u2013            <p>The state dictionary for the optimizer, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>The chosen optimizer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unsupported algorithm is requested.</p> </li> </ul> Source code in <code>src/autoden/models/config.py</code> <pre><code>def create_optimizer(\n    network: Module,\n    algo: str = \"adam\",\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-2,\n    optim_state: Mapping | None = None,\n) -&gt; Optimizer:\n    \"\"\"Instantiates the desired optimizer for the given model.\n\n    Parameters\n    ----------\n    network : torch.nn.Module\n        The network to train.\n    algo : str, optional\n        The requested optimizer, by default \"adam\".\n    learning_rate : float, optional\n        The desired learning rate, by default 1e-3.\n    weight_decay : float, optional\n        The desired weight decay, by default 1e-2.\n    optim_state : Mapping | None, optional\n        The state dictionary for the optimizer, by default None.\n\n    Returns\n    -------\n    torch.optim.Optimizer\n        The chosen optimizer.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported algorithm is requested.\n    \"\"\"\n    if algo.lower() == \"adam\":\n        optimizer = pt.optim.AdamW(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"sgd\":\n        optimizer = pt.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"rmsprop\":\n        optimizer = pt.optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif algo.lower() == \"lbfgs\":\n        optimizer = pt.optim.LBFGS(network.parameters(), lr=learning_rate, max_iter=10000, history_size=50)\n    else:\n        raise ValueError(f\"Unknown algorithm: {algo}\")\n\n    if optim_state is not None:\n        optimizer.load_state_dict(dict(**optim_state))\n\n    return optimizer\n</code></pre>"},{"location":"reference/autoden/models/dncnn/","title":"autoden.models.dncnn","text":""},{"location":"reference/autoden/models/dncnn/#autoden.models.dncnn","title":"dncnn","text":"<p>Classes:</p> <ul> <li> <code>ConvBlock</code>           \u2013            <p>Convolution block: conv =&gt; BN =&gt; act.</p> </li> <li> <code>DnCNN</code>           \u2013            <p>Implementation of the DnCNN architecture from [1].</p> </li> </ul>"},{"location":"reference/autoden/models/dncnn/#autoden.models.dncnn.ConvBlock","title":"ConvBlock","text":"<pre><code>ConvBlock(\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    n_dims: int = 2,\n    pad_mode: str = \"replicate\",\n    last_block: bool = False,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Convolution block: conv =&gt; BN =&gt; act.</p> Source code in <code>src/autoden/models/dncnn.py</code> <pre><code>def __init__(\n    self, in_ch: int, out_ch: int, kernel_size: int, n_dims: int = 2, pad_mode: str = \"replicate\", last_block: bool = False\n):\n    pad_size = (kernel_size - 1) // 2\n    if last_block:\n        post_conv = []\n    else:\n        post_conv = [NDBatchNorm[n_dims](out_ch), nn.LeakyReLU(0.2, inplace=True)]\n    super().__init__(\n        NDConv[n_dims](in_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=False),\n        *post_conv,\n    )\n</code></pre>"},{"location":"reference/autoden/models/dncnn/#autoden.models.dncnn.DnCNN","title":"DnCNN","text":"<pre><code>DnCNN(\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 20,\n    n_features: int = 32,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if is_available() else \"cpu\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Implementation of the DnCNN architecture from [1].</p> <p>[1] Zhang, et al., \"Beyond a Gaussian denoiser: Residual learning of deep CNN     for image denoising,\" IEEE Trans. on Image Processing, 2017.</p> Source code in <code>src/autoden/models/dncnn.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 20,\n    n_features: int = 32,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n):\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n    # From zhang-2017-beyon-gauss-denois:\n    #\n    #  Thus, for Gaussian denoising with a certain noise level, we\n    #  set the receptive field size of DnCNN to 35 \u00d7 35 with the\n    #  corresponding depth of 17. For other general image denoising\n    #  tasks, we adopt a larger receptive field and set the depth\n    #  to be 20.\n    #\n    # Hence, we set the standard depth to 20.\n    layers = [\n        ConvBlock(\n            n_channels_in if i_l == 0 else n_features,\n            n_channels_out if i_l == (n_layers - 1) else n_features,\n            kernel_size=kernel_size,\n            n_dims=n_dims,\n            pad_mode=pad_mode,\n            last_block=(i_l == (n_layers - 1)),\n        )\n        for i_l in range(n_layers)\n    ]\n\n    super().__init__(*layers)\n    self.init_params = init_params\n    self.device = device\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/io/","title":"autoden.models.io","text":""},{"location":"reference/autoden/models/io/#autoden.models.io","title":"io","text":"<p>IO module.</p> <p>Functions:</p> <ul> <li> <code>load_model</code>             \u2013              <p>Load a model from a file.</p> </li> <li> <code>load_model_state</code>             \u2013              <p>Load a model from disk.</p> </li> <li> <code>save_model</code>             \u2013              <p>Save a model and optionally its optimizer state to a file.</p> </li> <li> <code>save_model_state</code>             \u2013              <p>Save a model's state to disk.</p> </li> </ul>"},{"location":"reference/autoden/models/io/#autoden.models.io.load_model","title":"load_model","text":"<pre><code>load_model(file_path: str | Path) -&gt; dict\n</code></pre> <p>Load a model from a file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str or Path</code>)           \u2013            <p>The path to the file from which the model will be loaded.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>A dictionary containing the loaded model state, including the model class name, initialization parameters, epoch number, state dictionary, and optimizer state (if available).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the specified file does not exist.</p> </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def load_model(file_path: str | Path) -&gt; dict:\n    \"\"\"\n    Load a model from a file.\n\n    Parameters\n    ----------\n    file_path : str or Path\n        The path to the file from which the model will be loaded.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the loaded model state, including the model class name,\n        initialization parameters, epoch number, state dictionary, and optimizer state\n        (if available).\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    \"\"\"\n    return load(file_path)\n</code></pre>"},{"location":"reference/autoden/models/io/#autoden.models.io.load_model_state","title":"load_model_state","text":"<pre><code>load_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int | None = None,\n) -&gt; Mapping\n</code></pre> <p>Load a model from disk.</p> <p>Parameters:</p> <ul> <li> <code>save_epochs_dir</code>               (<code>str | Path</code>)           \u2013            <p>The director where the models are saved</p> </li> <li> <code>epoch_num</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The epoch number or if None/-1 the best state will be loaded, by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Mapping</code>           \u2013            <p>The loaded model state and possibly an optimizer state.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the directory does not exist or the requested model is not available.</p> </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def load_model_state(save_epochs_dir: str | Path, epoch_num: int | None = None) -&gt; Mapping:\n    \"\"\"Load a model from disk.\n\n    Parameters\n    ----------\n    save_epochs_dir : str | Path\n        The director where the models are saved\n    epoch_num : int | None, optional\n        The epoch number or if None/-1 the best state will be loaded, by default None\n\n    Returns\n    -------\n    Mapping\n        The loaded model state and possibly an optimizer state.\n\n    Raises\n    ------\n    ValueError\n        When the directory does not exist or the requested model is not available.\n    \"\"\"\n    epochs_base_path = Path(save_epochs_dir) / \"weights\"\n    if not epochs_base_path.exists():\n        raise ValueError(f\"Directory of the model state {epochs_base_path} does not exist!\")\n\n    if epoch_num is None or epoch_num == -1:\n        state_path = epochs_base_path / \"weights.pt\"\n    else:\n        state_path = epochs_base_path / f\"weights_epoch_{epoch_num}.pt\"\n    if not state_path.exists():\n        raise ValueError(f\"Model state {state_path} does not exist!\")\n\n    print(f\"Loading state path: {state_path}\")\n    return load_model(state_path)\n</code></pre>"},{"location":"reference/autoden/models/io/#autoden.models.io.save_model","title":"save_model","text":"<pre><code>save_model(\n    dst_file: str | Path,\n    model: Module,\n    optim_state: Mapping | None = None,\n    epoch_num: int = 0,\n) -&gt; None\n</code></pre> <p>Save a model and optionally its optimizer state to a file.</p> <p>Parameters:</p> <ul> <li> <code>dst_file</code>               (<code>str or Path</code>)           \u2013            <p>The destination file path where the model will be saved.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to be saved. It must implement the <code>SerializableModel</code> protocol.</p> </li> <li> <code>optim_state</code>               (<code>Mapping</code>, default:                   <code>None</code> )           \u2013            <p>The state of the optimizer to be saved. Default is None.</p> </li> <li> <code>epoch_num</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current epoch number. Default is 0.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model does not implement the <code>SerializableModel</code> protocol.</p> </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def save_model(dst_file: str | Path, model: Module, optim_state: Mapping | None = None, epoch_num: int = 0) -&gt; None:\n    \"\"\"\n    Save a model and optionally its optimizer state to a file.\n\n    Parameters\n    ----------\n    dst_file : str or Path\n        The destination file path where the model will be saved.\n    model : Module\n        The model to be saved. It must implement the `SerializableModel` protocol.\n    optim_state : Mapping, optional\n        The state of the optimizer to be saved. Default is None.\n    epoch_num : int, optional\n        The current epoch number. Default is 0.\n\n    Raises\n    ------\n    ValueError\n        If the model does not implement the `SerializableModel` protocol.\n    \"\"\"\n    if not isinstance(model, SerializableModel):\n        raise ValueError(\"The model needs to implement the protocol SerializableModel, in order to be writable to disk\")\n\n    save(\n        {\n            \"model_class\": model.__class__.__name__,\n            \"init_params\": model.init_params,\n            \"epoch\": epoch_num,\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optim_state,\n        },\n        dst_file,\n    )\n</code></pre>"},{"location":"reference/autoden/models/io/#autoden.models.io.save_model_state","title":"save_model_state","text":"<pre><code>save_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int,\n    model: Module,\n    optim_state: Mapping | None = None,\n    is_best: bool = False,\n) -&gt; None\n</code></pre> <p>Save a model's state to disk.</p> <p>This function saves the state of a model and optionally its optimizer to disk. The model state is saved in a directory specified by <code>save_epochs_dir</code>. If <code>is_best</code> is True, the model state is saved as \"weights.pt\". Otherwise, it is saved with a filename that includes the epoch number.</p> <p>Parameters:</p> <ul> <li> <code>save_epochs_dir</code>               (<code>str | Path</code>)           \u2013            <p>The directory where to save the model state.</p> </li> <li> <code>epoch_num</code>               (<code>int</code>)           \u2013            <p>The epoch number.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model whose state is to be saved.</p> </li> <li> <code>optim_state</code>               (<code>Mapping</code>, default:                   <code>None</code> )           \u2013            <p>The optimizer state to save, by default None.</p> </li> <li> <code>is_best</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether it is the best fitted model, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            </li> </ul> Source code in <code>src/autoden/models/io.py</code> <pre><code>def save_model_state(\n    save_epochs_dir: str | Path,\n    epoch_num: int,\n    model: Module,\n    optim_state: Mapping | None = None,\n    is_best: bool = False,\n) -&gt; None:\n    \"\"\"Save a model's state to disk.\n\n    This function saves the state of a model and optionally its optimizer to disk.\n    The model state is saved in a directory specified by `save_epochs_dir`. If\n    `is_best` is True, the model state is saved as \"weights.pt\". Otherwise, it is\n    saved with a filename that includes the epoch number.\n\n    Parameters\n    ----------\n    save_epochs_dir : str | Path\n        The directory where to save the model state.\n    epoch_num : int\n        The epoch number.\n    model : Module\n        The model whose state is to be saved.\n    optim_state : Mapping, optional\n        The optimizer state to save, by default None.\n    is_best : bool, optional\n        Whether it is the best fitted model, by default False.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    epochs_base_path = Path(save_epochs_dir) / \"weights\"\n    epochs_base_path.mkdir(parents=True, exist_ok=True)\n\n    dst_file = epochs_base_path / (\"weights.pt\" if is_best else f\"weights_epoch_{epoch_num}.pt\")\n    save_model(dst_file=dst_file, model=model, optim_state=optim_state, epoch_num=epoch_num)\n</code></pre>"},{"location":"reference/autoden/models/msd/","title":"autoden.models.msd","text":""},{"location":"reference/autoden/models/msd/#autoden.models.msd","title":"msd","text":"<p>Module implementing MS-D net.</p> <p>Classes:</p> <ul> <li> <code>DilatedConvBlock</code>           \u2013            <p>Dilated convolution block (dilated_conv =&gt; BN =&gt; ReLU).</p> </li> <li> <code>MSDDilBlock</code>           \u2013            <p>MS-D Block containing the sequence of dilated convolutional layers.</p> </li> <li> <code>MSDSampBlock</code>           \u2013            <p>MS-D Block containing the sequence of dilated convolutional layers.</p> </li> <li> <code>MSDnet</code>           \u2013            <p>Simple MS-D net implementation.</p> </li> <li> <code>SamplingConvBlock</code>           \u2013            <p>Down-sampling convolution module (down-samp =&gt; conv =&gt; BN =&gt; ReLU =&gt; up-samp).</p> </li> </ul>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.DilatedConvBlock","title":"DilatedConvBlock","text":"<pre><code>DilatedConvBlock(\n    in_ch: int,\n    out_ch: int,\n    dilation: int = 1,\n    pad_mode: str = \"replicate\",\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Dilated convolution block (dilated_conv =&gt; BN =&gt; ReLU).</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, dilation: int = 1, pad_mode: str = \"replicate\", n_dims: int = 2) -&gt; None:\n    super().__init__(\n        NDConv[n_dims](in_ch, out_ch, 3, padding=dilation, dilation=dilation, padding_mode=pad_mode),\n        NDBatchNorm[n_dims](out_ch),\n        nn.LeakyReLU(0.2, inplace=True),\n    )\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.MSDDilBlock","title":"MSDDilBlock","text":"<pre><code>MSDDilBlock(\n    n_channels_in: int,\n    n_features: int,\n    n_layers: int,\n    dilations: Sequence[int] | NDArray,\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MS-D Block containing the sequence of dilated convolutional layers.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(\n    self, n_channels_in: int, n_features: int, n_layers: int, dilations: Sequence[int] | NDArray, n_dims: int = 2\n) -&gt; None:\n    super().__init__()\n    self.n_features = n_features\n    self.n_layers = n_layers\n    self.dilations = dilations\n    self.n_dims = n_dims\n    convs = [\n        DilatedConvBlock(n_channels_in + n_features * ii, n_features, dilation=self._layer_dilation(ii), n_dims=n_dims)\n        for ii in range(n_layers)\n    ]\n    self.convs = nn.ModuleList(convs)\n    self.n_ch_in = n_channels_in\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.MSDSampBlock","title":"MSDSampBlock","text":"<pre><code>MSDSampBlock(\n    n_channels_in: int,\n    n_features: int,\n    n_layers: int,\n    dilations: Sequence[int] | NDArray,\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MS-D Block containing the sequence of dilated convolutional layers.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(\n    self, n_channels_in: int, n_features: int, n_layers: int, dilations: Sequence[int] | NDArray, n_dims: int = 2\n) -&gt; None:\n    super().__init__()\n    self.n_features = n_features\n    self.n_layers = n_layers\n    self.dilations = dilations\n    self.n_dims = n_dims\n    convs = [\n        SamplingConvBlock(n_channels_in + n_features * ii, n_features, samp_factor=self._layer_sampling(ii), n_dims=n_dims)\n        for ii in range(n_layers)\n    ]\n    self.convs = nn.ModuleList(convs)\n    self.n_ch_in = n_channels_in\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.MSDnet","title":"MSDnet","text":"<pre><code>MSDnet(\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    n_dims: int = 2,\n    dilations: Sequence[int] | NDArray = tuple(range(1, 5)),\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    use_dilations: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple MS-D net implementation.</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int = 1,\n    n_channels_out: int = 1,\n    n_layers: int = 12,\n    n_features: int = 1,\n    n_dims: int = 2,\n    dilations: Sequence[int] | NDArray = tuple(range(1, 5)),\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    use_dilations: bool = True,\n) -&gt; None:\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n\n    super().__init__()\n    self.init_params = init_params\n    self.device = device\n\n    if use_dilations:\n        self.msd_block = MSDDilBlock(n_channels_in, n_features, n_layers, dilations, n_dims=n_dims)\n    else:\n        self.msd_block = MSDSampBlock(n_channels_in, n_features, n_layers, dilations, n_dims=n_dims)\n    self.outc = NDConv[n_dims](n_channels_in + n_features * n_layers, n_channels_out, kernel_size=1)\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/msd/#autoden.models.msd.SamplingConvBlock","title":"SamplingConvBlock","text":"<pre><code>SamplingConvBlock(\n    in_ch: int,\n    out_ch: int,\n    samp_factor: int = 1,\n    n_dims: int = 2,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Down-sampling convolution module (down-samp =&gt; conv =&gt; BN =&gt; ReLU =&gt; up-samp).</p> Source code in <code>src/autoden/models/msd.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, samp_factor: int = 1, n_dims: int = 2) -&gt; None:\n    if samp_factor &gt; 1:\n        pre = [NDPool[n_dims](samp_factor)]\n        post = [nn.Upsample(scale_factor=samp_factor, mode=NDUpsampling[n_dims], align_corners=True)]\n    else:\n        pre = post = []\n    super().__init__(\n        *pre,\n        NDConv[n_dims](in_ch, out_ch, 3, padding=1),\n        NDBatchNorm[n_dims](out_ch),\n        nn.LeakyReLU(0.2, inplace=True),\n        *post,\n    )\n</code></pre>"},{"location":"reference/autoden/models/param_utils/","title":"autoden.models.param_utils","text":""},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils","title":"param_utils","text":"<p>This module provides utility functions for handling PyTorch models, including optimization, parameter management, and gradient retrieval.</p> <p>Functions:     create_optimizer: Instantiates the desired optimizer for the given model.     get_num_parameters: Returns the number of trainable parameters in the model.     set_parameters: Sets the parameters of the model from a given array of values.     get_parameters: Gets the parameters of the model.     get_gradients: Gets the gradients of the model parameters.</p> <p>Functions:</p> <ul> <li> <code>fix_invalid_gradient_values</code>             \u2013              <p>Fixes invalid gradient values in the model's parameters.</p> </li> <li> <code>get_gradients</code>             \u2013              <p>Gets the gradients of the model parameters.</p> </li> <li> <code>get_num_parameters</code>             \u2013              <p>Returns the number of trainable parameters in the model.</p> </li> <li> <code>get_parameters</code>             \u2013              <p>Gets the parameters of the model.</p> </li> <li> <code>set_parameters</code>             \u2013              <p>Sets the parameters of the model from a given array of values.</p> </li> </ul>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.fix_invalid_gradient_values","title":"fix_invalid_gradient_values","text":"<pre><code>fix_invalid_gradient_values(model: Module) -&gt; None\n</code></pre> <p>Fixes invalid gradient values in the model's parameters.</p> <p>This function iterates over all parameters of the given model and sets the gradient values to zero where they are not finite (i.e., NaN or infinity).</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The neural network model whose gradient values need to be fixed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>This function modifies the gradients in place and does not return anything.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def fix_invalid_gradient_values(model: nn.Module) -&gt; None:\n    \"\"\"\n    Fixes invalid gradient values in the model's parameters.\n\n    This function iterates over all parameters of the given model and sets the\n    gradient values to zero where they are not finite (i.e., NaN or infinity).\n\n    Parameters\n    ----------\n    model : nn.Module\n        The neural network model whose gradient values need to be fixed.\n\n    Returns\n    -------\n    None\n        This function modifies the gradients in place and does not return anything.\n    \"\"\"\n    for pars in model.parameters():\n        if pars.grad is not None:\n            pars.grad[pt.logical_not(pt.isfinite(pars.grad))] = 0.0\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.get_gradients","title":"get_gradients","text":"<pre><code>get_gradients(\n    model: Module, flatten: bool = True\n) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]\n</code></pre> <p>Gets the gradients of the model parameters.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to get the gradients from.</p> </li> <li> <code>flatten</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, flattens the gradients, by default True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]</code>           \u2013            <p>A tuple containing the gradient values and their shapes.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_gradients(model: nn.Module, flatten: bool = True) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]:\n    \"\"\"Gets the gradients of the model parameters.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to get the gradients from.\n    flatten : bool, optional\n        If True, flattens the gradients, by default True.\n\n    Returns\n    -------\n    tuple[numpy.typing.NDArray, Sequence[tuple[str, Sequence[int]]]]\n        A tuple containing the gradient values and their shapes.\n    \"\"\"\n    grads = []\n    info = []\n    for name, params in model.named_parameters():\n        if params.grad is not None:\n            g1 = params.grad.view(-1)\n            grad = g1.detach().cpu().numpy().copy()\n            if flatten:\n                grad = grad.flatten()\n            grads.append(grad)\n            info.append((name, [*params.shape]))\n    return np.concatenate(grads), info\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.get_num_parameters","title":"get_num_parameters","text":"<pre><code>get_num_parameters(\n    model: Module, verbose: bool = False\n) -&gt; int\n</code></pre> <p>Returns the number of trainable parameters in the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to count the parameters for.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, prints the number of parameters, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of trainable parameters.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_num_parameters(model: nn.Module, verbose: bool = False) -&gt; int:\n    \"\"\"Returns the number of trainable parameters in the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to count the parameters for.\n    verbose : bool, optional\n        If True, prints the number of parameters, by default False.\n\n    Returns\n    -------\n    int\n        The number of trainable parameters.\n    \"\"\"\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if verbose:\n        print(f\"Model {model.__class__.__name__} - num. parameters: {num_params}\")\n    return num_params\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters(\n    model: Module,\n    parameter_type: str | None = None,\n    filter_params: bool = True,\n) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]\n</code></pre> <p>Gets the parameters of the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to get the parameters from.</p> </li> <li> <code>parameter_type</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The type of parameters to filter, by default None.</p> </li> <li> <code>filter_params</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, filters the parameters based on the parameter_type, by default True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]</code>           \u2013            <p>A tuple containing the parameter values and their shapes.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def get_parameters(\n    model: nn.Module, parameter_type: str | None = None, filter_params: bool = True\n) -&gt; tuple[NDArray, Sequence[tuple[str, Sequence[int]]]]:\n    \"\"\"Gets the parameters of the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to get the parameters from.\n    parameter_type : str | None, optional\n        The type of parameters to filter, by default None.\n    filter_params : bool, optional\n        If True, filters the parameters based on the parameter_type, by default True.\n\n    Returns\n    -------\n    tuple[numpy.typing.NDArray, Sequence[tuple[str, Sequence[int]]]]\n        A tuple containing the parameter values and their shapes.\n    \"\"\"\n    vals = []\n    info = []\n    for name, params in model.named_parameters():\n        p1 = params.view(-1)\n        if parameter_type is None or name.split(\".\")[-1] == parameter_type.lower():\n            vals.append(p1.detach().cpu().numpy().copy().flatten())\n            info.append((name, [*params.shape]))\n        elif not filter_params:\n            vals.append(np.zeros_like(p1.detach().cpu().numpy()).flatten())\n            info.append((name, [*params.shape]))\n    return np.concatenate(vals), info\n</code></pre>"},{"location":"reference/autoden/models/param_utils/#autoden.models.param_utils.set_parameters","title":"set_parameters","text":"<pre><code>set_parameters(\n    model: Module,\n    values: NDArray,\n    info: Sequence[tuple[str, Sequence[int]]],\n) -&gt; None\n</code></pre> <p>Sets the parameters of the model from a given array of values.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to set the parameters for.</p> </li> <li> <code>values</code>               (<code>NDArray</code>)           \u2013            <p>The array of parameter values.</p> </li> <li> <code>info</code>               (<code>Sequence[tuple[str, Sequence[int]]]</code>)           \u2013            <p>Information about the parameter names and shapes.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the length of the values array does not match the total number of parameters.</p> </li> </ul> Source code in <code>src/autoden/models/param_utils.py</code> <pre><code>def set_parameters(model: nn.Module, values: NDArray, info: Sequence[tuple[str, Sequence[int]]]) -&gt; None:\n    \"\"\"Sets the parameters of the model from a given array of values.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to set the parameters for.\n    values : numpy.typing.NDArray\n        The array of parameter values.\n    info : Sequence[tuple[str, Sequence[int]]]\n        Information about the parameter names and shapes.\n\n    Raises\n    ------\n    ValueError\n        If the length of the values array does not match the total number of parameters.\n    \"\"\"\n    if len(values) != sum([np.prod(v) for _, v in info]):\n        raise ValueError(\"Inconsistent length of values array and parameters shapes\")\n    state_dict = model.state_dict()\n    params_start = 0\n    for name, p_shape in info:\n        params_end = params_start + np.prod(p_shape)\n        state_dict[name][:] = pt.tensor(values[params_start:params_end].reshape(p_shape))\n        params_start = params_end\n</code></pre>"},{"location":"reference/autoden/models/resnet/","title":"autoden.models.resnet","text":""},{"location":"reference/autoden/models/resnet/#autoden.models.resnet","title":"resnet","text":"<p>Classes:</p> <ul> <li> <code>ResBlock</code>           \u2013            <p>Residual block: conv =&gt; BN =&gt; act. =&gt; conv =&gt; BN =&gt; residual link =&gt; (optional) act.</p> </li> <li> <code>Resnet</code>           \u2013            <p>Implementation of the Resnet architecture.</p> </li> </ul>"},{"location":"reference/autoden/models/resnet/#autoden.models.resnet.ResBlock","title":"ResBlock","text":"<pre><code>ResBlock(\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    n_dims: int = 2,\n    pad_mode: str = \"replicate\",\n    last_block: bool = False,\n    bias: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Residual block: conv =&gt; BN =&gt; act. =&gt; conv =&gt; BN =&gt; residual link =&gt; (optional) act.</p> Source code in <code>src/autoden/models/resnet.py</code> <pre><code>def __init__(\n    self,\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    n_dims: int = 2,\n    pad_mode: str = \"replicate\",\n    last_block: bool = False,\n    bias: bool = True,\n):\n    super().__init__()\n    pad_size = (kernel_size - 1) // 2\n    self.main_seq = nn.ModuleList(\n        [\n            NDConv[n_dims](in_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=bias),\n            NDBatchNorm[n_dims](out_ch),\n            nn.LeakyReLU(0.2, inplace=True),\n            NDConv[n_dims](out_ch, out_ch, kernel_size=kernel_size, padding=pad_size, padding_mode=pad_mode, bias=bias),\n            NDBatchNorm[n_dims](out_ch),\n        ]\n    )\n    self.scale_inp = NDConv[n_dims](in_ch, out_ch, kernel_size=1, bias=bias) if in_ch != out_ch else None\n    self.post_res = nn.LeakyReLU(0.2, inplace=True) if not last_block else None\n</code></pre>"},{"location":"reference/autoden/models/resnet/#autoden.models.resnet.Resnet","title":"Resnet","text":"<pre><code>Resnet(\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 10,\n    n_features: int = 32,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if is_available() else \"cpu\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Implementation of the Resnet architecture.</p> Source code in <code>src/autoden/models/resnet.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int,\n    n_channels_out: int,\n    n_layers: int = 10,\n    n_features: int = 32,\n    n_dims: int = 2,\n    kernel_size: int = 3,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n):\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n\n    layers = [\n        ResBlock(\n            n_channels_in if i_l == 0 else n_features,\n            n_channels_out if i_l == (n_layers - 1) else n_features,\n            kernel_size=kernel_size,\n            n_dims=n_dims,\n            pad_mode=pad_mode,\n            last_block=(i_l == (n_layers - 1)),\n        )\n        for i_l in range(n_layers)\n    ]\n\n    super().__init__(*layers)\n    self.init_params = init_params\n    self.device = device\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/unet/","title":"autoden.models.unet","text":""},{"location":"reference/autoden/models/unet/#autoden.models.unet","title":"unet","text":"<p>Implementation of a flexible U-net.</p> <p>Originally inspired by: https://github.com/milesial/Pytorch-UNet</p> <p>Classes:</p> <ul> <li> <code>ConvBlock</code>           \u2013            <p>Convolution block: conv =&gt; BN =&gt; act.</p> </li> <li> <code>DoubleConv</code>           \u2013            <p>Double convolution (conv =&gt; BN =&gt; ReLU) * 2.</p> </li> <li> <code>DownBlock</code>           \u2013            <p>Down-scaling block.</p> </li> <li> <code>UNet</code>           \u2013            <p>U-net model.</p> </li> <li> <code>UpBlock</code>           \u2013            <p>Up-scaling block.</p> </li> </ul>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.ConvBlock","title":"ConvBlock","text":"<pre><code>ConvBlock(\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    n_dims: int = 2,\n    stride: int = 1,\n    dilation: int = 1,\n    pad_mode: str = \"replicate\",\n    residual: bool = False,\n    bias: bool = True,\n    last_block: bool = False,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Convolution block: conv =&gt; BN =&gt; act.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(\n    self,\n    in_ch: int,\n    out_ch: int,\n    kernel_size: int,\n    n_dims: int = 2,\n    stride: int = 1,\n    dilation: int = 1,\n    pad_mode: str = \"replicate\",\n    residual: bool = False,\n    bias: bool = True,\n    last_block: bool = False,\n):\n    pad_size = (kernel_size - 1) // 2 + (dilation - 1)\n    if last_block:\n        post_conv = []\n    else:\n        post_conv = [NDBatchNorm[n_dims](out_ch), nn.LeakyReLU(0.2, inplace=True)]\n    super().__init__(\n        NDConv[n_dims](\n            in_ch,\n            out_ch,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            padding=pad_size,\n            padding_mode=pad_mode,\n            bias=bias,\n        ),\n        *post_conv,\n    )\n    if residual and in_ch != out_ch:\n        print(f\"Warning: Residual connections not available when {in_ch=} is different from {out_ch=}\")\n        residual = False\n    self.residual = residual\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.DoubleConv","title":"DoubleConv","text":"<pre><code>DoubleConv(\n    in_ch: int,\n    out_ch: int,\n    n_dims: int = 2,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Double convolution (conv =&gt; BN =&gt; ReLU) * 2.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, n_dims: int = 2, pad_mode: str = \"replicate\"):\n    super().__init__(\n        ConvBlock(in_ch, out_ch, n_dims=n_dims, kernel_size=3, pad_mode=pad_mode),\n        ConvBlock(out_ch, out_ch, n_dims=n_dims, kernel_size=1),\n    )\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.DownBlock","title":"DownBlock","text":"<pre><code>DownBlock(\n    in_ch: int,\n    out_ch: int,\n    n_dims: int = 2,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Down-scaling block.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(self, in_ch: int, out_ch: int, n_dims: int = 2, bilinear: bool = True, pad_mode: str = \"replicate\"):\n    if bilinear:\n        down_block = [NDPool[n_dims](2)]\n    else:\n        down_block = [ConvBlock(in_ch, in_ch, n_dims=n_dims, kernel_size=2, stride=2)]\n    super().__init__(\n        *down_block,\n        DoubleConv(in_ch, out_ch, n_dims=n_dims, pad_mode=pad_mode),\n    )\n    self.n_dims = n_dims\n    self.pad_mode = pad_mode.lower()\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.UNet","title":"UNet","text":"<pre><code>UNet(\n    n_channels_in: int,\n    n_channels_out: int,\n    n_features: int = 32,\n    n_levels: int = 3,\n    n_dims: int = 2,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if is_available() else \"cpu\",\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>U-net model.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(\n    self,\n    n_channels_in: int,\n    n_channels_out: int,\n    n_features: int = 32,\n    n_levels: int = 3,\n    n_dims: int = 2,\n    n_channels_skip: int | None = None,\n    bilinear: bool = True,\n    pad_mode: str = \"replicate\",\n    device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n    verbose: bool = False,\n):\n    init_params = locals()\n    del init_params[\"self\"]\n    del init_params[\"__class__\"]\n\n    super().__init__()\n    self.init_params = init_params\n    self.device = device\n\n    if pad_mode.lower() not in PAD_MODES:\n        raise ValueError(f\"Padding mode {pad_mode} should be one of {PAD_MODES}\")\n\n    encoder, decoder = _compute_architecture(\n        n_levels=n_levels, n_features=n_features, n_skip=n_channels_skip, verbose=verbose\n    )\n\n    self.in_layer = DoubleConv(n_channels_in, n_features, n_dims=n_dims, pad_mode=pad_mode)\n    self.encoder_layers = nn.ModuleList(\n        [DownBlock(*lvl, n_dims=n_dims, bilinear=bilinear, pad_mode=pad_mode) for lvl in encoder]\n    )\n    self.decoder_layers = nn.ModuleList(\n        [UpBlock(*lvl, n_dims=n_dims, linear=bilinear, pad_mode=pad_mode) for lvl in decoder]\n    )\n    self.out_layer = ConvBlock(\n        n_features, n_channels_out, n_dims=n_dims, kernel_size=1, pad_mode=pad_mode, last_block=True\n    )\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/autoden/models/unet/#autoden.models.unet.UpBlock","title":"UpBlock","text":"<pre><code>UpBlock(\n    in_ch: int,\n    skip_ch: int | None,\n    out_ch: int,\n    n_dims: int = 2,\n    linear: bool = True,\n    pad_mode: str = \"replicate\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Up-scaling block.</p> Source code in <code>src/autoden/models/unet.py</code> <pre><code>def __init__(\n    self, in_ch: int, skip_ch: int | None, out_ch: int, n_dims: int = 2, linear: bool = True, pad_mode: str = \"replicate\"\n):\n    super().__init__()\n    self.skip_ch = skip_ch\n    self.n_dims = n_dims\n\n    # Bilinear up-sampling tends to give better results, and use fewer weights\n    if linear:\n        self.up_block = nn.Upsample(scale_factor=2, mode=NDUpsampling[n_dims], align_corners=True)\n    else:\n        self.up_block = NDConvT[n_dims](in_ch, in_ch, kernel_size=2, stride=2)\n\n    if skip_ch is not None:\n        n_skip = skip_ch\n        if skip_ch &gt; 0:\n            self.skip_block = ConvBlock(in_ch, skip_ch, n_dims=n_dims, kernel_size=1, pad_mode=pad_mode)\n    else:\n        n_skip = in_ch\n    self.conv_block = DoubleConv(in_ch + n_skip, out_ch, n_dims=n_dims, pad_mode=pad_mode)\n</code></pre>"},{"location":"tutorials/denoising_imgs/","title":"Denoising Images","text":"<p>Auto-Denoise (autoden) provides implementations for a variety of unsupervised and self-supervised Convolutional Neural Network (CNN) denoising methods. This tutorial will guide you through setting up the data, training different denoisers, performing inference, and visualizing the results.</p>"},{"location":"tutorials/denoising_imgs/#setting-up-the-data","title":"Setting Up the Data","text":"<p>First, we need to set up the data to be used for training and testing the denoisers. We will use the <code>skimage</code> library to generate a noisy image and create multiple noisy versions of it.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport skimage.color as skc\nimport skimage.data as skd\nimport skimage.transform as skt\nfrom numpy.typing import NDArray\nfrom tqdm.auto import tqdm\nimport autoden as ad\n\nUSE_CAMERA_MAN = True\nNUM_IMGS_TRN = 4\nNUM_IMGS_TST = 2\nNUM_IMGS_TOT = NUM_IMGS_TRN + NUM_IMGS_TST\n\nEPOCHS = 1024\nREG_TV_VAL = 1e-7\n\nif USE_CAMERA_MAN:\n    img_orig = skd.camera()\n    img_orig = skt.downscale_local_mean(img_orig, 4)\nelse:\n    img_orig = skd.cat()\n    img_orig = skc.rgb2gray(img_orig)\n    img_orig *= 255 / img_orig.max()\n\nimgs_noisy: NDArray = np.stack(\n    [(img_orig + 20 * np.random.randn(*img_orig.shape)) for _ in tqdm(range(NUM_IMGS_TOT), desc=\"Create noisy images\")],\n    axis=0,\n)\ntst_inds = np.arange(NUM_IMGS_TRN, NUM_IMGS_TOT)\n\nprint(f\"Img orig -&gt; [{img_orig.min()}, {img_orig.max()}], Img noisy -&gt; [{imgs_noisy[0].min()}, {imgs_noisy[0].max()}]\")\nprint(f\"Img shape: {img_orig.shape}\")\n</code></pre> <p>Training vs testing images</p> <p>Certain algorithms require to allocate a certain number of input images to the test set. This set is used to verify the model's training convergence, and to select the most appropriate epoch. The variable <code>tst_inds</code> serves this purpose, by containing the indexes of the images to use for testing.</p> <p>The algorithms that do not use this variable, will randomly select a certain fixed number of pixels as leave-out set.</p>"},{"location":"tutorials/denoising_imgs/#training-the-denoisers","title":"Training the Denoisers","text":"<p>We will train four different denoisers: Supervised Denoiser, Noise2Noise (N2N), Noise2Void (N2V), and Deep Image Prior (DIP). We first define the type of model that we will want to use. In this case it will be a U-net [1], with 16 features: <pre><code>net_params = ad.NetworkParamsUNet(n_features=16)\n</code></pre></p> <p>The variable <code>net_params</code> only defines the type of architecture that we want. When passed to the denoising algorithms, they will use it to create and initialize a U-net model. Other pre-configured models are available: MS-D net [2], DnCNN [3], and a custom ResNet implementation [4].</p>"},{"location":"tutorials/denoising_imgs/#1d-and-3d-signals","title":"1D and 3D signals","text":"<p>We have recently introduced the support for 1D and 3D signals in <code>auto-denoise</code>. To correctly process these signals, it is enough to instantiate a model with the correct dimensionality. This means that we need to set the correct value for the parameter <code>n_dims</code> in the model definition: <pre><code>net_params = ad.NetworkParamsUNet(n_features=16, n_dims=1)  # 1D Convolutions\n</code></pre> or <pre><code>net_params = ad.NetworkParamsUNet(n_features=16, n_dims=3)  # 3D Convolutions\n</code></pre></p> <p>Using a 2D model with 3D data</p> <p>It is possible to use a 3D dataset with a 2D model. The depth direction will be interpreted as the batch dimension. The 2D model will interpret each volume slice as a different image example.</p> <p>This could be useful in GPU memory constrained scenarios, where the higher memory requirements of 3D convolutions (w.r.t. 2D convolutions) could exceed the available GPU memory. This technique could save around 30\\% memory, but it would lose the information along the depth direction.</p>"},{"location":"tutorials/denoising_imgs/#supervised-denoiser","title":"Supervised Denoiser","text":"<p>The supervised denoiser is trained using pairs of noisy and clean images. It learns to map noisy images to their clean counterparts.</p> <pre><code>denoiser_sup = ad.Supervised(model=net_params, reg_val=REG_TV_VAL)\nsup_data = denoiser_sup.prepare_data(imgs_noisy, img_orig, num_tst_ratio=NUM_IMGS_TST / NUM_IMGS_TOT)\ndenoiser_sup.train(*sup_data, epochs=EPOCHS)\n</code></pre>"},{"location":"tutorials/denoising_imgs/#noise2void-n2v","title":"Noise2Void (N2V)","text":"<p>Noise2Void is a self-supervised denoising method that can work with a single noisy image [5]. This implementation can also work with structured noise [6]. It applies randomly generated masks to the images and learns to predict the masked pixels.</p> <pre><code>denoiser_n2v = ad.N2V(model=net_params, reg_val=REG_TV_VAL)\ndenoiser_n2v.train(imgs_noisy, epochs=EPOCHS, tst_inds=tst_inds)\n</code></pre>"},{"location":"tutorials/denoising_imgs/#noise2noise-n2n","title":"Noise2Noise (N2N)","text":"<p>Noise2Noise is a self-supervised denoising method that uses pairs of noisy images of the same object [7]. It learns to map one noisy image to another noisy image of the same object. The <code>prepare_data</code> function is used to organize the data in such a way that the algorithm can handle it correctly.</p> <pre><code>denoiser_n2n = ad.N2N(model=net_params, reg_val=REG_TV_VAL)\nn2n_data = denoiser_n2n.prepare_data(imgs_noisy)\ndenoiser_n2n.train(*n2n_data, epochs=EPOCHS)\n</code></pre>"},{"location":"tutorials/denoising_imgs/#batched-processing","title":"Batched processing","text":"<p>N2N and Supervised support batched processing (both during training and inference). This is usually required for large datasets, where they cannot fully fit into GPU memory. The batch size is selected through the <code>batch_size</code> argument when N2N is initialized.</p> <pre><code>denoiser_sup = ad.Supervised(model=net_params, reg_val=REG_TV_VAL, batch_size=16)\n</code></pre> <p>and</p> <pre><code>denoiser_n2n = ad.N2N(model=net_params, reg_val=REG_TV_VAL, batch_size=16)\n</code></pre>"},{"location":"tutorials/denoising_imgs/#data-augmentation","title":"Data augmentation","text":"<p>N2N and Supervised also support data augmentation in the form of image and volume flips. This could be used to virtually increase the data size during training.</p> <pre><code>denoiser_sup = ad.Supervised(model=net_params, reg_val=REG_TV_VAL, augmentation=\"flip\")\n</code></pre> <p>and</p> <pre><code>denoiser_n2n = ad.N2N(model=net_params, reg_val=REG_TV_VAL, augmentation=\"flip\")\n</code></pre>"},{"location":"tutorials/denoising_imgs/#deep-image-prior-dip","title":"Deep Image Prior (DIP)","text":"<p>Deep Image Prior is an unsupervised denoising method that can also work with a single image [8]. It uses the prior knowledge embedded in the network architecture to denoise the image. The <code>prepare_data</code> function is used to organize the data in such a way that the algorithm can handle it correctly.</p> <pre><code>denoiser_dip = ad.DIP(model=net_params, reg_val=REG_TV_VAL * 1e1)\ndip_data = denoiser_dip.prepare_data(imgs_noisy)\ndenoiser_dip.train(*dip_data, epochs=EPOCHS)\n</code></pre> <p>Regularization weight</p> <p>The DIP is more sensitive to the regularization weight, and it can be adjusted to obtain better results.</p>"},{"location":"tutorials/denoising_imgs/#performing-inference","title":"Performing Inference","text":"<p>Inference is the process of using the trained models to denoise new images. The <code>infer</code> method takes the noisy images as input and outputs the denoised images.</p>"},{"location":"tutorials/denoising_imgs/#supervised-denoiser-inference","title":"Supervised Denoiser Inference","text":"<pre><code>den_sup = denoiser_sup.infer(sup_data[0]).mean(0)\n</code></pre> <p>Inference input</p> <p>The output of the <code>prepare_data</code> function is also preferred for the inference of Supervised, even though the noisy images should still work for the foreseeable future.</p>"},{"location":"tutorials/denoising_imgs/#noise2void-n2v-inference","title":"Noise2Void (N2V) Inference","text":"<pre><code>den_n2v = denoiser_n2v.infer(imgs_noisy).mean(0)\n</code></pre>"},{"location":"tutorials/denoising_imgs/#noise2noise-n2n-inference","title":"Noise2Noise (N2N) Inference","text":"<pre><code>den_n2n = denoiser_n2n.infer(n2n_data[0])\n</code></pre> <p>Inference input</p> <p>The output of the <code>prepare_data</code> function is also needed for the inference of N2N.</p> <p>Inference output</p> <p>The <code>inference</code> function of N2N  automatically averages the splits, unless the <code>average_splits</code> flag is set to <code>False</code>.</p>"},{"location":"tutorials/denoising_imgs/#deep-image-prior-dip-inference","title":"Deep Image Prior (DIP) Inference","text":"<pre><code>den_dip = denoiser_dip.infer(dip_data[0])\n</code></pre> <p>Inference input</p> <p>The output of the <code>prepare_data</code> function is also needed for the inference of DIP.</p>"},{"location":"tutorials/denoising_imgs/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Finally, we visualize the results of the different denoisers.</p> ImageCode <p></p> <pre><code>fig, axs = plt.subplots(2, 3, sharex=True, sharey=True)\naxs[0, 0].imshow(img_orig)\naxs[0, 0].set_title(\"Original image\")\naxs[0, 1].imshow(imgs_noisy[0])\naxs[0, 1].set_title(\"Noisy image\")\naxs[0, 2].imshow(den_sup)\naxs[0, 2].set_title(\"Denoised supervised\")\naxs[1, 0].imshow(den_n2n)\naxs[1, 0].set_title(\"Denoised N2N\")\naxs[1, 1].imshow(den_n2v)\naxs[1, 1].set_title(\"Denoised N2V\")\naxs[1, 2].imshow(den_dip)\naxs[1, 2].set_title(\"Denoised DIP\")\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> <p>And here below we present the PSNR (Peak Signal-to-Noise Ration), SSIM (Structural Similarity Index) and FRC (Fourier Ring Correlation) of the results.</p> ImageCode <p>PSNR:</p> <ul> <li>Supervised: 33.7</li> <li>Noise2Void: 26.1</li> <li>Noise2Noise: 32.6</li> <li>Deep Image Prior: 29.7</li> </ul> <p>SSIM:</p> <ul> <li>Supervised: 0.901</li> <li>Noise2Void: 0.776</li> <li>Noise2Noise: 0.884</li> <li>Deep Image Prior: 0.825</li> </ul> <p></p> <pre><code>from corrct.processing.post import plot_frcs\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\nall_recs = [den_sup, den_n2v, den_n2n, den_dip]\nall_labs = [\"Supervised\", \"Noise2Void\", \"Noise2Noise\", \"Deep Image Prior\"]\n\ndata_range = img_orig.max() - img_orig.min()\nprint(\"PSNR:\")\nfor rec, lab in zip(all_recs, all_labs):\n    print(f\"- {lab}: {psnr(img_orig, rec, data_range=data_range):.3}\")\nprint(\"SSIM:\")\nfor rec, lab in zip(all_recs, all_labs):\n    print(f\"- {lab}: {ssim(img_orig, rec, data_range=data_range):.3}\")\n\nplot_frcs([(img_orig.astype(np.float32), rec) for rec in all_recs], all_labs)\n</code></pre>"},{"location":"tutorials/denoising_imgs/#references","title":"References","text":"<ol> <li> O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Convolutional Networks for Biomedical Image Segmentation,\u201d in Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, 2015, pp. 234\u2013241. doi: 10.1007/978-3-319-24574-4_28.</li> <li> D. M. Pelt and J. A. Sethian, \u201cA mixed-scale dense convolutional neural network for image analysis,\u201d Proceedings of the National Academy of Sciences, vol. 115, no. 2, pp. 254\u2013259, 2018, doi: 10.1073/pnas.1715832114.</li> <li> K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, \u201cBeyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,\u201d IEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3142\u20133155, Jul. 2017, doi: 10.1109/TIP.2017.2662206.</li> <li> K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep Residual Learning for Image Recognition,\u201d in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Jun. 2016, pp. 770\u2013778. doi: 10.1109/CVPR.2016.90.</li> <li> A. Krull, T.-O. Buchholz, and F. Jug, \u201cNoise2Void - Learning Denoising From Single Noisy Images,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Jun. 2019, pp. 2124\u20132132. doi: 10.1109/CVPR.2019.00223.</li> <li> C. Broaddus, A. Krull, M. Weigert, U. Schmidt, and G. Myers, \u201cRemoving Structured Noise with Self-Supervised Blind-Spot Networks,\u201d in 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE, Apr. 2020, pp. 159\u2013163. doi: 10.1109/ISBI45749.2020.9098336.</li> <li> J. Lehtinen et al., \u201cNoise2Noise: Learning Image Restoration without Clean Data,\u201d in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, Eds., in Proceedings of Machine Learning Research, vol. 80. PMLR, 2018, pp. 2965\u20132974. https://proceedings.mlr.press/v80/lehtinen18a.html.</li> <li> V. Lempitsky, A. Vedaldi, and D. Ulyanov, \u201cDeep Image Prior,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, Jun. 2018, pp. 9446\u20139454. doi: 10.1109/CVPR.2018.00984.</li> </ol>"},{"location":"tutorials/denoising_spectra/","title":"Denoising Spectra using Noise2Noise","text":"<p>Auto-Denoise (autoden) provides implementations for a variety of unsupervised and self-supervised Convolutional Neural Network (CNN) denoising methods. This tutorial will guide you through setting up the data, training the Noise2Noise (N2N) denoiser, performing inference, and visualizing the results using X-ray fluorescence (XRF) spectral data.</p> <p>The main idea of this tutorial is to show how we can predict higher quality spectra from shorter acquisitions. In particular, we present the case of a long 3.2 seconds XRF acquisition, obtained by accumulating 32 shorter exposures of 0.1 seconds each. Here, we show that we can obtain accurate spectra with only 4 of those 0.1 seconds acquisitions, thus reducing measurement time and deposited dose by 8 times.</p>"},{"location":"tutorials/denoising_spectra/#setting-up-the-data","title":"Setting Up the Data","text":"<p>First, we need to set up the data to be used for training and testing the denoiser. We will use the <code>pandas</code> library to load the XRF spectral data from the provided CSV file.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport autoden as ad\n\n# Load the XRF spectral data from a CSV file\nxrf_data = pd.read_csv(\"05_xrf_spectra.csv\")\n\n# Extract the energy values and count values\nenergies_keV = np.array([float(e) for e in xrf_data.columns[1:-1]])\ncounts = xrf_data.to_numpy()[:, 1:-1]\n\n# Plot the original spectra\nfig, axs = plt.subplots(1, 1, figsize=(8, 3))\naxs.plot(energies_keV, counts[0], label=\"Spectrum #1\")\naxs.plot(energies_keV, counts.mean(axis=0), label=\"Mean of spectra\")\naxs.set_xlim(energies_keV[0], energies_keV[-1])\naxs.grid()\naxs.legend()\nfig.tight_layout()\n</code></pre> </p>"},{"location":"tutorials/denoising_spectra/#training-the-noise2noise-denoiser","title":"Training the Noise2Noise Denoiser","text":"<p>We will train the Noise2Noise (N2N) denoiser using the XRF spectral data. We first define the type of model that we will want to use. In this case, it will be a DnCNN [1], with 4 features and 6 layers:</p> <pre><code>model = ad.NetworkParamsDnCNN(n_dims=1, n_features=4, n_layers=6).get_model()\n</code></pre> <p>This model is very small for CNN model standards (only 256 parameters!), as the data is quite simple, and we do not want the model to overfit the data.</p>"},{"location":"tutorials/denoising_spectra/#noise2noise-n2n","title":"Noise2Noise (N2N)","text":"<p>Noise2Noise is a self-supervised denoising method that uses pairs of noisy images of the same object [2]. It learns to map one noisy image to another noisy image of the same object. The <code>prepare_data</code> function is used to organize the data in such a way that the algorithm can handle it correctly.</p> <p><pre><code>n2n = ad.N2N(model=model, reg_val=None)\nn2n_data = n2n.prepare_data(counts[:4])\n_ = n2n.train(*n2n_data, epochs=10_000, lower_limit=0.0, learning_rate=1e-2, restarts=1)\n</code></pre> </p> <p>Here, we impose a lower limit of 0.0, as we know that the detected photons cannot be negative. We also set a high initial learning rate, to speed up the convergence (after all, we're starting from a completely untrained /  randomly initialized network). However, we then progressively reduce the learning using a cosine annealing scheduler, through the <code>restarts=1</code> argument. This value would allow us to do multiple waves of high-to-low learning rate variations for a number of restarts &gt; 1, but that is unnecessary in this instance.</p>"},{"location":"tutorials/denoising_spectra/#performing-inference","title":"Performing Inference","text":"<p>Inference is the process of using the trained model to denoise new spectra. The <code>infer</code> method takes the noisy spectra as input and outputs the denoised spectra.</p> <pre><code>pred_counts = n2n.infer(n2n_data[0])\n</code></pre>"},{"location":"tutorials/denoising_spectra/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Finally, we visualize the results of the Noise2Noise denoiser.</p> ImageCode <p></p> <pre><code>fig, axs = plt.subplots(1, 1, figsize=(8, 3))\naxs.plot(energies_keV, counts[0], \".\", label=\"Spectrum #1\")\naxs.plot(energies_keV, counts.mean(axis=0), label=\"Mean of spectra\")\naxs.plot(energies_keV, pred_counts, label=\"Predicted spectrum\")\naxs.set_xlim(energies_keV[0], energies_keV[-1])\naxs.grid()\naxs.legend()\nfig.tight_layout()\n</code></pre>"},{"location":"tutorials/denoising_spectra/#zooming-into-specific-energy-ranges","title":"Zooming into Specific Energy Ranges","text":"<p>We can also zoom into specific energy ranges to better visualize the denoising results.</p> ImageCode <p></p> <pre><code>ranges: tuple[tuple[float, float], ...] = ((2.5, 7.5), (7.75, 9.75), (15.0, 18.5), (15.2, 16.25))\n\nfig, axs = plt.subplots(len(ranges), 1, figsize=(8, len(ranges) * 2.5))\nfor ii, (e_s, e_e) in enumerate(ranges):\n    bin_s = np.abs(energies_keV - e_s).argmin()\n    bin_e = np.abs(energies_keV - e_e).argmin()\n    axs[ii].plot(energies_keV[bin_s:bin_e], counts[0, bin_s:bin_e], \".\", label=\"Spectrum #1\")\n    axs[ii].plot(energies_keV[bin_s:bin_e], counts[:, bin_s:bin_e].mean(axis=0), label=\"Mean of spectra\")\n    axs[ii].plot(energies_keV[bin_s:bin_e], pred_counts[bin_s:bin_e], label=\"Predicted spectrum\")\n    axs[ii].set_xlim(e_s, e_e)\n    axs[ii].grid()\n    axs[ii].legend()\nfig.tight_layout()\n</code></pre>"},{"location":"tutorials/denoising_spectra/#references","title":"References","text":"<ol> <li> K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, \u201cBeyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,\u201d IEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3142\u20133155, Jul. 2017, doi: 10.1109/TIP.2017.2662206.</li> <li> J. Lehtinen et al., \u201cNoise2Noise: Learning Image Restoration without Clean Data,\u201d in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, Eds., in Proceedings of Machine Learning Research, vol. 80. PMLR, 2018, pp. 2965\u20132974. https://proceedings.mlr.press/v80/lehtinen18a.html.</li> </ol>"},{"location":"tutorials/models_io/","title":"Loading and storing models","text":"<p>Auto-denoise offers the ability to store models for later use. The reasons why one would want to do that include: reproducibility for batches of tests, pre-training, fine-tuning, etc. While Auto-denoise provides a handful of pre-implemented model types, you can always use your own model. In order to use your customized models in the provided algorithms, they need to inherit from <code>PyTorch</code>'s <code>nn.Module</code> class. At the end of this tutorial, we will explain how to make your own models compatible with the storing/loading mechanics of auto-denoise.</p>"},{"location":"tutorials/models_io/#creating-models","title":"Creating models","text":"<p>Auto-denoise offers two ways to create a pre-configured model: (a) model configuration class, and (b) instantiating the model directly: (a) Configuration class<pre><code>import autoden as ad\n\nmodel_def = ad.models.config.NetworkParamsUNet(n_features=32, n_levels=3)\nmodel = model_def.get_model()\n</code></pre> or simply: (b) Direct model instantiation<pre><code>import autoden as ad\n\nmodel = ad.models.unet.UNet(1, 1, n_features=32, n_levels=3)\n</code></pre></p> <p>While the second is more compact, it might be useful to use a model configuration class when we just want to pass around a description of the model architecture, without having to instantiate and initialize its weights.</p>"},{"location":"tutorials/models_io/#storing-models","title":"Storing models","text":"<p>Storing models to file can be done through the following function: <pre><code>ad.models.io.save_model(\"file_dest.pt\", model)\n</code></pre> Optionally, <code>save_model</code> can store the optimizer state and epoch number. The model weights, name, and architecture are all saved in the same file.</p>"},{"location":"tutorials/models_io/#loading-models","title":"Loading models","text":"<p>Stored models can be loaded with <code>PyTorch</code>'s <code>load</code> function, and then the <code>create_network</code> function from auto-denoise: <pre><code>from torch import load as load_model\n\nmodel_dict = load_model(\"file_dest.pt\")\nmodel = ad.models.config.create_network(model_dict)\n</code></pre></p>"},{"location":"tutorials/models_io/#making-your-model-compatible","title":"Making your model compatible","text":"<p>To make your custom model compatible with <code>auto-denoise</code>'s storing/loading mechanics, you need to implement the <code>SerializableModel</code> protocol from <code>ad.models.config</code>:</p> SerializableModel<pre><code>from collections.abc import Mapping\nfrom typing import Protocol, runtime_checkable\n\n\n@runtime_checkable\nclass SerializableModel(Protocol):\n    \"\"\"\n    Protocol for serializable models.\n\n    Provides a dictionary containing the initialization parameters of the model.\n    \"\"\"\n\n    init_params: Mapping\n</code></pre> <p>The <code>init_params</code> dictionary should contain the input arguments necessary to initialize your model. As an instructive example, the following is the implementation from the model <code>UNet</code>:</p> UNet initialization parameters storing<pre><code>class UNet(nn.Module):\n    \"\"\"U-net model.\"\"\"\n\n    def __init__(\n        self,\n        n_channels_in: int,\n        n_channels_out: int,\n        n_features: int = 32,\n        n_levels: int = 3,\n        n_channels_skip: int | None = None,\n        bilinear: bool = True,\n        pad_mode: str = \"replicate\",\n        device: str = \"cuda\" if pt.cuda.is_available() else \"cpu\",\n        verbose: bool = False,\n    ):\n        init_params = locals()\n        del init_params[\"self\"]\n        del init_params[\"__class__\"]\n\n        super().__init__()\n        self.init_params = init_params\n        ...\n</code></pre> <p>Currently, the <code>create_network</code> function only knows the pre-configured models, so for the time being you will have to patch that function too. In the future we will provide mechanics to register your model types.</p>"},{"location":"tutorials/noise2inverse/","title":"Noise2Inverse (Self-Supervised Denoising)","text":"<p>This tutorial demonstrates how to implement Noise2Inverse (self-supervised denoising) using the Auto-Denoise (autoden) library. Noise2Inverse is a self-supervised deep convolutional denoising method for tomography, as described in [1]. This tutorial is a walk-through of example number 03, from the <code>examples</code> directory.</p> <ul> <li>[1] A. A. Hendriksen, D. M. Pelt, and K. J. Batenburg, \"Noise2Inverse: Self-Supervised Deep Convolutional Denoising for Tomography,\" IEEE Transactions on Computational Imaging, vol. 6, pp. 1320\u20131335, 2020, doi: 10.1109/TCI.2020.3019647.</li> </ul>"},{"location":"tutorials/noise2inverse/#setting-up-the-data","title":"Setting Up the Data","text":"<p>First, we need to set up the data to be used for training and testing the denoisers. We will use the <code>skimage</code> library to generate a phantom image, create a sinogram, add noise to the sinogram, and then reconstruct the noisy sinogram.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport skimage.data as skd\nimport skimage.transform as skt\nfrom corrct.models import get_vol_geom_from_data, get_vol_geom_from_volume\nfrom corrct.processing.post import plot_frcs\nfrom corrct.projectors import ProjectorUncorrected\nfrom corrct.solvers import FBP, PDHG\nfrom corrct.regularizers import Regularizer_TV2D\nfrom corrct.testing import add_noise\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\nimport autoden as ad\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib widget\n\nprint(\"Creating phantom\")\nphantom = skd.shepp_logan_phantom()\nphantom = skt.downscale_local_mean(phantom, 4).astype(np.float32)\n\nprint(\"Creating sinogram\")\nvol_geom = get_vol_geom_from_volume(phantom)\nangles_rad = np.deg2rad(np.linspace(start=0, stop=180, num=int((180 * np.pi) // 2), endpoint=False))\nwith ProjectorUncorrected(vol_geom, angles_rad) as prj:\n    sino_clean = prj.fp(phantom)\n\nprint(\"Adding noise\")\nsino_noisy, _, _ = add_noise(sino_clean, num_photons=1, readout_noise_std=2)\n\nPIX_TO_INCH = 0.02\n\nfig_size = (sino_clean.shape[-1] * 2 * PIX_TO_INCH, sino_clean.shape[-2] * PIX_TO_INCH)\nfig, axes = plt.subplots(1, 2, figsize=fig_size)\naxes[0].imshow(sino_clean, cmap='gray')\naxes[0].set_title('Clean Sinogram')\naxes[1].imshow(sino_noisy, cmap='gray')\naxes[1].set_title('Noisy Sinogram')\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> </p>"},{"location":"tutorials/noise2inverse/#creating-noisy-reconstructions","title":"Creating Noisy Reconstructions","text":"<p>We split the sinograms into two sets and create noisy reconstructions using the Filtered Back Projection (FBP) method. This provides two statistically independent reconstructions of the same object. This will be used by N2I to train the denoiser model.</p> <pre><code>print(\"Creating noisy reconstructions (FBP)\")\nvol_geom = get_vol_geom_from_data(sino_clean)\nsolver = FBP()\nwith ProjectorUncorrected(vol_geom, angles_rad) as prj:\n    rec_noisy = solver(prj, sino_noisy)[0]\n\n# Splitting sinograms\nangles_rad_rec = [angles_rad[0::2], angles_rad[1::2]]\nsinos_noisy_rec = [sino_noisy[0::2], sino_noisy[1::2]]\n\nrecs_noisy = []\nfor sino, angles in zip(sinos_noisy_rec, angles_rad_rec):\n    with ProjectorUncorrected(vol_geom, angles) as prj:\n        recs_noisy.append(solver(prj, sino)[0])\n\nrecs_noisy_stack = np.stack(recs_noisy, axis=0)\n</code></pre>"},{"location":"tutorials/noise2inverse/#total-variation-minimization","title":"Total Variation Minimization","text":"<p>We perform the Total Variation (TV) minimization reconstruction of the entire noisy sinogram as reference.</p> <p><pre><code>reg = Regularizer_TV2D(1e1)\npdhg = PDHG(regularizer=reg, verbose=True)\n\nwith ProjectorUncorrected(vol_geom, angles_rad) as prj:\n    rec_tv = pdhg(prj, sino_noisy, iterations=500)[0]\n\nPIX_TO_INCH = 0.04\nfig_size = (phantom.shape[-1] * 3 * PIX_TO_INCH, phantom.shape[-2] * PIX_TO_INCH)\nfig, axs = plt.subplots(1, 3, sharex=True, sharey=True, figsize=fig_size)\naxs[0].imshow(phantom)\naxs[0].set_title(\"Phantom\")\naxs[1].imshow(rec_noisy, vmin=0.0, vmax=1.0)\naxs[1].set_title(\"Noisy reconstruction\")\naxs[2].imshow(rec_tv, vmin=0.0, vmax=1.0)\naxs[2].set_title(\"TV-min reconstruction\")\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> </p>"},{"location":"tutorials/noise2inverse/#training-the-noise2inverse-denoiser","title":"Training the Noise2Inverse Denoiser","text":"<p>We train the N2I denoiser using the split noisy reconstructions.</p> <pre><code>EPOCHS = 1024 * 1\nREG_TV_VAL = 3e-6\n\nprint(\"Denoising reconstructions with N2N\")\nnet_params = ad.NetworkParamsUNet(n_features=24)\ndenoiser_un = ad.N2N(model=net_params, reg_val=REG_TV_VAL)\nn2n_data = denoiser_un.prepare_data(recs_noisy_stack)\ndenoiser_un.train(*n2n_data, epochs=EPOCHS)\n</code></pre> <p>The training algorithm sets aside a small portion of the images' pixels to test the quality of the denoised image. When the training is over, it will automatically select the model weights that exhibit the highest denoising performance (according to the MSE loss) on the said pixel leave-out set. </p>"},{"location":"tutorials/noise2inverse/#performing-inference","title":"Performing Inference","text":"<p>We use the trained N2I model to produce the denoised reconstruction.</p> <pre><code>rec_n2i = denoiser_un.infer(n2n_data[0]).mean(0)\n</code></pre>"},{"location":"tutorials/noise2inverse/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Finally, we visualize the results of the different denoising methods.</p> <p><pre><code>PIX_TO_INCH = 0.04\nfig_size = (phantom.shape[-1] * 3 * PIX_TO_INCH, phantom.shape[-2] * PIX_TO_INCH)\nfig, axs = plt.subplots(1, 3, sharex=True, sharey=True, figsize=fig_size)\naxs[0].imshow(phantom)\naxs[0].set_title(\"Phantom\")\naxs[1].imshow(rec_noisy, vmin=0.0, vmax=1.0)\naxs[1].set_title(\"Noisy reconstruction\")\naxs[2].imshow(rec_n2i, vmin=0.0, vmax=1.0)\naxs[2].set_title(\"Noise2Inverse reconstruction\")\nfig.tight_layout()\nplt.show(block=False)\n</code></pre> </p>"},{"location":"tutorials/noise2inverse/#evaluating-the-results","title":"Evaluating the Results","text":"<p>We now evaluate the results using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM).</p> <p><pre><code>all_recs = [rec_noisy, rec_tv, rec_n2i]\nall_labs = [\"FBP\", \"TV-min\", \"Noise2Inverse\"]\n\nprint(\"PSNR:\")\nfor rec, lab in zip(all_recs, all_labs):\n    print(f\"- {lab}: {psnr(phantom, rec, data_range=1.0):.3}\")\nprint(\"SSIM:\")\nfor rec, lab in zip(all_recs, all_labs):\n    print(f\"- {lab}: {ssim(phantom, rec, data_range=1.0):.3}\")\n\nplot_frcs([(phantom, rec) for rec in all_recs], all_labs)\n</code></pre> Resulting in: <pre><code>PSNR:\n- FBP: 21.5\n- TV-min: 24.8\n- Noise2Inverse: 25.2\nSSIM:\n- FBP: 0.388\n- TV-min: 0.558\n- Noise2Inverse: 0.707\n</code></pre> </p>"}]}